# -*- coding: utf-8 -*-
"""CL_loss_function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fHc1MADeunViXuZfXCSt4TtaIgaeL0QY
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms
import numpy as np
from sklearn.decomposition import PCA

def augment(images):
    """
    Apply tensor-based augmentations to images.

    Args:
        images (torch.Tensor): Batch of images.

    Returns:
        torch.Tensor: Augmented images.
    """
    # Resize and crop
    images = resize(images, size=[28, 28])
    # Random horizontal flip
    if torch.rand(1) > 0.5:
        images = hflip(images)
    return images

class VicRegLoss(nn.Module):
    def __init__(self, lambda_var=25, mu_mean=25, nu_cov=1):
        """
        Implements the Variance-Invariance-Covariance Regularization (VicReg) loss.
        """
        super(VicRegLoss, self).__init__()
        self.lambda_var = lambda_var
        self.mu_mean = mu_mean
        self.nu_cov = nu_cov

    def forward(self, z1, z2):
        """
        Compute the VicReg loss between two embeddings.
        """
        # Variance Loss
        variance_loss = torch.mean(torch.relu(1 - torch.std(z1, dim=0))) + \
                        torch.mean(torch.relu(1 - torch.std(z2, dim=0)))

        # Mean Loss
        mean_loss = torch.mean((torch.mean(z1, dim=0) - torch.mean(z2, dim=0))**2)

        # Covariance Loss
        def compute_covariance_loss(z):
            z_centered = z - z.mean(dim=0)
            covariance_matrix = torch.mm(z_centered.T, z_centered) / (z.size(0) - 1)
            off_diagonal_sum = torch.sum(covariance_matrix ** 2) - torch.sum(torch.diag(covariance_matrix) ** 2)
            return off_diagonal_sum

        covariance_loss = compute_covariance_loss(z1) + compute_covariance_loss(z2)

        # Total Loss
        total_loss = self.lambda_var * variance_loss + \
                     self.mu_mean * mean_loss + \
                     self.nu_cov * covariance_loss
        return total_loss

def contrastive_ntxent_loss(z1, z2, temperature=0.5):
    """
    Compute the NT-Xent (Normalized Temperature-scaled Cross Entropy) Loss.

    Args:
        z1, z2 (torch.Tensor): Embedding tensors.
        temperature (float): Scaling factor for similarity scores.

    Returns:
        torch.Tensor: Loss value.
    """
    z1 = F.normalize(z1, dim=1)
    z2 = F.normalize(z2, dim=1)

    sim_matrix = torch.mm(z1, z2.T) / temperature
    batch_size = z1.size(0)
    sim_matrix.fill_diagonal_(-float('inf'))
    labels = torch.arange(batch_size, device=z1.device)
    return nn.CrossEntropyLoss()(sim_matrix, labels)

# Contrastive Learning Loss Functions
def contrastive_loss(z1, z2, temperature=0.5):
    """
    Basic contrastive loss.

    Args:
        z1 (torch.Tensor): First set of embeddings.
        z2 (torch.Tensor): Second set of embeddings.
        temperature (float): Scaling factor for similarity scores.

    Returns:
        torch.Tensor: Computed contrastive loss.
    """
    z1 = F.normalize(z1, p=2, dim=1)
    z2 = F.normalize(z2, p=2, dim=1)
    similarity_matrix = torch.mm(z1, z2.T) / temperature
    labels = torch.arange(z1.size(0)).to(z1.device)
    return nn.CrossEntropyLoss()(similarity_matrix, labels)

def info_nce_loss(z1, z2, temperature=0.5):
    """
    InfoNCE loss for contrastive learning.

    Args:
        z1 (torch.Tensor): First set of embeddings.
        z2 (torch.Tensor): Second set of embeddings.
        temperature (float): Scaling factor for similarity scores.

    Returns:
        torch.Tensor: Computed InfoNCE loss.
    """
    z1 = F.normalize(z1, p=2, dim=1)
    z2 = F.normalize(z2, p=2, dim=1)
    batch_size = z1.size(0)
    similarity_matrix = torch.mm(z1, z2.T) / temperature

    pos_mask = torch.eye(batch_size).to(z1.device)
    neg_mask = 1 - pos_mask

    numerator = torch.exp(similarity_matrix * pos_mask)
    denominator = torch.sum(torch.exp(similarity_matrix * neg_mask), dim=1, keepdim=True) + numerator

    loss = -torch.log(numerator / denominator)
    return loss.mean()

def nt_xent_loss(z1, z2, temperature=0.5):
    """
    NT-Xent loss for self-supervised contrastive learning.

    Args:
        z1 (torch.Tensor): First set of embeddings.
        z2 (torch.Tensor): Second set of embeddings.
        temperature (float): Scaling factor for similarity scores.

    Returns:
        torch.Tensor: Computed NT-Xent loss.
    """
    z1 = F.normalize(z1, p=2, dim=1)
    z2 = F.normalize(z2, p=2, dim=1)
    batch_size = z1.size(0)
    z = torch.cat([z1, z2], dim=0)
    similarity_matrix = torch.matmul(z, z.T) / temperature
    mask = ~torch.eye(2 * batch_size, device=z.device).bool()
    positives = torch.cat([torch.diag(similarity_matrix, batch_size), torch.diag(similarity_matrix, -batch_size)])
    negatives = similarity_matrix.masked_select(mask).view(2 * batch_size, -1)

    numerator = torch.exp(positives)
    denominator = torch.sum(torch.exp(negatives), dim=-1)
    return -torch.mean(torch.log(numerator / denominator))

# NT-Xent Loss Class
class NTXentLoss(nn.Module):
    def __init__(self, temperature=0.5):
        """
        NT-Xent Loss module for contrastive learning.

        Args:
            temperature (float): Scaling factor for similarity scores.
        """
        super(NTXentLoss, self).__init__()
        self.temperature = temperature

    def forward(self, z_i, z_j):
        batch_size = z_i.size(0)

        z_i = F.normalize(z_i, dim=1)
        z_j = F.normalize(z_j, dim=1)

        z = torch.cat([z_i, z_j], dim=0)
        similarity_matrix = torch.matmul(z, z.T) / self.temperature

        mask = ~torch.eye(2 * batch_size, device=z.device).bool()
        positives = torch.cat([
            torch.diag(similarity_matrix, batch_size),
            torch.diag(similarity_matrix, -batch_size)
        ])
        negatives = similarity_matrix.masked_select(mask).view(2 * batch_size, -1)

        numerator = torch.exp(positives)
        denominator = torch.sum(torch.exp(negatives), dim=-1)
        return -torch.mean(torch.log(numerator / denominator))

# Triplet Loss
class TripletLoss(nn.Module):
    def __init__(self, margin=1.0):
        """
        Triplet Loss module for metric learning.

        Args:
            margin (float): Margin for triplet loss.
        """
        super(TripletLoss, self).__init__()
        self.margin = margin
        self.criterion = nn.TripletMarginWithDistanceLoss(
            distance_function=lambda a, b: 1.0 - F.cosine_similarity(a, b),
            margin=self.margin
        )

    def forward(self, anchor, positive, negative):
        return self.criterion(anchor, positive, negative)

# Contrastive Head
class ContrastiveHead(nn.Module):
    """
    Contrastive learning projection head for embeddings.
    """
    def __init__(self, input_dim, projection_dim=128):
        super(ContrastiveHead, self).__init__()
        self.projector = nn.Sequential(
            nn.Linear(input_dim, projection_dim),
            nn.ReLU(),
            nn.Linear(projection_dim, projection_dim)
        )

    def forward(self, x):
        return self.projector(x)

# Augmented NT-Xent Loss

def compute_nt_xent_loss_with_augmentation(images, model, contrastive_head, temperature=0.5):
    """
    Computes NT-Xent loss with data augmentation.

    Args:
        images (torch.Tensor): Input images.
        model (nn.Module): Base model for feature extraction.
        contrastive_head (ContrastiveHead): Projection head for embeddings.
        temperature (float): Scaling factor for similarity scores.

    Returns:
        torch.Tensor: Computed NT-Xent loss.
    """
    image1 = torch.stack([transform(image) for image in images])
    image2 = torch.stack([transform(image) for image in images])
    z1 = contrastive_head(model(image1))
    z2 = contrastive_head(model(image2))
    loss_fn = NTXentLoss(temperature=temperature)
    return loss_fn(z1, z2)

# Augmented Triplet Loss

def compute_triplet_loss_with_augmentation(images, model, contrastive_head, margin=1.0):
    """
    Computes Triplet loss with data augmentation.

    Args:
        images (torch.Tensor): Input images.
        model (nn.Module): Base model for feature extraction.
        contrastive_head (ContrastiveHead): Projection head for embeddings.
        margin (float): Margin for triplet loss.

    Returns:
        torch.Tensor: Computed Triplet loss.
    """
    indices = torch.randperm(images.size(0))
    anchor_images = images
    positive_images = images[indices]
    negative_images = images[torch.randperm(images.size(0))]

    anchor_images = torch.stack([transform(image) for image in anchor_images])
    positive_images = torch.stack([transform(image) for image in positive_images])
    negative_images = torch.stack([transform(image) for image in negative_images])

    anchor_embeddings = contrastive_head(model(anchor_images))
    positive_embeddings = contrastive_head(model(positive_images))
    negative_embeddings = contrastive_head(model(negative_images))

    loss_fn = TripletLoss(margin=margin)
    return loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)