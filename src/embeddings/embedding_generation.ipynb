{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Embedding Generation Notebook\n","\n","## Objective\n","This notebook demonstrates how to use custom encoder models to generate embeddings from the MNIST dataset. These models are implemented in `encoder_models.py`, and the training processes are defined in `encoder_training.py`.\n","\n","## Workflow\n","1. **Load and Preprocess Data**:\n","   - Load the MNIST dataset for testing the embedding generation process.\n","   - Normalize and prepare the data.\n","2. **Model Selection and Training**:\n","   - Train selected encoder models from `encoder_models.py`.\n","   - Generate embeddings from the bottleneck layer.\n","3. **Feature Extraction**:\n","   - Generate embeddings using matrix factorization (PCA, SVD, NMF) and SIFT.\n","4. **Save Embeddings**:\n","   - Save all embeddings and trained models for reuse.\n","\n","## Models and Methods\n","### Supported Models\n","The following encoder models are available for training and embedding generation. Each model is implemented in `encoder_models.py`:\n","- **Encoder Models**:\n","  - BasicAutoencoder, IntermediateAutoencoder, AdvancedAutoencoder, EnhancedAutoencoder.\n","  - BasicVAE, VAEWithFCDecoder, ImprovedVAE, FlexibleVAE.\n","- **Feature Extraction**:\n","  - PCA, SVD, NMF.\n","  - SIFT, Kernel PCA.\n","\n","#### **Autoencoders**\n","1. **BasicAutoencoder**:\n","   - A simple autoencoder with:\n","     - **Encoder**: Two convolutional layers followed by max-pooling.\n","     - **Decoder**: Two transposed convolutional layers to reconstruct the input.\n","   - Designed for grayscale datasets like MNIST.\n","   - Suitable for basic dimensionality reduction and reconstruction tasks.\n","\n","2. **IntermediateAutoencoder**:\n","   - A deeper autoencoder with:\n","     - **Batch Normalization** for improved stability.\n","     - Additional feature maps for a more expressive latent space.\n","   - Designed for moderately complex embedding tasks requiring better feature extraction.\n","\n","3. **AdvancedAutoencoder**:\n","   - A sophisticated autoencoder with:\n","     - **Skip Connections** to improve gradient flow and reconstruction accuracy.\n","     - **LeakyReLU Activations** and Batch Normalization for robust performance.\n","   - Suitable for high-dimensional or structured data requiring detailed reconstruction.\n","\n","4. **EnhancedAutoencoder**:\n","   - A deeper autoencoder with:\n","     - Additional convolutional layers in the encoder.\n","     - Transposed convolutional layers in the decoder.\n","     - LeakyReLU activations and Batch Normalization for better embedding representation.\n","   - Designed for datasets requiring intricate reconstructions under noisy conditions.\n","\n","#### **Variational Autoencoders (VAEs)**\n","5. **BasicVAE**:\n","   - A simple VAE with:\n","     - **Encoder**: Two convolutional layers and a fully connected layer to parameterize the latent space.\n","     - **Decoder**: Fully connected and transposed convolution layers to reconstruct input images.\n","   - Suitable for generative tasks with simple latent spaces.\n","\n","6. **VAEWithFCDecoder**:\n","   - A VAE with a fully connected decoder for enhanced latent-to-feature mapping.\n","   - Features:\n","     - **Encoder**: Convolutional layers with Batch Normalization.\n","     - **Decoder**: A combination of fully connected and transposed convolutional layers.\n","\n","7. **ImprovedVAE**:\n","   - An advanced VAE with:\n","     - A bottleneck layer for enhanced feature extraction.\n","     - Transposed convolutions for smooth reconstructions.\n","     - KL divergence loss for latent space regularization.\n","   - Designed for datasets requiring expressive latent representations.\n","\n","8. **FlexibleVAE**:\n","   - A flexible VAE that supports dynamic input shapes and optional projection heads for contrastive learning.\n","   - Suitable for embedding tasks with varying input dimensions.\n","\n","9. **ImprovedFlexibleVAE**:\n","   - Combines convolutional and fully connected layers in the encoder.\n","   - Uses transposed convolutions in the decoder for better reconstruction.\n","   - Optional **Projection Head** for self-supervised contrastive learning tasks.\n","\n","#### **Denoising Autoencoders**\n","10. **DenoisingAutoencoder**:\n","    - A denoising autoencoder with:\n","      - **Encoder**: Convolutional layers for feature extraction.\n","      - **Decoder**: Transposed convolutional layers for reconstruction.\n","      - Optional **Projection Head** for contrastive learning.\n","    - Supports two architectures:\n","      - **Basic**: Simpler structure for standard denoising tasks.\n","      - **Strong**: Deeper architecture for challenging noisy datasets.\n","\n","#### **Feature Extraction and Normalizing Flow Models**\n","11. **Matrix Factorization**:\n","    - Embeddings generated using PCA, SVD, and NMF.\n","    - Useful for dimensionality reduction and compact representations.\n","\n","12. **SIFT (Scale-Invariant Feature Transform)**:\n","    - Extracts scale-invariant features from images.\n","    - Pads feature descriptors to ensure consistent dimensionality.\n","\n","13. **Kernel PCA**:\n","    - Nonlinear dimensionality reduction using Kernel PCA with adjustable kernels.\n","\n","14. **Normalizing Flow Models**:\n","    - Transforms embeddings into a latent space using invertible transformations.\n","    - Useful for embedding refinement and generative tasks.\n","\n","\n","**Training**:\n","   - Each model is trained using the corresponding training loop defined in `encoder_training.py`.\n","   - Training includes support for reconstruction loss, KL divergence (for VAE), and optional noise injection.\n","**Embedding Generation**:\n","   - Once the models are trained, embeddings are generated for the MNIST dataset.\n","   - Encodings from the bottleneck layer are extracted for downstream tasks.\n","**Results Storage**:\n","   - Save trained models to `.pth` files.\n","   - Save generated embeddings to `.pt` files for reuse in downstream applications.\n","\n","## Supported Features\n","- **Flexible Model Selection**:\n","  - Choose specific models to train and generate embeddings for, bypassing others if needed.\n","- **Custom Configuration**:\n","  - Easily modify parameters like the bottleneck size (`code_dim`), number of training epochs, and learning rates.\n","\n","## Outputs\n","- Trained models saved as `.pth` files.\n","- Generated embeddings saved as `.pt` files in a structured directory (`./embeddings`).\n","\n","## Notes\n","This notebook is designed for flexibility and reusability. You can:\n","- Add new encoder models in `encoder_models.py`.\n","- Customize training loops in `encoder_training.py`.\n","- Modify this notebook to train specific models or generate embeddings for specific datasets.\n"],"metadata":{"id":"VUQM0eOdRPMH"}},{"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","# Mount Google Drive and set repository path\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Repository path (adjust if needed)\n","repo_path = \"/content/drive/MyDrive/GAN-thesis-project\"\n","\n","# Add repository path to sys.path for module imports\n","if repo_path not in sys.path:\n","    sys.path.append(repo_path)\n","\n","# Change working directory to the repository\n","os.chdir(repo_path)\n","\n","# Verify the working directory\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","# Configuration\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLvDZtTSZFUb","executionInfo":{"status":"ok","timestamp":1737811903437,"user_tz":-210,"elapsed":52751,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"0135ac05-8c04-4337-c319-2aac459dc779"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/GAN-thesis-project\n","Using device: cpu\n"]}]},{"cell_type":"code","source":["import inspect\n","\n","# Import the entire modules\n","import src.data_utils as data_utils\n","import src.cl_loss_function as cl_loss\n","import src.losses as losses\n","import src.embeddings.encoder_models as encoder_models\n","import src.embeddings.encoder_training as encoder_training\n","\n","# Function to list functions and classes in a module\n","def list_functions_and_classes(module):\n","    members = inspect.getmembers(module)\n","    functions = [name for name, obj in members if inspect.isfunction(obj)]\n","    classes = [name for name, obj in members if inspect.isclass(obj)]\n","    return functions, classes\n","\n","# Function to print functions and classes in a readable format\n","def print_functions_and_classes(module_name, module):\n","    functions, classes = list_functions_and_classes(module)\n","    print(f\"Module: {module_name}\")\n","    print(\"  Functions:\")\n","    for func in functions:\n","        print(f\"    - {func}\")\n","    print(\"  Classes:\")\n","    for cls in classes:\n","        print(f\"    - {cls}\")\n","    print()  # Add a blank line for separation\n","\n","# Print functions and classes for each module\n","print_functions_and_classes(\"src.data_utils\", data_utils)\n","print_functions_and_classes(\"src.cl_loss_function\", cl_loss)\n","print_functions_and_classes(\"src.losses\", losses)\n","print_functions_and_classes(\"src.embeddings.encoder_models\", encoder_models)\n","print_functions_and_classes(\"src.embeddings.encoder_training\", encoder_training)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jah7kWk-SpUb","executionInfo":{"status":"ok","timestamp":1737811910006,"user_tz":-210,"elapsed":6572,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"84a82c68-e6a4-48ea-9745-7b2c4948a001"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Module: src.data_utils\n","  Functions:\n","    - analyze_embeddings\n","    - analyze_embeddings_v2\n","    - create_dataloader\n","    - create_embedding_loaders\n","    - kurtosis\n","    - load_data\n","    - load_embeddings\n","    - load_mnist_data\n","    - pdist\n","    - preprocess_images\n","    - save_embeddings\n","    - skew\n","    - split_dataset\n","    - train_test_split\n","    - visualize_embeddings\n","  Classes:\n","    - DataLoader\n","    - LocalOutlierFactor\n","    - TensorDataset\n","\n","Module: src.cl_loss_function\n","  Functions:\n","    - augment\n","    - compute_nt_xent_loss_with_augmentation\n","    - compute_triplet_loss_with_augmentation\n","    - contrastive_loss\n","    - hflip\n","    - info_nce_loss\n","    - resize\n","  Classes:\n","    - ContrastiveHead\n","    - DataLoader\n","    - NTXentLoss\n","    - PCA\n","    - TensorDataset\n","    - TripletLoss\n","    - VicRegLoss\n","\n","Module: src.losses\n","  Functions:\n","    - add_noise\n","    - cyclical_beta_schedule\n","    - linear_beta_schedule\n","    - loss_function_dae_ssim\n","    - vae_loss\n","    - vae_ssim_loss\n","  Classes:\n","\n","Module: src.embeddings.encoder_models\n","  Functions:\n","    - apply_dimensionality_reduction\n","    - apply_sift\n","    - init_weights\n","    - log_prob\n","    - process_feature_extraction\n","    - process_matrix_factorization\n","    - refine_embeddings_NF\n","    - train_nf_model\n","  Classes:\n","    - AdvancedAutoencoder\n","    - BasicAutoencoder\n","    - BasicVAE\n","    - DataLoader\n","    - DenoisingAutoencoder\n","    - EnhancedAutoencoder\n","    - FlexibleVAE\n","    - FlowLayer\n","    - ImprovedFlexibleVAE\n","    - ImprovedVAE\n","    - IntermediateAutoencoder\n","    - KernelPCA\n","    - MinMaxScaler\n","    - NMF\n","    - NormalizingFlowModel\n","    - PCA\n","    - ProjectionHead\n","    - SimCLR\n","    - StandardScaler\n","    - TensorDataset\n","    - TruncatedSVD\n","    - tqdm\n","\n","Module: src.embeddings.encoder_training\n","  Functions:\n","    - add_noise\n","    - ssim\n","    - train_autoencoder\n","    - train_dae\n","    - train_simclr\n","    - train_vae\n","  Classes:\n","    - DataLoader\n","    - EarlyStopping\n","    - MinMaxScaler\n","    - StandardScaler\n","    - TensorDataset\n","    - ToTensor\n","    - tqdm\n","\n"]}]},{"cell_type":"code","source":["# Load and Preprocess MNIST Data\n","fraction = 1  # Fraction of the dataset to use\n","batch_size = 64\n","\n","# Load Combined MNIST Dataset (Train + Test)\n","mnist_loader = data_utils.load_mnist_data(fraction=fraction, batch_size=batch_size, shuffle=True)\n","\n","# Inspect Combined Dataset\n","for batch in mnist_loader:\n","    images, labels = batch\n","    print(\"Batch Shape:\", images.shape, labels.shape)\n","    break\n","\n","# Visualize Original Images\n","n = 30\n","sample_indices = np.random.choice(len(mnist_loader.dataset), n, replace=False)\n","sampled_images = mnist_loader.dataset.tensors[0][sample_indices].numpy()\n","sampled_images = (sampled_images * 127.5 + 127.5).astype(np.uint8).squeeze()  # Denormalize for display\n","\n","plt.figure(figsize=(20, 6))\n","for i in range(n):\n","    ax = plt.subplot(3, 10, i + 1)\n","    plt.imshow(sampled_images[i], cmap=\"gray\")\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()\n"],"metadata":{"id":"9ByfVYCjeT7P","colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"status":"ok","timestamp":1737811913662,"user_tz":-210,"elapsed":3659,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"23b245c6-3ff3-482e-8fe0-b77411c7f711"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (70000, 1, 28, 28) (70000,)\n","Batch Shape: torch.Size([64, 1, 28, 28]) torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 2000x600 with 30 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoder\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"vae\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"dae\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels"],"metadata":{"id":"lC_8uAUF7t0r","executionInfo":{"status":"ok","timestamp":1737811913662,"user_tz":-210,"elapsed":9,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# class NTXentLoss(nn.Module):\n","#     \"\"\"\n","#     NT-Xent (Normalized Temperature-scaled Cross Entropy) Loss for contrastive learning.\n","\n","#     Args:\n","#         temperature (float): Scaling factor for similarity scores.\n","#     \"\"\"\n","#     def __init__(self, temperature=0.5):\n","#         super(NTXentLoss, self).__init__()\n","#         self.temperature = temperature\n","\n","#     def forward(self, z_i, z_j):\n","#         \"\"\"\n","#         Compute the NT-Xent loss between two sets of embeddings.\n","\n","#         Args:\n","#             z_i (torch.Tensor): First set of embeddings.\n","#             z_j (torch.Tensor): Second set of embeddings.\n","\n","#         Returns:\n","#             torch.Tensor: Computed NT-Xent loss.\n","#         \"\"\"\n","#         batch_size = z_i.size(0)\n","\n","#         # Normalize embeddings\n","#         z_i = F.normalize(z_i, dim=1)\n","#         z_j = F.normalize(z_j, dim=1)\n","\n","#         print(f\"Shape of z_i: {z_i.shape}\")\n","#         print(f\"Shape of z_j: {z_j.shape}\")\n","\n","#         # Concatenate embeddings\n","#         z = torch.cat([z_i, z_j], dim=0)\n","#         print(f\"Shape of z: {z.shape}\")\n","#         print(f\"Shape of zT: {z.T.shape}\")\n","#         # Compute similarity matrix\n","#         similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","\n","#         # Mask for positives and negatives\n","#         mask = torch.eye(2 * batch_size, device=z.device).bool()  # Mask for self-similarities\n","#         positives = torch.cat([\n","#             torch.diag(similarity_matrix, batch_size),  # Similarity between z_i and z_j\n","#             torch.diag(similarity_matrix, -batch_size)  # Similarity between z_j and z_i\n","#         ])\n","\n","#         # Mask out self-similarities and positives\n","#         negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n","\n","#         # Compute NT-Xent loss\n","#         numerator = torch.exp(positives)\n","#         denominator = torch.sum(torch.exp(negatives), dim=-1)\n","\n","#         # Avoid numerical instability by using log-sum-exp trick\n","#         loss = -torch.log(numerator / denominator).mean()\n","\n","#         return loss\n","\n","class NTXentLoss(nn.Module):\n","    def __init__(self, temperature=0.5):\n","        super(NTXentLoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, z_i, z_j):\n","        batch_size = z_i.size(0)\n","\n","        # Normalize embeddings\n","        z_i = F.normalize(z_i, dim=0)\n","        z_j = F.normalize(z_j, dim=0)\n","\n","        # Concatenate embeddings\n","        z = torch.cat([z_i, z_j], dim=0)\n","        print(f\"Shape of z: {z_i.shape}\")\n","        print(f\"Shape of zT: {z_j.shape}\")\n","        print(f\"Shape of z: {z.shape}\")\n","        print(f\"Shape of zT: {z.T.shape}\")\n","        # Compute similarity matrix\n","        similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","\n","        # Mask for positives and negatives\n","        mask = torch.eye(2 * batch_size, device=z.device).bool()  # Mask for self-similarities\n","        positives = torch.cat([\n","            torch.diag(similarity_matrix, batch_size),\n","            torch.diag(similarity_matrix, -batch_size)\n","        ])\n","\n","        # Mask out self-similarities and positives\n","        negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n","\n","        # Compute NT-Xent loss\n","        numerator = torch.exp(positives)\n","        denominator = torch.sum(torch.exp(negatives), dim=-1)\n","        loss = -torch.log(numerator / denominator).mean()\n","\n","        return loss\n"],"metadata":{"id":"Y8LPTX8iKf1j","executionInfo":{"status":"ok","timestamp":1737811913663,"user_tz":-210,"elapsed":8,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Define Configuration\n","# ------------------------------\n","\n","# Configuration\n","config = {\n","    \"model_type\": \"autoencoder\",  # Options: \"autoencoder\", \"vae\", \"dae\"\n","    \"model_name\": \"EnhancedAutoencoder\",  # Options: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\", \"BasicVAE\", \"ImprovedVAE\", \"FlexibleVAE\", \"ImprovedFlexibleVAE\", \"DenoisingAutoencoder\"\n","    \"code_dim\": 50,  # Dimensionality of the embedding\n","    \"loss_type\": \"mse\",  # Options: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","    \"noise_factor\": 0.1,  # Noise factor for denoising autoencoders\n","    \"temperature\": 0.5,  # Temperature parameter for NT-Xent loss\n","    \"margin\": 1.0,  # Margin for Triplet Loss\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-3,\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    \"save_best\": True,  # Whether to save the best model\n","    \"save_path\": \"best_model.pth\",  # Path to save the best model\n","    \"beta\": 1.0,  # Weight for KL divergence (VAE only)\n","    \"alpha\": 0.5,  # Weight for contrastive or triplet loss\n","    \"fraction\": 0.01,  # Fraction of the dataset to use\n","    \"projection_dim\": None,  # Optional projection head dimension for VAEs\n","    \"strong_architecture\": False,  # Whether to use a deeper architecture for DenoisingAutoencoder\n","    \"input_shape\": (1, 28, 28),  # Input shape for FlexibleVAE and ImprovedFlexibleVAE\n","    \"patience\": 5,\n","    \"min_delta\": 0.001,\n","}\n","\n","# ------------------------------\n","# Step 2: Load and Preprocess Data\n","# ------------------------------\n","\n","# Load Combined MNIST Dataset (Train + Test)\n","mnist_loader = data_utils.load_mnist_data(fraction=config[\"fraction\"], batch_size=config[\"batch_size\"], shuffle=True)\n","\n","# Inspect Combined Dataset\n","for batch in mnist_loader:\n","    images, labels = batch\n","    print(\"Batch Shape:\", images.shape, labels.shape)\n","    break\n","\n","# Visualize Original Images\n","n = 20\n","sample_indices = np.random.choice(len(mnist_loader.dataset), n, replace=False)\n","sampled_images = mnist_loader.dataset.tensors[0][sample_indices].numpy()\n","sampled_images = (sampled_images * 127.5 + 127.5).astype(np.uint8).squeeze()  # Denormalize for display\n","\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    ax = plt.subplot(2, 10, i + 1)\n","    plt.imshow(sampled_images[i], cmap=\"gray\")\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()\n","\n","# ------------------------------\n","# Step 3: Initialize Model, Loss, and Optimizer\n","# ------------------------------\n","\n","# Initialize the model\n","model_classes = {\n","    \"BasicAutoencoder\": encoder_models.BasicAutoencoder,\n","    \"IntermediateAutoencoder\": encoder_models.IntermediateAutoencoder,\n","    \"AdvancedAutoencoder\": encoder_models.AdvancedAutoencoder,\n","    \"EnhancedAutoencoder\": encoder_models.EnhancedAutoencoder,\n","    \"BasicVAE\": encoder_models.BasicVAE,\n","    \"ImprovedVAE\": encoder_models.ImprovedVAE,\n","    \"FlexibleVAE\": encoder_models.FlexibleVAE,\n","    \"ImprovedFlexibleVAE\": encoder_models.ImprovedFlexibleVAE,\n","    \"DenoisingAutoencoder\": encoder_models.DenoisingAutoencoder,\n","}\n","\n","# Initialize model with appropriate arguments\n","if config[\"model_name\"] in [\"FlexibleVAE\", \"ImprovedFlexibleVAE\"]:\n","    model = model_classes[config[\"model_name\"]](\n","        input_shape=config[\"input_shape\"],\n","        code_dim=config[\"code_dim\"],\n","        projection_dim=config[\"projection_dim\"]\n","    ).to(config[\"device\"])\n","elif config[\"model_name\"] == \"DenoisingAutoencoder\":\n","    model = model_classes[config[\"model_name\"]](\n","        code_dim=config[\"code_dim\"],\n","        projection_dim=config[\"projection_dim\"],\n","        strong_architecture=config[\"strong_architecture\"]\n","    ).to(config[\"device\"])\n","else:\n","    model = model_classes[config[\"model_name\"]](\n","        code_dim=config[\"code_dim\"]\n","    ).to(config[\"device\"])\n","\n","# # Define the loss function\n","# loss_functions = {\n","#     \"mse\": nn.MSELoss(),  # Reconstruction loss\n","#     \"vae_loss\": losses.vae_loss,  # VAE loss\n","#     \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","#     \"ntxent\": cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","#     \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","# }\n","# criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the loss function\n","if config[\"model_type\"] == \"vae\":\n","    criterion = losses.vae_loss  # Use VAE loss for VAEs\n","else:\n","    loss_functions = {\n","        \"mse\": nn.MSELoss(),  # Reconstruction loss\n","        \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","        \"ntxent\": NTXentLoss(temperature=config[\"temperature\"]),\n","        # cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","        \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","    }\n","    criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","\n","# Define scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 4: Train the Model\n","# ------------------------------\n","\n","if config[\"model_type\"] == \"autoencoder\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    encoder_training.train_autoencoder(\n","        model=model,\n","        data_loader=mnist_loader,\n","        loss_fn=criterion,\n","        optimizer=optimizer,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        noise_factor=config[\"noise_factor\"],\n","        scheduler=scheduler,\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\", \"triplet\"] else None,\n","        triplet_data=(config[\"loss_type\"] == \"triplet\"),\n","        augment_fn=cl_loss.augment if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","        patience=config[\"patience\"],\n","        min_delta=config[\"min_delta\"],\n","    )\n","\n","elif config[\"model_type\"] == \"vae\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    encoder_training.train_vae(\n","        vae=model,\n","        train_loader=mnist_loader,\n","        optimizer=optimizer,\n","        loss_fn=criterion,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        val_loader=None,  # No validation loader for simplicity\n","        scheduler=scheduler,\n","        save_best=config[\"save_best\"],\n","        save_path=config[\"save_path\"],\n","        beta=config[\"beta\"],\n","        alpha=config[\"alpha\"],\n","        temperature=config[\"temperature\"],\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","        patience=config[\"patience\"],\n","        min_delta=config[\"min_delta\"],\n","    )\n","\n","elif config[\"model_type\"] == \"dae\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    encoder_training.train_dae(\n","        dae=model,\n","        train_loader=mnist_loader,\n","        optimizer=optimizer,\n","        loss_fn=criterion,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        val_loader=None,  # No validation loader for simplicity\n","        scheduler=scheduler,\n","        save_best=config[\"save_best\"],\n","        save_path=config[\"save_path\"],\n","        noise_factor=config[\"noise_factor\"],\n","        alpha=config[\"alpha\"],\n","        temperature=config[\"temperature\"],\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","        triplet_loss_fn=criterion if config[\"loss_type\"] == \"triplet\" else None,\n","        ssim_func=losses.ssim if config[\"loss_type\"] == \"ssim\" else None,\n","        patience=config[\"patience\"],\n","        min_delta=config[\"min_delta\"],\n","    )\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Generate embeddings\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    embedding_type=config[\"model_type\"],\n","    data_loader=mnist_loader,\n","    device=config[\"device\"],\n",")\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{config['model_type']}_{config['model_name']}_{config['loss_type']}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n","\n","# ------------------------------\n","# Step 6: Visualize Embeddings\n","# ------------------------------\n","\n","# Visualize embeddings\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zAKUO7472Gn7","executionInfo":{"status":"ok","timestamp":1737812015891,"user_tz":-210,"elapsed":102235,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"fe62054f-5dbf-440a-ca0b-e35f357edcf1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (700, 1, 28, 28) (700,)\n","Batch Shape: torch.Size([64, 1, 28, 28]) torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 2000x400 with 20 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training EnhancedAutoencoder with mse loss...\n","Epoch [1/100], Train Loss: 0.7872\n","Epoch [2/100], Train Loss: 0.4579\n","Epoch [3/100], Train Loss: 0.4013\n","Epoch [4/100], Train Loss: 0.3511\n","Epoch [5/100], Train Loss: 0.3019\n","Epoch [6/100], Train Loss: 0.2646\n","Epoch [7/100], Train Loss: 0.2353\n","Epoch [8/100], Train Loss: 0.2117\n","Epoch [9/100], Train Loss: 0.1887\n","Epoch [10/100], Train Loss: 0.1699\n","Epoch [11/100], Train Loss: 0.1586\n","Epoch [12/100], Train Loss: 0.1537\n","Epoch [13/100], Train Loss: 0.1330\n","Epoch [14/100], Train Loss: 0.1261\n","Epoch [15/100], Train Loss: 0.1133\n","Epoch [16/100], Train Loss: 0.1014\n","Epoch [17/100], Train Loss: 0.0951\n","Epoch [18/100], Train Loss: 0.0874\n","Epoch [19/100], Train Loss: 0.0844\n","Epoch [20/100], Train Loss: 0.0795\n","Epoch [21/100], Train Loss: 0.0742\n","Epoch [22/100], Train Loss: 0.0712\n","Epoch [23/100], Train Loss: 0.0668\n","Epoch [24/100], Train Loss: 0.0649\n","Epoch [25/100], Train Loss: 0.0659\n","Epoch [26/100], Train Loss: 0.0636\n","Epoch [27/100], Train Loss: 0.0631\n","Epoch [28/100], Train Loss: 0.0620\n","Epoch [29/100], Train Loss: 0.0621\n","Epoch [30/100], Train Loss: 0.0604\n","Epoch [31/100], Train Loss: 0.0596\n","Epoch [32/100], Train Loss: 0.0597\n","Epoch [33/100], Train Loss: 0.0602\n","Epoch [34/100], Train Loss: 0.0580\n","Epoch [35/100], Train Loss: 0.0619\n","Epoch [36/100], Train Loss: 0.0593\n","Epoch [37/100], Train Loss: 0.0581\n","Epoch [38/100], Train Loss: 0.0568\n","Epoch [39/100], Train Loss: 0.0576\n","Epoch [40/100], Train Loss: 0.0603\n","Epoch [41/100], Train Loss: 0.0551\n","Epoch [42/100], Train Loss: 0.0545\n","Epoch [43/100], Train Loss: 0.0556\n","Epoch [44/100], Train Loss: 0.0570\n","Epoch [45/100], Train Loss: 0.0565\n","Epoch [46/100], Train Loss: 0.0547\n","Early stopping triggered at epoch 46.\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoder_EnhancedAutoencoder_mse/EnhancedAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoder_EnhancedAutoencoder_mse/EnhancedAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Define Configuration\n","# ------------------------------\n","\n","# Configuration\n","config = {\n","    \"model_name\": \"IntermediateAutoencoder\",  # Options: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"\n","    \"code_dim\": 50,  # Dimensionality of the embedding\n","    \"embedding_type\": \"autoencoders\",  # Options: \"autoencoders\", \"VAEs\", \"denoising_autoencoders\"\n","    \"loss_type\": \"mse\",  # Options: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","    \"noise_factor\": 0.0,  # Noise factor for denoising autoencoders\n","    \"temperature\": 0.5,  # Temperature parameter for NT-Xent loss\n","    \"margin\": 1.0,  # Margin for Triplet Loss\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-3,\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","}\n","\n","# ------------------------------\n","# Step 2: Load and Preprocess Data\n","# ------------------------------\n","\n","# Load MNIST data\n","train_data, test_data = data_utils.load_mnist_data()\n","\n","# Preprocess images\n","train_data = data_utils.preprocess_images(train_data)\n","test_data = data_utils.preprocess_images(test_data)\n","\n","# Create DataLoader\n","train_loader = data_utils.create_dataloader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n","test_loader = data_utils.create_dataloader(test_data, batch_size=config[\"batch_size\"], shuffle=False)\n","\n","# ------------------------------\n","# Step 3: Initialize Model, Loss, and Optimizer\n","# ------------------------------\n","\n","# Initialize the model\n","model_classes = {\n","    \"BasicAutoencoder\": encoder_models.BasicAutoencoder,\n","    \"IntermediateAutoencoder\": encoder_models.IntermediateAutoencoder,\n","    \"AdvancedAutoencoder\": encoder_models.AdvancedAutoencoder,\n","    \"EnhancedAutoencoder\": encoder_models.EnhancedAutoencoder,\n","}\n","model = model_classes[config[\"model_name\"]](code_dim=config[\"code_dim\"]).to(config[\"device\"])\n","\n","# Define the loss function\n","loss_functions = {\n","    \"mse\": nn.MSELoss(),  # Reconstruction loss\n","    \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","    \"ntxent\": cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","    \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","}\n","criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","\n","# Define scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 4: Train the Model\n","# ------------------------------\n","\n","print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","encoder_training.train_autoencoder(\n","    model=model,\n","    data_loader=train_loader,\n","    loss_fn=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    epochs=config[\"epochs\"],\n","    device=config[\"device\"],\n","    noise_factor=config[\"noise_factor\"] if config[\"embedding_type\"] == \"denoising_autoencoders\" else 0.0,\n","    contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\", \"triplet\"] else None,\n","    augment_fn=cl_loss.augment if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","    triplet_data=(config[\"loss_type\"] == \"triplet\"),\n",")\n","\n","# ------------------------------\n","# Step 5: Generate and Save Embeddings\n","# ------------------------------\n","\n","# Generate embeddings\n","embeddings, labels = encoder_training.generate_embeddings(\n","    model=model,\n","    data_loader=test_loader,\n","    device=config[\"device\"],\n",")\n","\n","# Save embeddings\n","data_utils.save_embeddings(\n","    embeddings=embeddings,\n","    labels=labels,\n","    base_dir=\"./saved_embeddings\",\n","    model_name=config[\"model_name\"],\n","    loss_type=config[\"loss_type\"],\n","    embedding_type=config[\"embedding_type\"],\n","    save_format=\"pt\",  # Options: \"pt\" (PyTorch) or \"npy\" (NumPy)\n",")\n","\n","# Save the model\n","model_file = os.path.join(\"./saved_embeddings\", f\"{config['model_name']}_{config['loss_type']}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n","\n","# ------------------------------\n","# Step 6: Visualize Embeddings\n","# ------------------------------\n","\n","# Visualize embeddings\n","data_utils.visualize_embeddings(embeddings, labels, title=f\"{config['model_name']} Embeddings\")"],"metadata":{"id":"3tEV_wCe0DSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EnhancedAutoencoder(nn.Module):\n","    \"\"\"\n","    A deep autoencoder with advanced reconstruction capabilities.\n","\n","    Features:\n","    - Deeper architecture with additional convolutional and transposed convolutional layers.\n","    - Utilizes Batch Normalization and LeakyReLU activations.\n","    - Capable of learning highly expressive embeddings.\n","\n","    Designed for datasets requiring intricate reconstructions.\n","    \"\"\"\n","    def __init__(self, code_dim):\n","        super(EnhancedAutoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: (1, 28, 28)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","            nn.MaxPool2d(2, 2),  # Output: (32, 14, 14)\n","\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 14, 14)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d(2, 2),  # Output: (64, 7, 7)\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 7, 7)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d(2, 2)  # Output: (128, 3, 3)\n","        )\n","\n","        self.fc_encoder = nn.Linear(128 * 3 * 3, code_dim)\n","\n","        # Decoder with learned upsampling\n","        self.fc_decoder = nn.Linear(code_dim, 128 * 3 * 3)\n","        self.decoder = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Unflatten(1, (128, 3, 3)),\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Learned upsampling\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","\n","            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (1, 28, 28)\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Encoding\n","        encoded = self.encoder(x)\n","        encoded = encoded.view(batch_size, -1)\n","        encoded = self.fc_encoder(encoded)\n","\n","        # Decoding\n","        decoded = self.fc_decoder(encoded)\n","        decoded = self.decoder(decoded)\n","\n","        return encoded, decoded"],"metadata":{"id":"u56Uti9P5_ws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels"],"metadata":{"id":"_E5NCVi_6tOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","import numpy as np\n","from torch.utils.data import DataLoader\n","\n","# ------------------------------\n","# Step 1: Define Contrastive Loss Functions\n","# ------------------------------\n","\n","class NTXentLoss(nn.Module):\n","    def __init__(self, temperature=0.5):\n","        super(NTXentLoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, z_i, z_j):\n","        batch_size = z_i.size(0)\n","        z_i = F.normalize(z_i, dim=1)\n","        z_j = F.normalize(z_j, dim=1)\n","        z = torch.cat([z_i, z_j], dim=0)\n","        similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","        mask = ~torch.eye(2 * batch_size, device=z.device).bool()\n","        positives = torch.cat([torch.diag(similarity_matrix, batch_size), torch.diag(similarity_matrix, -batch_size)])\n","        negatives = similarity_matrix.masked_select(mask).view(2 * batch_size, -1)\n","        numerator = torch.exp(positives)\n","        denominator = torch.sum(torch.exp(negatives), dim=-1)\n","        return -torch.mean(torch.log(numerator / denominator))\n","\n","# VicReg Loss\n","class VicRegLoss(nn.Module):\n","    def __init__(self, lambda_var=25, mu_mean=25, nu_cov=1):\n","        super(VicRegLoss, self).__init__()\n","        self.lambda_var = lambda_var\n","        self.mu_mean = mu_mean\n","        self.nu_cov = nu_cov\n","\n","    def forward(self, z1, z2):\n","        variance_loss = torch.mean(torch.relu(1 - torch.std(z1, dim=0))) + torch.mean(torch.relu(1 - torch.std(z2, dim=0)))\n","        mean_loss = torch.mean((torch.mean(z1, dim=0) - torch.mean(z2, dim=0))**2)\n","        z1_centered = z1 - z1.mean(dim=0)\n","        z2_centered = z2 - z2.mean(dim=0)\n","        covariance_matrix_z1 = torch.mm(z1_centered.T, z1_centered) / (z1.size(0) - 1)\n","        covariance_matrix_z2 = torch.mm(z2_centered.T, z2_centered) / (z2.size(0) - 1)\n","        covariance_loss = torch.sum(covariance_matrix_z1 ** 2) - torch.sum(torch.diag(covariance_matrix_z1) ** 2) + \\\n","                          torch.sum(covariance_matrix_z2 ** 2) - torch.sum(torch.diag(covariance_matrix_z2) ** 2)\n","        return self.lambda_var * variance_loss + self.mu_mean * mean_loss + self.nu_cov * covariance_loss\n","\n","# Triplet Loss\n","class TripletLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(TripletLoss, self).__init__()\n","        self.margin = margin\n","        self.criterion = nn.TripletMarginWithDistanceLoss(\n","            distance_function=lambda a, b: 1.0 - F.cosine_similarity(a, b),\n","            margin=self.margin\n","        )\n","\n","    def forward(self, anchor, positive, negative):\n","        return self.criterion(anchor, positive, negative)\n","\n","# ------------------------------\n","# Step 2: Define Augmentation Function\n","# ------------------------------\n","\n","def augment(images):\n","    \"\"\"\n","    Apply tensor-based augmentations to images.\n","\n","    Args:\n","        images (torch.Tensor): Batch of images of shape (batch_size, 1, 28, 28).\n","\n","    Returns:\n","        torch.Tensor: Augmented images of the same shape as input.\n","    \"\"\"\n","    # Resize and crop (ensure output size matches input size)\n","    images = resize(images, size=[28, 28])\n","\n","    # Random horizontal flip\n","    if torch.rand(1) > 0.5:\n","        images = hflip(images)\n","\n","    return images\n","\n","# ------------------------------\n","# Step 3: Define Training Function\n","# ------------------------------\n","\n","def train_autoencoder(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    scheduler=None,\n","    epochs=10,\n","    device=\"cpu\",\n","    noise_factor=0.0,\n","    augment_fn=None,\n","    contrastive_loss_fn=None,\n","    temperature=0.5,\n","    triplet_data=False,\n","):\n","    \"\"\"\n","    Train an autoencoder model with support for contrastive learning, noise injection, and augmentations.\n","\n","    Args:\n","        model (nn.Module): The autoencoder model.\n","        data_loader (DataLoader): DataLoader for training data.\n","        loss_fn (callable): Primary loss function (e.g., reconstruction loss).\n","        optimizer (torch.optim.Optimizer): Optimizer for the model.\n","        scheduler (torch.optim.lr_scheduler._LRScheduler, optional): Learning rate scheduler.\n","        epochs (int): Number of epochs to train.\n","        device (str): Device to train on ('cpu' or 'cuda').\n","        noise_factor (float): Factor for adding noise to input images (denoising autoencoder).\n","        augment_fn (callable, optional): Augmentation function for contrastive learning.\n","        contrastive_loss_fn (callable, optional): Contrastive loss function (e.g., NT-Xent, triplet loss).\n","        temperature (float): Temperature parameter for NT-Xent loss.\n","        triplet_data (bool): Whether the data_loader provides triplets (anchor, positive, negative).\n","\n","    Returns:\n","        None: Prints loss values for each epoch.\n","    \"\"\"\n","    model.to(device).train()\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for batch in data_loader:\n","            # Prepare data based on whether it's triplet data or not\n","            if triplet_data:\n","                anchor, positive, negative = batch\n","                anchor, positive, negative = (\n","                    anchor.to(device).float(),\n","                    positive.to(device).float(),\n","                    negative.to(device).float(),\n","                )\n","                images = anchor  # Use anchor as the primary input for reconstruction\n","            else:\n","                images, _ = batch\n","                images = images.to(device).float()\n","\n","            # Add noise if specified\n","            if noise_factor > 0:\n","                noisy_images = images + noise_factor * torch.randn_like(images)\n","                noisy_images = torch.clamp(noisy_images, 0.0, 1.0)\n","                encoded, decoded = model(noisy_images)\n","            else:\n","                encoded, decoded = model(images)\n","\n","            # Compute reconstruction loss\n","            reconstruction_loss = loss_fn(decoded, images)\n","\n","            # Compute contrastive loss if specified\n","            contrastive_loss_value = 0\n","            if contrastive_loss_fn is not None:\n","                if triplet_data:\n","                    # Triplet loss\n","                    positive_encoded, _ = model(positive)\n","                    negative_encoded, _ = model(negative)\n","                    contrastive_loss_value = contrastive_loss_fn(encoded, positive_encoded, negative_encoded)\n","                else:\n","                    # NT-Xent or other contrastive loss\n","                    if augment_fn:\n","                        augmented_1 = augment_fn(images)\n","                        augmented_2 = augment_fn(images)\n","                        z1, _ = model(augmented_1)\n","                        z2, _ = model(augmented_2)\n","                    else:\n","                        z1, z2 = encoded, encoded  # Use the same embeddings if no augmentation\n","                    contrastive_loss_value = contrastive_loss_fn(z1, z2, temperature)\n","\n","            # Total loss\n","            total_loss_value = reconstruction_loss + contrastive_loss_value\n","\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            total_loss_value.backward()\n","            optimizer.step()\n","\n","            total_loss += total_loss_value.item()\n","\n","        # Step the scheduler if provided\n","        if scheduler:\n","            scheduler.step()\n","\n","        # Print epoch loss\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(data_loader):.4f}\")\n","\n","# ------------------------------\n","# Step 4: Configure Training Pipeline\n","# ------------------------------\n","\n","# ------------------------------\n","# Step 1: Model Selection and Parameters\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"IntermediateAutoencoder\"  # Choose from: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Choose from: \"autoencoders\", \"VAEs\", \"denoising_autoencoders\"\n","loss_type = \"mse\"  # Choose from: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","noise_factor = 0.0  # Noise factor for denoising autoencoders\n","temperature = 0.5  # Temperature parameter for NT-Xent loss\n","margin = 1.0  # Margin for Triplet Loss\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()  # Reconstruction loss\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1)  # VicReg loss\n","elif loss_type == \"ntxent\":\n","    criterion = NTXentLoss(temperature=temperature)  # NT-Xent loss\n","elif loss_type == \"triplet\":\n","    criterion = TripletLoss(margin=margin)  # Triplet loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 20 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 2: Training the Model\n","# ------------------------------\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_autoencoder(\n","    model=model,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    loss_fn=criterion,  # Primary loss function\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    epochs=100,\n","    device=device,\n","    noise_factor=noise_factor if embedding_type == \"denoising_autoencoders\" else 0.0,  # Add noise for denoising\n","    contrastive_loss_fn=criterion if loss_type in [\"vicreg\", \"ntxent\", \"triplet\"] else None,  # Optional: Contrastive loss\n","    augment_fn=augment if loss_type in [\"vicreg\", \"ntxent\"] else None,  # Optional: Augmentation function\n","    triplet_data=(loss_type == \"triplet\")  # Optional: Use triplet data\n",")\n","\n","# ------------------------------\n","# Step 3: Generate Embeddings\n","# ------------------------------\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    device=device\n",")\n","\n","# ------------------------------\n","# Step 4: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SDmzPRsGzDQp","executionInfo":{"status":"error","timestamp":1737562456314,"user_tz":-210,"elapsed":14409637,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"7135051e-61bf-4541-8aee-2f8252c3f03a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training IntermediateAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.0518\n","Epoch [2/100], Loss: 0.0172\n","Epoch [3/100], Loss: 0.0141\n","Epoch [4/100], Loss: 0.0125\n","Epoch [5/100], Loss: 0.0116\n","Epoch [6/100], Loss: 0.0108\n","Epoch [7/100], Loss: 0.0103\n","Epoch [8/100], Loss: 0.0100\n","Epoch [9/100], Loss: 0.0096\n","Epoch [10/100], Loss: 0.0093\n","Epoch [11/100], Loss: 0.0091\n","Epoch [12/100], Loss: 0.0089\n","Epoch [13/100], Loss: 0.0087\n","Epoch [14/100], Loss: 0.0086\n","Epoch [15/100], Loss: 0.0084\n","Epoch [16/100], Loss: 0.0083\n","Epoch [17/100], Loss: 0.0082\n","Epoch [18/100], Loss: 0.0081\n","Epoch [19/100], Loss: 0.0080\n","Epoch [20/100], Loss: 0.0080\n","Epoch [21/100], Loss: 0.0072\n","Epoch [22/100], Loss: 0.0071\n","Epoch [23/100], Loss: 0.0071\n","Epoch [24/100], Loss: 0.0071\n","Epoch [25/100], Loss: 0.0071\n","Epoch [26/100], Loss: 0.0071\n","Epoch [27/100], Loss: 0.0070\n","Epoch [28/100], Loss: 0.0070\n","Epoch [29/100], Loss: 0.0070\n","Epoch [30/100], Loss: 0.0070\n","Epoch [31/100], Loss: 0.0070\n","Epoch [32/100], Loss: 0.0070\n","Epoch [33/100], Loss: 0.0070\n","Epoch [34/100], Loss: 0.0070\n","Epoch [35/100], Loss: 0.0070\n","Epoch [36/100], Loss: 0.0069\n","Epoch [37/100], Loss: 0.0069\n","Epoch [38/100], Loss: 0.0069\n","Epoch [39/100], Loss: 0.0069\n","Epoch [40/100], Loss: 0.0069\n","Epoch [41/100], Loss: 0.0068\n","Epoch [42/100], Loss: 0.0068\n","Epoch [43/100], Loss: 0.0068\n","Epoch [44/100], Loss: 0.0068\n","Epoch [45/100], Loss: 0.0068\n","Epoch [46/100], Loss: 0.0068\n","Epoch [47/100], Loss: 0.0068\n","Epoch [48/100], Loss: 0.0068\n","Epoch [49/100], Loss: 0.0068\n","Epoch [50/100], Loss: 0.0068\n","Epoch [51/100], Loss: 0.0068\n","Epoch [52/100], Loss: 0.0068\n","Epoch [53/100], Loss: 0.0068\n","Epoch [54/100], Loss: 0.0068\n","Epoch [55/100], Loss: 0.0068\n","Epoch [56/100], Loss: 0.0068\n","Epoch [57/100], Loss: 0.0068\n","Epoch [58/100], Loss: 0.0068\n","Epoch [59/100], Loss: 0.0068\n","Epoch [60/100], Loss: 0.0068\n","Epoch [61/100], Loss: 0.0068\n","Epoch [62/100], Loss: 0.0068\n","Epoch [63/100], Loss: 0.0068\n","Epoch [64/100], Loss: 0.0068\n","Epoch [65/100], Loss: 0.0068\n","Epoch [66/100], Loss: 0.0068\n","Epoch [67/100], Loss: 0.0068\n","Epoch [68/100], Loss: 0.0068\n","Epoch [69/100], Loss: 0.0068\n","Epoch [70/100], Loss: 0.0068\n","Epoch [71/100], Loss: 0.0068\n","Epoch [72/100], Loss: 0.0068\n","Epoch [73/100], Loss: 0.0068\n","Epoch [74/100], Loss: 0.0068\n","Epoch [75/100], Loss: 0.0068\n","Epoch [76/100], Loss: 0.0068\n","Epoch [77/100], Loss: 0.0068\n","Epoch [78/100], Loss: 0.0068\n","Epoch [79/100], Loss: 0.0068\n","Epoch [80/100], Loss: 0.0068\n","Epoch [81/100], Loss: 0.0068\n","Epoch [82/100], Loss: 0.0068\n","Epoch [83/100], Loss: 0.0068\n","Epoch [84/100], Loss: 0.0068\n","Epoch [85/100], Loss: 0.0068\n","Epoch [86/100], Loss: 0.0068\n","Epoch [87/100], Loss: 0.0068\n","Epoch [88/100], Loss: 0.0068\n","Epoch [89/100], Loss: 0.0068\n","Epoch [90/100], Loss: 0.0068\n","Epoch [91/100], Loss: 0.0068\n","Epoch [92/100], Loss: 0.0068\n","Epoch [93/100], Loss: 0.0068\n","Epoch [94/100], Loss: 0.0068\n","Epoch [95/100], Loss: 0.0068\n","Epoch [96/100], Loss: 0.0068\n","Epoch [97/100], Loss: 0.0068\n","Epoch [98/100], Loss: 0.0068\n","Epoch [99/100], Loss: 0.0068\n","Epoch [100/100], Loss: 0.0068\n","Generating embeddings using IntermediateAutoencoder with mse loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"generate_embeddings() missing 1 required positional argument: 'embedding_type'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-540c87ac3aa1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;31m# Generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generating embeddings using {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m embeddings, labels = generate_embeddings(\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Your DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: generate_embeddings() missing 1 required positional argument: 'embedding_type'"]}]},{"cell_type":"code","source":["print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    embedding_type= embedding_type,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    device=device\n",")\n","\n","# ------------------------------\n","# Step 4: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhthd1DRy3w8","executionInfo":{"status":"ok","timestamp":1737562615217,"user_tz":-210,"elapsed":59878,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"8e829cd2-551c-4ea4-a2b0-ce45a1b4cd7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating embeddings using IntermediateAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"AdvancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHkQTUPaOsBs","executionInfo":{"status":"ok","timestamp":1737386767714,"user_tz":-210,"elapsed":1502224,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"2f26c37a-85ac-47d2-daaa-d53499521289"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training AdvancedAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.6372\n","Epoch [2/100], Loss: 0.3665\n","Epoch [3/100], Loss: 0.3064\n","Epoch [4/100], Loss: 0.2097\n","Epoch [5/100], Loss: 0.0671\n","Epoch [6/100], Loss: 0.0357\n","Epoch [7/100], Loss: 0.0296\n","Epoch [8/100], Loss: 0.0267\n","Epoch [9/100], Loss: 0.0248\n","Epoch [10/100], Loss: 0.0227\n","Epoch [11/100], Loss: 0.0216\n","Epoch [12/100], Loss: 0.0205\n","Epoch [13/100], Loss: 0.0197\n","Epoch [14/100], Loss: 0.0188\n","Epoch [15/100], Loss: 0.0180\n","Epoch [16/100], Loss: 0.0174\n","Epoch [17/100], Loss: 0.0167\n","Epoch [18/100], Loss: 0.0162\n","Epoch [19/100], Loss: 0.0158\n","Epoch [20/100], Loss: 0.0154\n","Epoch [21/100], Loss: 0.0149\n","Epoch [22/100], Loss: 0.0147\n","Epoch [23/100], Loss: 0.0144\n","Epoch [24/100], Loss: 0.0142\n","Epoch [25/100], Loss: 0.0138\n","Epoch [26/100], Loss: 0.0136\n","Epoch [27/100], Loss: 0.0135\n","Epoch [28/100], Loss: 0.0131\n","Epoch [29/100], Loss: 0.0128\n","Epoch [30/100], Loss: 0.0128\n","Epoch [31/100], Loss: 0.0124\n","Epoch [32/100], Loss: 0.0124\n","Epoch [33/100], Loss: 0.0121\n","Epoch [34/100], Loss: 0.0118\n","Epoch [35/100], Loss: 0.0120\n","Epoch [36/100], Loss: 0.0117\n","Epoch [37/100], Loss: 0.0117\n","Epoch [38/100], Loss: 0.0117\n","Epoch [39/100], Loss: 0.0112\n","Epoch [40/100], Loss: 0.0115\n","Epoch [41/100], Loss: 0.0111\n","Epoch [42/100], Loss: 0.0112\n","Epoch [43/100], Loss: 0.0109\n","Epoch [44/100], Loss: 0.0106\n","Epoch [45/100], Loss: 0.0105\n","Epoch [46/100], Loss: 0.0103\n","Epoch [47/100], Loss: 0.0103\n","Epoch [48/100], Loss: 0.0103\n","Epoch [49/100], Loss: 0.0101\n","Epoch [50/100], Loss: 0.0099\n","Epoch [51/100], Loss: 0.0099\n","Epoch [52/100], Loss: 0.0098\n","Epoch [53/100], Loss: 0.0098\n","Epoch [54/100], Loss: 0.0098\n","Epoch [55/100], Loss: 0.0097\n","Epoch [56/100], Loss: 0.0097\n","Epoch [57/100], Loss: 0.0094\n","Epoch [58/100], Loss: 0.0094\n","Epoch [59/100], Loss: 0.0092\n","Epoch [60/100], Loss: 0.0092\n","Epoch [61/100], Loss: 0.0092\n","Epoch [62/100], Loss: 0.0090\n","Epoch [63/100], Loss: 0.0090\n","Epoch [64/100], Loss: 0.0090\n","Epoch [65/100], Loss: 0.0089\n","Epoch [66/100], Loss: 0.0090\n","Epoch [67/100], Loss: 0.0088\n","Epoch [68/100], Loss: 0.0088\n","Epoch [69/100], Loss: 0.0088\n","Epoch [70/100], Loss: 0.0086\n","Epoch [71/100], Loss: 0.0086\n","Epoch [72/100], Loss: 0.0085\n","Epoch [73/100], Loss: 0.0084\n","Epoch [74/100], Loss: 0.0084\n","Epoch [75/100], Loss: 0.0085\n","Epoch [76/100], Loss: 0.0084\n","Epoch [77/100], Loss: 0.0083\n","Epoch [78/100], Loss: 0.0082\n","Epoch [79/100], Loss: 0.0082\n","Epoch [80/100], Loss: 0.0082\n","Epoch [81/100], Loss: 0.0079\n","Epoch [82/100], Loss: 0.0080\n","Epoch [83/100], Loss: 0.0080\n","Epoch [84/100], Loss: 0.0081\n","Epoch [85/100], Loss: 0.0078\n","Epoch [86/100], Loss: 0.0078\n","Epoch [87/100], Loss: 0.0079\n","Epoch [88/100], Loss: 0.0078\n","Epoch [89/100], Loss: 0.0078\n","Epoch [90/100], Loss: 0.0077\n","Epoch [91/100], Loss: 0.0077\n","Epoch [92/100], Loss: 0.0076\n","Epoch [93/100], Loss: 0.0076\n","Epoch [94/100], Loss: 0.0076\n","Epoch [95/100], Loss: 0.0076\n","Epoch [96/100], Loss: 0.0075\n","Epoch [97/100], Loss: 0.0074\n","Epoch [98/100], Loss: 0.0075\n","Epoch [99/100], Loss: 0.0074\n","Epoch [100/100], Loss: 0.0074\n","Generating embeddings using AdvancedAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"IntermediateAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOYys9LMQ4Bc","executionInfo":{"status":"ok","timestamp":1737388309401,"user_tz":-210,"elapsed":1080757,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"ea235020-6123-42e9-b127-eadaca700bca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training IntermediateAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.3440\n","Epoch [2/100], Loss: 0.0693\n","Epoch [3/100], Loss: 0.0423\n","Epoch [4/100], Loss: 0.0327\n","Epoch [5/100], Loss: 0.0277\n","Epoch [6/100], Loss: 0.0246\n","Epoch [7/100], Loss: 0.0222\n","Epoch [8/100], Loss: 0.0205\n","Epoch [9/100], Loss: 0.0191\n","Epoch [10/100], Loss: 0.0181\n","Epoch [11/100], Loss: 0.0174\n","Epoch [12/100], Loss: 0.0165\n","Epoch [13/100], Loss: 0.0157\n","Epoch [14/100], Loss: 0.0149\n","Epoch [15/100], Loss: 0.0147\n","Epoch [16/100], Loss: 0.0139\n","Epoch [17/100], Loss: 0.0136\n","Epoch [18/100], Loss: 0.0136\n","Epoch [19/100], Loss: 0.0130\n","Epoch [20/100], Loss: 0.0128\n","Epoch [21/100], Loss: 0.0124\n","Epoch [22/100], Loss: 0.0122\n","Epoch [23/100], Loss: 0.0116\n","Epoch [24/100], Loss: 0.0116\n","Epoch [25/100], Loss: 0.0114\n","Epoch [26/100], Loss: 0.0111\n","Epoch [27/100], Loss: 0.0111\n","Epoch [28/100], Loss: 0.0109\n","Epoch [29/100], Loss: 0.0106\n","Epoch [30/100], Loss: 0.0103\n","Epoch [31/100], Loss: 0.0106\n","Epoch [32/100], Loss: 0.0103\n","Epoch [33/100], Loss: 0.0101\n","Epoch [34/100], Loss: 0.0098\n","Epoch [35/100], Loss: 0.0099\n","Epoch [36/100], Loss: 0.0095\n","Epoch [37/100], Loss: 0.0095\n","Epoch [38/100], Loss: 0.0095\n","Epoch [39/100], Loss: 0.0094\n","Epoch [40/100], Loss: 0.0093\n","Epoch [41/100], Loss: 0.0091\n","Epoch [42/100], Loss: 0.0091\n","Epoch [43/100], Loss: 0.0088\n","Epoch [44/100], Loss: 0.0088\n","Epoch [45/100], Loss: 0.0087\n","Epoch [46/100], Loss: 0.0086\n","Epoch [47/100], Loss: 0.0088\n","Epoch [48/100], Loss: 0.0086\n","Epoch [49/100], Loss: 0.0084\n","Epoch [50/100], Loss: 0.0085\n","Epoch [51/100], Loss: 0.0084\n","Epoch [52/100], Loss: 0.0082\n","Epoch [53/100], Loss: 0.0082\n","Epoch [54/100], Loss: 0.0082\n","Epoch [55/100], Loss: 0.0079\n","Epoch [56/100], Loss: 0.0079\n","Epoch [57/100], Loss: 0.0079\n","Epoch [58/100], Loss: 0.0079\n","Epoch [59/100], Loss: 0.0079\n","Epoch [60/100], Loss: 0.0076\n","Epoch [61/100], Loss: 0.0076\n","Epoch [62/100], Loss: 0.0076\n","Epoch [63/100], Loss: 0.0077\n","Epoch [64/100], Loss: 0.0075\n","Epoch [65/100], Loss: 0.0075\n","Epoch [66/100], Loss: 0.0075\n","Epoch [67/100], Loss: 0.0074\n","Epoch [68/100], Loss: 0.0073\n","Epoch [69/100], Loss: 0.0073\n","Epoch [70/100], Loss: 0.0072\n","Epoch [71/100], Loss: 0.0071\n","Epoch [72/100], Loss: 0.0072\n","Epoch [73/100], Loss: 0.0071\n","Epoch [74/100], Loss: 0.0071\n","Epoch [75/100], Loss: 0.0069\n","Epoch [76/100], Loss: 0.0069\n","Epoch [77/100], Loss: 0.0070\n","Epoch [78/100], Loss: 0.0070\n","Epoch [79/100], Loss: 0.0068\n","Epoch [80/100], Loss: 0.0069\n","Epoch [81/100], Loss: 0.0069\n","Epoch [82/100], Loss: 0.0067\n","Epoch [83/100], Loss: 0.0066\n","Epoch [84/100], Loss: 0.0066\n","Epoch [85/100], Loss: 0.0067\n","Epoch [86/100], Loss: 0.0066\n","Epoch [87/100], Loss: 0.0066\n","Epoch [88/100], Loss: 0.0066\n","Epoch [89/100], Loss: 0.0065\n","Epoch [90/100], Loss: 0.0065\n","Epoch [91/100], Loss: 0.0065\n","Epoch [92/100], Loss: 0.0065\n","Epoch [93/100], Loss: 0.0067\n","Epoch [94/100], Loss: 0.0066\n","Epoch [95/100], Loss: 0.0064\n","Epoch [96/100], Loss: 0.0063\n","Epoch [97/100], Loss: 0.0063\n","Epoch [98/100], Loss: 0.0063\n","Epoch [99/100], Loss: 0.0063\n","Epoch [100/100], Loss: 0.0063\n","Generating embeddings using IntermediateAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"BasicAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztH2i-oLRBUm","executionInfo":{"status":"ok","timestamp":1737388913536,"user_tz":-210,"elapsed":604139,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"2b5c963a-fc06-4b83-d253-fb74b3f2e0b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training BasicAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.3403\n","Epoch [2/100], Loss: 0.2061\n","Epoch [3/100], Loss: 0.1146\n","Epoch [4/100], Loss: 0.0868\n","Epoch [5/100], Loss: 0.0737\n","Epoch [6/100], Loss: 0.0653\n","Epoch [7/100], Loss: 0.0595\n","Epoch [8/100], Loss: 0.0546\n","Epoch [9/100], Loss: 0.0507\n","Epoch [10/100], Loss: 0.0478\n","Epoch [11/100], Loss: 0.0455\n","Epoch [12/100], Loss: 0.0433\n","Epoch [13/100], Loss: 0.0413\n","Epoch [14/100], Loss: 0.0398\n","Epoch [15/100], Loss: 0.0387\n","Epoch [16/100], Loss: 0.0373\n","Epoch [17/100], Loss: 0.0362\n","Epoch [18/100], Loss: 0.0352\n","Epoch [19/100], Loss: 0.0340\n","Epoch [20/100], Loss: 0.0332\n","Epoch [21/100], Loss: 0.0327\n","Epoch [22/100], Loss: 0.0319\n","Epoch [23/100], Loss: 0.0312\n","Epoch [24/100], Loss: 0.0305\n","Epoch [25/100], Loss: 0.0299\n","Epoch [26/100], Loss: 0.0295\n","Epoch [27/100], Loss: 0.0291\n","Epoch [28/100], Loss: 0.0286\n","Epoch [29/100], Loss: 0.0280\n","Epoch [30/100], Loss: 0.0278\n","Epoch [31/100], Loss: 0.0274\n","Epoch [32/100], Loss: 0.0269\n","Epoch [33/100], Loss: 0.0267\n","Epoch [34/100], Loss: 0.0263\n","Epoch [35/100], Loss: 0.0262\n","Epoch [36/100], Loss: 0.0257\n","Epoch [37/100], Loss: 0.0254\n","Epoch [38/100], Loss: 0.0251\n","Epoch [39/100], Loss: 0.0248\n","Epoch [40/100], Loss: 0.0246\n","Epoch [41/100], Loss: 0.0244\n","Epoch [42/100], Loss: 0.0241\n","Epoch [43/100], Loss: 0.0238\n","Epoch [44/100], Loss: 0.0235\n","Epoch [45/100], Loss: 0.0234\n","Epoch [46/100], Loss: 0.0234\n","Epoch [47/100], Loss: 0.0230\n","Epoch [48/100], Loss: 0.0228\n","Epoch [49/100], Loss: 0.0227\n","Epoch [50/100], Loss: 0.0225\n","Epoch [51/100], Loss: 0.0225\n","Epoch [52/100], Loss: 0.0223\n","Epoch [53/100], Loss: 0.0221\n","Epoch [54/100], Loss: 0.0219\n","Epoch [55/100], Loss: 0.0217\n","Epoch [56/100], Loss: 0.0216\n","Epoch [57/100], Loss: 0.0214\n","Epoch [58/100], Loss: 0.0212\n","Epoch [59/100], Loss: 0.0212\n","Epoch [60/100], Loss: 0.0211\n","Epoch [61/100], Loss: 0.0210\n","Epoch [62/100], Loss: 0.0207\n","Epoch [63/100], Loss: 0.0205\n","Epoch [64/100], Loss: 0.0206\n","Epoch [65/100], Loss: 0.0204\n","Epoch [66/100], Loss: 0.0204\n","Epoch [67/100], Loss: 0.0202\n","Epoch [68/100], Loss: 0.0202\n","Epoch [69/100], Loss: 0.0199\n","Epoch [70/100], Loss: 0.0199\n","Epoch [71/100], Loss: 0.0199\n","Epoch [72/100], Loss: 0.0197\n","Epoch [73/100], Loss: 0.0197\n","Epoch [74/100], Loss: 0.0195\n","Epoch [75/100], Loss: 0.0195\n","Epoch [76/100], Loss: 0.0194\n","Epoch [77/100], Loss: 0.0192\n","Epoch [78/100], Loss: 0.0193\n","Epoch [79/100], Loss: 0.0192\n","Epoch [80/100], Loss: 0.0190\n","Epoch [81/100], Loss: 0.0189\n","Epoch [82/100], Loss: 0.0187\n","Epoch [83/100], Loss: 0.0189\n","Epoch [84/100], Loss: 0.0188\n","Epoch [85/100], Loss: 0.0187\n","Epoch [86/100], Loss: 0.0187\n","Epoch [87/100], Loss: 0.0186\n","Epoch [88/100], Loss: 0.0185\n","Epoch [89/100], Loss: 0.0185\n","Epoch [90/100], Loss: 0.0184\n","Epoch [91/100], Loss: 0.0183\n","Epoch [92/100], Loss: 0.0182\n","Epoch [93/100], Loss: 0.0182\n","Epoch [94/100], Loss: 0.0180\n","Epoch [95/100], Loss: 0.0181\n","Epoch [96/100], Loss: 0.0180\n","Epoch [97/100], Loss: 0.0179\n","Epoch [98/100], Loss: 0.0179\n","Epoch [99/100], Loss: 0.0179\n","Epoch [100/100], Loss: 0.0178\n","Generating embeddings using BasicAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"EnhancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=20, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"MAaK-hZjah3-","executionInfo":{"status":"error","timestamp":1737472116552,"user_tz":-210,"elapsed":921,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"1b3b5306-3d86-44c6-cb6f-db17e3b68ccf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training EnhancedAutoencoder with mse loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"train_autoencoder() got an unexpected keyword argument 'scheduler'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-53762437c672>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: train_autoencoder() got an unexpected keyword argument 'scheduler'"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"FlexibleVAE\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"vae\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"Y_eXsL-sYuUl","executionInfo":{"status":"error","timestamp":1737455119844,"user_tz":-210,"elapsed":22324,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"08b18305-34ca-4681-8483-06f3db77f65d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training FlexibleVAE with vae loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"'function' object is not iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-615e66d254db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/embeddings/encoder_training.py\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(vae, train_loader, val_loader, optimizer, scheduler, loss_fn, epochs, device, save_best, save_path)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mtotal_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"AdvancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"ntxent\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"c4lXegQrZEhF","executionInfo":{"status":"error","timestamp":1737471735460,"user_tz":-210,"elapsed":16599432,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"c5a121af-1603-4ac2-c017-94c8870c0526"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training AdvancedAutoencoder with ntxent loss...\n","Epoch [1/200], Loss: 3.2167\n","Epoch [2/200], Loss: 3.1131\n","Epoch [3/200], Loss: 3.0889\n","Epoch [4/200], Loss: 3.0769\n","Epoch [5/200], Loss: 3.0689\n","Epoch [6/200], Loss: 3.0646\n","Epoch [7/200], Loss: 3.0587\n","Epoch [8/200], Loss: 3.0553\n","Epoch [9/200], Loss: 3.0534\n","Epoch [10/200], Loss: 3.0497\n","Epoch [11/200], Loss: 3.0399\n","Epoch [12/200], Loss: 3.0386\n","Epoch [13/200], Loss: 3.0380\n","Epoch [14/200], Loss: 3.0374\n","Epoch [15/200], Loss: 3.0367\n","Epoch [16/200], Loss: 3.0362\n","Epoch [17/200], Loss: 3.0360\n","Epoch [18/200], Loss: 3.0358\n","Epoch [19/200], Loss: 3.0352\n","Epoch [20/200], Loss: 3.0348\n","Epoch [21/200], Loss: 3.0337\n","Epoch [22/200], Loss: 3.0331\n","Epoch [23/200], Loss: 3.0336\n","Epoch [24/200], Loss: 3.0332\n","Epoch [25/200], Loss: 3.0330\n","Epoch [26/200], Loss: 3.0332\n","Epoch [27/200], Loss: 3.0334\n","Epoch [28/200], Loss: 3.0329\n","Epoch [29/200], Loss: 3.0338\n","Epoch [30/200], Loss: 3.0334\n","Epoch [31/200], Loss: 3.0331\n","Epoch [32/200], Loss: 3.0330\n","Epoch [33/200], Loss: 3.0332\n","Epoch [34/200], Loss: 3.0333\n","Epoch [35/200], Loss: 3.0332\n","Epoch [36/200], Loss: 3.0328\n","Epoch [37/200], Loss: 3.0333\n","Epoch [38/200], Loss: 3.0334\n","Epoch [39/200], Loss: 3.0329\n","Epoch [40/200], Loss: 3.0332\n","Epoch [41/200], Loss: 3.0327\n","Epoch [42/200], Loss: 3.0332\n","Epoch [43/200], Loss: 3.0328\n","Epoch [44/200], Loss: 3.0327\n","Epoch [45/200], Loss: 3.0331\n","Epoch [46/200], Loss: 3.0333\n","Epoch [47/200], Loss: 3.0329\n","Epoch [48/200], Loss: 3.0332\n","Epoch [49/200], Loss: 3.0330\n","Epoch [50/200], Loss: 3.0325\n","Epoch [51/200], Loss: 3.0330\n","Epoch [52/200], Loss: 3.0334\n","Epoch [53/200], Loss: 3.0337\n","Epoch [54/200], Loss: 3.0332\n","Epoch [55/200], Loss: 3.0332\n","Epoch [56/200], Loss: 3.0329\n","Epoch [57/200], Loss: 3.0328\n","Epoch [58/200], Loss: 3.0329\n","Epoch [59/200], Loss: 3.0331\n","Epoch [60/200], Loss: 3.0331\n","Epoch [61/200], Loss: 3.0329\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fd37e98c6c71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/embeddings/encoder_training.py\u001b[0m in \u001b[0;36mtrain_with_ntxent_loss\u001b[0;34m(model, data_loader, ntxent_loss_fn, optimizer, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"FlexibleVAE\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"vae\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"id":"d3HnUC18Y12q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Configuration\n","# ------------------------------\n","\n","save_format = \"pt\"  # Change to \"npy\" if needed\n","\n","# Ensure valid save format\n","if save_format not in ['pt', 'npy']:\n","    print(f\"Invalid save format: {save_format}. Defaulting to 'pt'.\")\n","    save_format = 'pt'\n","\n","# Define model and loss type for naming conventions (update as needed)\n","model_name = \"matrix_factorization\"  # Example model name\n","loss_type = \"default_loss\"  # Update this as necessary (e.g., \"mse\", \"contrastive\", etc.)\n","\n","# ------------------------------\n","# Step 2: Matrix Factorization\n","# ------------------------------\n","\n","# Extract flattened images and labels\n","sampled_x, sampled_y = mnist_loader.dataset.tensors[0].numpy(), mnist_loader.dataset.tensors[1].numpy()\n","\n","print(\"Processing matrix factorization models (PCA, SVD, NMF)...\")\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","factorized_embeddings, factorized_labels = process_matrix_factorization(\n","    sampled_x, sampled_y, n_components=50\n",")\n","for method, embeddings in factorized_embeddings.items():\n","    embedding_subdir = f\"matrix_factorization_{method}\"\n","    embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","    os.makedirs(embedding_dir, exist_ok=True)\n","\n","    if save_format == \"pt\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_{method}_embeddings.pt\")\n","        torch.save({\"embeddings\": embeddings, \"labels\": factorized_labels}, embedding_file)\n","        print(f\"{method} embeddings saved in PyTorch format: {embedding_file}\")\n","    elif save_format == \"npy\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_{method}_embeddings.npy\")\n","        np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": factorized_labels.numpy()})\n","        print(f\"{method} embeddings saved in NumPy format: {embedding_file}\")\n","    else:\n","        raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 3: SIFT Features\n","# ------------------------------\n","\n","print(\"Processing SIFT features...\")\n","sift_features = apply_sift(sampled_x, n_features=50)\n","sift_labels = torch.tensor(sampled_y, dtype=torch.long)\n","\n","embedding_subdir = \"sift_features\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_sift_embeddings.pt\")\n","    torch.save({\"embeddings\": torch.tensor(sift_features), \"labels\": sift_labels}, embedding_file)\n","    print(f\"SIFT embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_sift_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": sift_features, \"labels\": sift_labels.numpy()})\n","    print(f\"SIFT embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 4: Kernel PCA\n","# ------------------------------\n","\n","print(\"Processing Kernel PCA...\")\n","kernel_pca_features, kernel_pca_labels = process_feature_extraction(\n","    sampled_x, sampled_y, n_features=50, kernel=\"rbf\", n_components=50\n",")\n","for method, embeddings in kernel_pca_features.items():\n","    embedding_subdir = f\"kernel_pca_{method}\"\n","    embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","    os.makedirs(embedding_dir, exist_ok=True)\n","\n","    if save_format == \"pt\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_kernel_pca_{method}_embeddings.pt\")\n","        torch.save({\"embeddings\": embeddings, \"labels\": kernel_pca_labels}, embedding_file)\n","        print(f\"{method} Kernel PCA embeddings saved in PyTorch format: {embedding_file}\")\n","    elif save_format == \"npy\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_kernel_pca_{method}_embeddings.npy\")\n","        np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": kernel_pca_labels.numpy()})\n","        print(f\"{method} Kernel PCA embeddings saved in NumPy format: {embedding_file}\")\n","    else:\n","        raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 5: Normalizing Flow\n","# ------------------------------\n","\n","print(\"Processing Normalizing Flow...\")\n","for method, embeddings in factorized_embeddings.items():\n","    # Initialize Normalizing Flow model\n","    input_dim = embeddings.size(1)\n","    nf_model = NormalizingFlowModel(input_dim=input_dim, num_flows=4)\n","    nf_model.to(device)\n","\n","    # Train Normalizing Flow model\n","    trained_nf_model = train_nf_model(\n","        nf_model, embeddings, num_epochs=200, lr=1e-3, batch_size=128\n","    )\n","\n","    # Refine embeddings\n","    with torch.no_grad():\n","        refined_embeddings, _ = trained_nf_model(embeddings)\n","\n","        embedding_subdir = f\"normalizing_flow_{method}\"\n","        embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","        os.makedirs(embedding_dir, exist_ok=True)\n","\n","        if save_format == \"pt\":\n","            embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_normalizing_flow_{method}_refined_embeddings.pt\")\n","            torch.save({\"embeddings\": refined_embeddings, \"labels\": factorized_labels}, embedding_file)\n","            print(f\"{method} refined embeddings (Normalizing Flow) saved in PyTorch format: {embedding_file}\")\n","        elif save_format == \"npy\":\n","            embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_normalizing_flow_{method}_refined_embeddings.npy\")\n","            np.save(embedding_file, {\"embeddings\": refined_embeddings.numpy(), \"labels\": factorized_labels.numpy()})\n","            print(f\"{method} refined embeddings (Normalizing Flow) saved in NumPy format: {embedding_file}\")\n","        else:\n","            raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","print(\"Feature extraction and normalizing flow processing complete!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BavfjvO2VAjZ","executionInfo":{"status":"ok","timestamp":1737454621474,"user_tz":-210,"elapsed":6311,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"e0a053ea-f4c2-44cb-ab01-5b8c07c45d6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing matrix factorization models (PCA, SVD, NMF)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_PCA/matrix_factorization_default_loss_PCA_embeddings.pt\n","SVD embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_SVD/matrix_factorization_default_loss_SVD_embeddings.pt\n","NMF embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_NMF/matrix_factorization_default_loss_NMF_embeddings.pt\n","Processing SIFT features...\n","SIFT embeddings saved in PyTorch format: ./saved_embeddings/embeddings/sift_features/matrix_factorization_default_loss_sift_embeddings.pt\n","Processing Kernel PCA...\n","SIFT Kernel PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/kernel_pca_SIFT/matrix_factorization_default_loss_kernel_pca_SIFT_embeddings.pt\n","Kernel PCA Kernel PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/kernel_pca_Kernel PCA/matrix_factorization_default_loss_kernel_pca_Kernel PCA_embeddings.pt\n","Processing Normalizing Flow...\n","Epoch 1/200, Loss: 2105532.0000\n","Epoch 2/200, Loss: 1371861.2500\n","Epoch 3/200, Loss: 1126910.5000\n","Epoch 4/200, Loss: 959351.8750\n","Epoch 5/200, Loss: 864164.3125\n","Epoch 6/200, Loss: 786244.5000\n","Epoch 7/200, Loss: 721899.8125\n","Epoch 8/200, Loss: 664512.6250\n","Epoch 9/200, Loss: 609381.1875\n","Epoch 10/200, Loss: 559293.9375\n","Epoch 11/200, Loss: 510937.0938\n","Epoch 12/200, Loss: 470874.3438\n","Epoch 13/200, Loss: 436454.6250\n","Epoch 14/200, Loss: 404345.8750\n","Epoch 15/200, Loss: 379761.9062\n","Epoch 16/200, Loss: 357777.1250\n","Epoch 17/200, Loss: 335547.7500\n","Epoch 18/200, Loss: 314221.4062\n","Epoch 19/200, Loss: 294646.0000\n","Epoch 20/200, Loss: 277729.7500\n","Epoch 21/200, Loss: 262339.3750\n","Epoch 22/200, Loss: 248801.9062\n","Epoch 23/200, Loss: 237040.6875\n","Epoch 24/200, Loss: 225382.1250\n","Epoch 25/200, Loss: 213554.0469\n","Epoch 26/200, Loss: 203046.3906\n","Epoch 27/200, Loss: 192870.7812\n","Epoch 28/200, Loss: 184185.5625\n","Epoch 29/200, Loss: 175942.1875\n","Epoch 30/200, Loss: 167710.6562\n","Epoch 31/200, Loss: 159133.3750\n","Epoch 32/200, Loss: 151090.0625\n","Epoch 33/200, Loss: 143847.4219\n","Epoch 34/200, Loss: 136919.7656\n","Epoch 35/200, Loss: 130742.0703\n","Epoch 36/200, Loss: 125214.0391\n","Epoch 37/200, Loss: 120406.5625\n","Epoch 38/200, Loss: 116063.0234\n","Epoch 39/200, Loss: 111900.5469\n","Epoch 40/200, Loss: 107834.9375\n","Epoch 41/200, Loss: 103941.9062\n","Epoch 42/200, Loss: 100229.7344\n","Epoch 43/200, Loss: 96637.6250\n","Epoch 44/200, Loss: 93144.7109\n","Epoch 45/200, Loss: 89765.5312\n","Epoch 46/200, Loss: 86543.3594\n","Epoch 47/200, Loss: 83587.8281\n","Epoch 48/200, Loss: 80868.2891\n","Epoch 49/200, Loss: 78263.9922\n","Epoch 50/200, Loss: 75727.1953\n","Epoch 51/200, Loss: 73238.8438\n","Epoch 52/200, Loss: 70822.8906\n","Epoch 53/200, Loss: 68554.4844\n","Epoch 54/200, Loss: 66264.5625\n","Epoch 55/200, Loss: 64225.9844\n","Epoch 56/200, Loss: 62319.0078\n","Epoch 57/200, Loss: 60492.9102\n","Epoch 58/200, Loss: 58792.5703\n","Epoch 59/200, Loss: 57237.7578\n","Epoch 60/200, Loss: 55801.0859\n","Epoch 61/200, Loss: 54346.9727\n","Epoch 62/200, Loss: 52881.1914\n","Epoch 63/200, Loss: 51527.9141\n","Epoch 64/200, Loss: 50278.5625\n","Epoch 65/200, Loss: 49091.9766\n","Epoch 66/200, Loss: 47941.2852\n","Epoch 67/200, Loss: 46817.4375\n","Epoch 68/200, Loss: 45729.6016\n","Epoch 69/200, Loss: 44685.0000\n","Epoch 70/200, Loss: 43661.2500\n","Epoch 71/200, Loss: 42656.9375\n","Epoch 72/200, Loss: 41696.7031\n","Epoch 73/200, Loss: 40776.6484\n","Epoch 74/200, Loss: 39884.7383\n","Epoch 75/200, Loss: 39019.6680\n","Epoch 76/200, Loss: 38188.8594\n","Epoch 77/200, Loss: 37394.0898\n","Epoch 78/200, Loss: 36625.8203\n","Epoch 79/200, Loss: 35882.1875\n","Epoch 80/200, Loss: 35169.9102\n","Epoch 81/200, Loss: 34486.4688\n","Epoch 82/200, Loss: 33823.5742\n","Epoch 83/200, Loss: 33177.8164\n","Epoch 84/200, Loss: 32551.8242\n","Epoch 85/200, Loss: 31946.8691\n","Epoch 86/200, Loss: 31359.3652\n","Epoch 87/200, Loss: 30787.3340\n","Epoch 88/200, Loss: 30233.3398\n","Epoch 89/200, Loss: 29697.4609\n","Epoch 90/200, Loss: 29175.7383\n","Epoch 91/200, Loss: 28666.6055\n","Epoch 92/200, Loss: 28172.3789\n","Epoch 93/200, Loss: 27692.1426\n","Epoch 94/200, Loss: 27222.4727\n","Epoch 95/200, Loss: 26765.9258\n","Epoch 96/200, Loss: 26324.6719\n","Epoch 97/200, Loss: 25896.7305\n","Epoch 98/200, Loss: 25481.9746\n","Epoch 99/200, Loss: 25080.5391\n","Epoch 100/200, Loss: 24689.1719\n","Epoch 101/200, Loss: 24305.9316\n","Epoch 102/200, Loss: 23931.9062\n","Epoch 103/200, Loss: 23566.7090\n","Epoch 104/200, Loss: 23209.2207\n","Epoch 105/200, Loss: 22859.9180\n","Epoch 106/200, Loss: 22518.7441\n","Epoch 107/200, Loss: 22184.3594\n","Epoch 108/200, Loss: 21856.7676\n","Epoch 109/200, Loss: 21537.1270\n","Epoch 110/200, Loss: 21225.8438\n","Epoch 111/200, Loss: 20922.8809\n","Epoch 112/200, Loss: 20628.1504\n","Epoch 113/200, Loss: 20341.0020\n","Epoch 114/200, Loss: 20060.4609\n","Epoch 115/200, Loss: 19785.9141\n","Epoch 116/200, Loss: 19516.7344\n","Epoch 117/200, Loss: 19252.1523\n","Epoch 118/200, Loss: 18992.1680\n","Epoch 119/200, Loss: 18737.8105\n","Epoch 120/200, Loss: 18490.2461\n","Epoch 121/200, Loss: 18250.1562\n","Epoch 122/200, Loss: 18017.6680\n","Epoch 123/200, Loss: 17792.3457\n","Epoch 124/200, Loss: 17573.5000\n","Epoch 125/200, Loss: 17360.6152\n","Epoch 126/200, Loss: 17153.1270\n","Epoch 127/200, Loss: 16950.0996\n","Epoch 128/200, Loss: 16750.5918\n","Epoch 129/200, Loss: 16554.3926\n","Epoch 130/200, Loss: 16361.8965\n","Epoch 131/200, Loss: 16173.4922\n","Epoch 132/200, Loss: 15989.2266\n","Epoch 133/200, Loss: 15808.9131\n","Epoch 134/200, Loss: 15632.2549\n","Epoch 135/200, Loss: 15458.9541\n","Epoch 136/200, Loss: 15288.8477\n","Epoch 137/200, Loss: 15121.9639\n","Epoch 138/200, Loss: 14958.4229\n","Epoch 139/200, Loss: 14798.3320\n","Epoch 140/200, Loss: 14641.7598\n","Epoch 141/200, Loss: 14488.6709\n","Epoch 142/200, Loss: 14338.8359\n","Epoch 143/200, Loss: 14191.9453\n","Epoch 144/200, Loss: 14047.8545\n","Epoch 145/200, Loss: 13906.6152\n","Epoch 146/200, Loss: 13768.2539\n","Epoch 147/200, Loss: 13632.6572\n","Epoch 148/200, Loss: 13499.6143\n","Epoch 149/200, Loss: 13368.9805\n","Epoch 150/200, Loss: 13240.6631\n","Epoch 151/200, Loss: 13114.6094\n","Epoch 152/200, Loss: 12990.8145\n","Epoch 153/200, Loss: 12869.2852\n","Epoch 154/200, Loss: 12750.0078\n","Epoch 155/200, Loss: 12632.9160\n","Epoch 156/200, Loss: 12517.9258\n","Epoch 157/200, Loss: 12404.9639\n","Epoch 158/200, Loss: 12294.0127\n","Epoch 159/200, Loss: 12185.0859\n","Epoch 160/200, Loss: 12078.1631\n","Epoch 161/200, Loss: 11973.1680\n","Epoch 162/200, Loss: 11870.0000\n","Epoch 163/200, Loss: 11768.5840\n","Epoch 164/200, Loss: 11668.8877\n","Epoch 165/200, Loss: 11570.8867\n","Epoch 166/200, Loss: 11474.5254\n","Epoch 167/200, Loss: 11379.7246\n","Epoch 168/200, Loss: 11286.4268\n","Epoch 169/200, Loss: 11194.6035\n","Epoch 170/200, Loss: 11104.2188\n","Epoch 171/200, Loss: 11015.2275\n","Epoch 172/200, Loss: 10927.5889\n","Epoch 173/200, Loss: 10841.2588\n","Epoch 174/200, Loss: 10756.1904\n","Epoch 175/200, Loss: 10672.3398\n","Epoch 176/200, Loss: 10589.6748\n","Epoch 177/200, Loss: 10508.1709\n","Epoch 178/200, Loss: 10427.7969\n","Epoch 179/200, Loss: 10348.5186\n","Epoch 180/200, Loss: 10270.3154\n","Epoch 181/200, Loss: 10193.1895\n","Epoch 182/200, Loss: 10117.1660\n","Epoch 183/200, Loss: 10042.2715\n","Epoch 184/200, Loss: 9968.5381\n","Epoch 185/200, Loss: 9896.0156\n","Epoch 186/200, Loss: 9824.7305\n","Epoch 187/200, Loss: 9754.6768\n","Epoch 188/200, Loss: 9685.8164\n","Epoch 189/200, Loss: 9618.0967\n","Epoch 190/200, Loss: 9551.4570\n","Epoch 191/200, Loss: 9485.8428\n","Epoch 192/200, Loss: 9421.2070\n","Epoch 193/200, Loss: 9357.5127\n","Epoch 194/200, Loss: 9294.7295\n","Epoch 195/200, Loss: 9232.8203\n","Epoch 196/200, Loss: 9171.7578\n","Epoch 197/200, Loss: 9111.5137\n","Epoch 198/200, Loss: 9052.0703\n","Epoch 199/200, Loss: 8993.4043\n","Epoch 200/200, Loss: 8935.5127\n","PCA refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_PCA/matrix_factorization_default_loss_normalizing_flow_PCA_refined_embeddings.pt\n","Epoch 1/200, Loss: 1729079.7500\n","Epoch 2/200, Loss: 1269361.7500\n","Epoch 3/200, Loss: 1074794.2500\n","Epoch 4/200, Loss: 965479.5000\n","Epoch 5/200, Loss: 857789.2500\n","Epoch 6/200, Loss: 751496.5000\n","Epoch 7/200, Loss: 628635.3750\n","Epoch 8/200, Loss: 575189.8125\n","Epoch 9/200, Loss: 540162.9375\n","Epoch 10/200, Loss: 488456.0938\n","Epoch 11/200, Loss: 459694.4688\n","Epoch 12/200, Loss: 449375.0000\n","Epoch 13/200, Loss: 415592.5000\n","Epoch 14/200, Loss: 390766.3125\n","Epoch 15/200, Loss: 368766.2812\n","Epoch 16/200, Loss: 349098.9688\n","Epoch 17/200, Loss: 332017.5938\n","Epoch 18/200, Loss: 316723.5938\n","Epoch 19/200, Loss: 302317.8125\n","Epoch 20/200, Loss: 288276.3750\n","Epoch 21/200, Loss: 274500.5312\n","Epoch 22/200, Loss: 261038.2969\n","Epoch 23/200, Loss: 247624.7188\n","Epoch 24/200, Loss: 233180.6250\n","Epoch 25/200, Loss: 219732.7031\n","Epoch 26/200, Loss: 206750.2500\n","Epoch 27/200, Loss: 197098.7812\n","Epoch 28/200, Loss: 188452.6250\n","Epoch 29/200, Loss: 179932.4844\n","Epoch 30/200, Loss: 171514.6406\n","Epoch 31/200, Loss: 164366.2969\n","Epoch 32/200, Loss: 157405.9219\n","Epoch 33/200, Loss: 150652.1406\n","Epoch 34/200, Loss: 144091.1875\n","Epoch 35/200, Loss: 137732.9062\n","Epoch 36/200, Loss: 131878.7031\n","Epoch 37/200, Loss: 126356.6875\n","Epoch 38/200, Loss: 121095.7188\n","Epoch 39/200, Loss: 116108.7031\n","Epoch 40/200, Loss: 111570.0000\n","Epoch 41/200, Loss: 107226.4844\n","Epoch 42/200, Loss: 103056.3359\n","Epoch 43/200, Loss: 99171.6719\n","Epoch 44/200, Loss: 95610.7344\n","Epoch 45/200, Loss: 92332.8594\n","Epoch 46/200, Loss: 89287.8906\n","Epoch 47/200, Loss: 86436.2188\n","Epoch 48/200, Loss: 83686.1250\n","Epoch 49/200, Loss: 80969.9219\n","Epoch 50/200, Loss: 78286.3906\n","Epoch 51/200, Loss: 75665.6250\n","Epoch 52/200, Loss: 73122.6562\n","Epoch 53/200, Loss: 70650.6875\n","Epoch 54/200, Loss: 68278.5469\n","Epoch 55/200, Loss: 66032.1719\n","Epoch 56/200, Loss: 63903.3477\n","Epoch 57/200, Loss: 61912.3945\n","Epoch 58/200, Loss: 60011.9414\n","Epoch 59/200, Loss: 58107.3555\n","Epoch 60/200, Loss: 56300.2188\n","Epoch 61/200, Loss: 54520.9023\n","Epoch 62/200, Loss: 52711.2383\n","Epoch 63/200, Loss: 50970.0469\n","Epoch 64/200, Loss: 49512.9688\n","Epoch 65/200, Loss: 48227.9961\n","Epoch 66/200, Loss: 47020.4219\n","Epoch 67/200, Loss: 45882.2031\n","Epoch 68/200, Loss: 44753.2812\n","Epoch 69/200, Loss: 43619.6289\n","Epoch 70/200, Loss: 42507.7773\n","Epoch 71/200, Loss: 41419.2734\n","Epoch 72/200, Loss: 40364.6250\n","Epoch 73/200, Loss: 39372.5312\n","Epoch 74/200, Loss: 38441.9258\n","Epoch 75/200, Loss: 37558.5469\n","Epoch 76/200, Loss: 36725.7539\n","Epoch 77/200, Loss: 35931.5156\n","Epoch 78/200, Loss: 35162.0234\n","Epoch 79/200, Loss: 34424.8867\n","Epoch 80/200, Loss: 33711.4453\n","Epoch 81/200, Loss: 33019.6836\n","Epoch 82/200, Loss: 32354.8418\n","Epoch 83/200, Loss: 31709.5078\n","Epoch 84/200, Loss: 31081.6172\n","Epoch 85/200, Loss: 30475.1504\n","Epoch 86/200, Loss: 29885.5156\n","Epoch 87/200, Loss: 29311.1289\n","Epoch 88/200, Loss: 28754.8945\n","Epoch 89/200, Loss: 28213.1367\n","Epoch 90/200, Loss: 27683.5391\n","Epoch 91/200, Loss: 27168.1055\n","Epoch 92/200, Loss: 26666.0156\n","Epoch 93/200, Loss: 26176.6797\n","Epoch 94/200, Loss: 25698.3984\n","Epoch 95/200, Loss: 25222.3105\n","Epoch 96/200, Loss: 24744.2773\n","Epoch 97/200, Loss: 24283.6523\n","Epoch 98/200, Loss: 23864.8809\n","Epoch 99/200, Loss: 23471.5195\n","Epoch 100/200, Loss: 23086.2910\n","Epoch 101/200, Loss: 22707.4648\n","Epoch 102/200, Loss: 22335.4727\n","Epoch 103/200, Loss: 21970.8633\n","Epoch 104/200, Loss: 21613.3086\n","Epoch 105/200, Loss: 21262.4492\n","Epoch 106/200, Loss: 20919.8320\n","Epoch 107/200, Loss: 20588.4941\n","Epoch 108/200, Loss: 20272.1270\n","Epoch 109/200, Loss: 19967.5312\n","Epoch 110/200, Loss: 19666.4375\n","Epoch 111/200, Loss: 19371.8691\n","Epoch 112/200, Loss: 19087.2324\n","Epoch 113/200, Loss: 18811.8652\n","Epoch 114/200, Loss: 18546.0430\n","Epoch 115/200, Loss: 18292.3867\n","Epoch 116/200, Loss: 18051.0723\n","Epoch 117/200, Loss: 17816.7227\n","Epoch 118/200, Loss: 17583.8086\n","Epoch 119/200, Loss: 17351.0234\n","Epoch 120/200, Loss: 17120.6172\n","Epoch 121/200, Loss: 16896.7012\n","Epoch 122/200, Loss: 16682.3027\n","Epoch 123/200, Loss: 16476.0645\n","Epoch 124/200, Loss: 16273.7061\n","Epoch 125/200, Loss: 16073.8555\n","Epoch 126/200, Loss: 15877.9326\n","Epoch 127/200, Loss: 15686.4023\n","Epoch 128/200, Loss: 15498.8008\n","Epoch 129/200, Loss: 15314.8877\n","Epoch 130/200, Loss: 15134.8193\n","Epoch 131/200, Loss: 14958.9785\n","Epoch 132/200, Loss: 14787.5879\n","Epoch 133/200, Loss: 14620.3809\n","Epoch 134/200, Loss: 14456.5010\n","Epoch 135/200, Loss: 14294.8496\n","Epoch 136/200, Loss: 14135.3115\n","Epoch 137/200, Loss: 13979.0918\n","Epoch 138/200, Loss: 13827.0576\n","Epoch 139/200, Loss: 13678.8662\n","Epoch 140/200, Loss: 13533.6719\n","Epoch 141/200, Loss: 13390.9814\n","Epoch 142/200, Loss: 13250.8604\n","Epoch 143/200, Loss: 13113.5928\n","Epoch 144/200, Loss: 12979.1807\n","Epoch 145/200, Loss: 12847.3379\n","Epoch 146/200, Loss: 12718.1055\n","Epoch 147/200, Loss: 12591.7930\n","Epoch 148/200, Loss: 12468.2969\n","Epoch 149/200, Loss: 12347.1289\n","Epoch 150/200, Loss: 12227.9580\n","Epoch 151/200, Loss: 12110.8643\n","Epoch 152/200, Loss: 11996.0781\n","Epoch 153/200, Loss: 11883.5645\n","Epoch 154/200, Loss: 11773.1113\n","Epoch 155/200, Loss: 11664.7139\n","Epoch 156/200, Loss: 11558.4287\n","Epoch 157/200, Loss: 11454.0859\n","Epoch 158/200, Loss: 11351.4814\n","Epoch 159/200, Loss: 11250.6113\n","Epoch 160/200, Loss: 11151.6025\n","Epoch 161/200, Loss: 11054.4502\n","Epoch 162/200, Loss: 10959.0186\n","Epoch 163/200, Loss: 10865.2744\n","Epoch 164/200, Loss: 10773.2354\n","Epoch 165/200, Loss: 10682.7891\n","Epoch 166/200, Loss: 10593.8076\n","Epoch 167/200, Loss: 10506.3105\n","Epoch 168/200, Loss: 10420.3359\n","Epoch 169/200, Loss: 10335.8193\n","Epoch 170/200, Loss: 10252.6895\n","Epoch 171/200, Loss: 10170.9512\n","Epoch 172/200, Loss: 10090.5752\n","Epoch 173/200, Loss: 10011.4775\n","Epoch 174/200, Loss: 9933.6318\n","Epoch 175/200, Loss: 9857.0508\n","Epoch 176/200, Loss: 9781.7051\n","Epoch 177/200, Loss: 9707.5400\n","Epoch 178/200, Loss: 9634.5430\n","Epoch 179/200, Loss: 9562.6924\n","Epoch 180/200, Loss: 9491.9375\n","Epoch 181/200, Loss: 9422.2441\n","Epoch 182/200, Loss: 9353.6221\n","Epoch 183/200, Loss: 9286.0420\n","Epoch 184/200, Loss: 9219.4717\n","Epoch 185/200, Loss: 9153.8994\n","Epoch 186/200, Loss: 9089.3047\n","Epoch 187/200, Loss: 9025.6475\n","Epoch 188/200, Loss: 8962.9111\n","Epoch 189/200, Loss: 8901.0820\n","Epoch 190/200, Loss: 8840.1367\n","Epoch 191/200, Loss: 8780.0547\n","Epoch 192/200, Loss: 8720.8291\n","Epoch 193/200, Loss: 8662.4316\n","Epoch 194/200, Loss: 8604.8438\n","Epoch 195/200, Loss: 8548.0508\n","Epoch 196/200, Loss: 8492.0332\n","Epoch 197/200, Loss: 8436.7705\n","Epoch 198/200, Loss: 8382.2559\n","Epoch 199/200, Loss: 8328.4707\n","Epoch 200/200, Loss: 8275.3965\n","SVD refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_SVD/matrix_factorization_default_loss_normalizing_flow_SVD_refined_embeddings.pt\n","Epoch 1/200, Loss: 3338.7485\n","Epoch 2/200, Loss: 3224.9641\n","Epoch 3/200, Loss: 3159.9268\n","Epoch 4/200, Loss: 3114.6672\n","Epoch 5/200, Loss: 3082.0623\n","Epoch 6/200, Loss: 3058.4419\n","Epoch 7/200, Loss: 3041.1448\n","Epoch 8/200, Loss: 3028.2073\n","Epoch 9/200, Loss: 3018.2388\n","Epoch 10/200, Loss: 3010.2837\n","Epoch 11/200, Loss: 3003.7068\n","Epoch 12/200, Loss: 2998.1006\n","Epoch 13/200, Loss: 2993.2102\n","Epoch 14/200, Loss: 2988.8782\n","Epoch 15/200, Loss: 2985.0061\n","Epoch 16/200, Loss: 2981.5303\n","Epoch 17/200, Loss: 2978.4077\n","Epoch 18/200, Loss: 2975.6042\n","Epoch 19/200, Loss: 2973.0889\n","Epoch 20/200, Loss: 2970.8335\n","Epoch 21/200, Loss: 2968.8108\n","Epoch 22/200, Loss: 2966.9912\n","Epoch 23/200, Loss: 2965.3506\n","Epoch 24/200, Loss: 2963.8628\n","Epoch 25/200, Loss: 2962.5071\n","Epoch 26/200, Loss: 2961.2622\n","Epoch 27/200, Loss: 2960.1118\n","Epoch 28/200, Loss: 2959.0422\n","Epoch 29/200, Loss: 2958.0398\n","Epoch 30/200, Loss: 2957.0974\n","Epoch 31/200, Loss: 2956.2058\n","Epoch 32/200, Loss: 2955.3608\n","Epoch 33/200, Loss: 2954.5566\n","Epoch 34/200, Loss: 2953.7896\n","Epoch 35/200, Loss: 2953.0562\n","Epoch 36/200, Loss: 2952.3540\n","Epoch 37/200, Loss: 2951.6809\n","Epoch 38/200, Loss: 2951.0344\n","Epoch 39/200, Loss: 2950.4111\n","Epoch 40/200, Loss: 2949.8086\n","Epoch 41/200, Loss: 2949.2263\n","Epoch 42/200, Loss: 2948.6609\n","Epoch 43/200, Loss: 2948.1123\n","Epoch 44/200, Loss: 2947.5759\n","Epoch 45/200, Loss: 2947.0527\n","Epoch 46/200, Loss: 2946.5405\n","Epoch 47/200, Loss: 2946.0378\n","Epoch 48/200, Loss: 2945.5442\n","Epoch 49/200, Loss: 2945.0588\n","Epoch 50/200, Loss: 2944.5796\n","Epoch 51/200, Loss: 2944.1072\n","Epoch 52/200, Loss: 2943.6406\n","Epoch 53/200, Loss: 2943.1775\n","Epoch 54/200, Loss: 2942.7190\n","Epoch 55/200, Loss: 2942.2634\n","Epoch 56/200, Loss: 2941.8105\n","Epoch 57/200, Loss: 2941.3596\n","Epoch 58/200, Loss: 2940.9097\n","Epoch 59/200, Loss: 2940.4614\n","Epoch 60/200, Loss: 2940.0139\n","Epoch 61/200, Loss: 2939.5664\n","Epoch 62/200, Loss: 2939.1187\n","Epoch 63/200, Loss: 2938.6714\n","Epoch 64/200, Loss: 2938.2236\n","Epoch 65/200, Loss: 2937.7749\n","Epoch 66/200, Loss: 2937.3264\n","Epoch 67/200, Loss: 2936.8760\n","Epoch 68/200, Loss: 2936.4248\n","Epoch 69/200, Loss: 2935.9724\n","Epoch 70/200, Loss: 2935.5181\n","Epoch 71/200, Loss: 2935.0623\n","Epoch 72/200, Loss: 2934.6050\n","Epoch 73/200, Loss: 2934.1448\n","Epoch 74/200, Loss: 2933.6826\n","Epoch 75/200, Loss: 2933.2178\n","Epoch 76/200, Loss: 2932.7505\n","Epoch 77/200, Loss: 2932.2800\n","Epoch 78/200, Loss: 2931.8066\n","Epoch 79/200, Loss: 2931.3306\n","Epoch 80/200, Loss: 2930.8503\n","Epoch 81/200, Loss: 2930.3669\n","Epoch 82/200, Loss: 2929.8794\n","Epoch 83/200, Loss: 2929.3879\n","Epoch 84/200, Loss: 2928.8936\n","Epoch 85/200, Loss: 2928.3940\n","Epoch 86/200, Loss: 2927.8906\n","Epoch 87/200, Loss: 2927.3818\n","Epoch 88/200, Loss: 2926.8691\n","Epoch 89/200, Loss: 2926.3506\n","Epoch 90/200, Loss: 2925.8281\n","Epoch 91/200, Loss: 2925.2993\n","Epoch 92/200, Loss: 2924.7659\n","Epoch 93/200, Loss: 2924.2266\n","Epoch 94/200, Loss: 2923.6812\n","Epoch 95/200, Loss: 2923.1299\n","Epoch 96/200, Loss: 2922.5732\n","Epoch 97/200, Loss: 2922.0093\n","Epoch 98/200, Loss: 2921.4397\n","Epoch 99/200, Loss: 2920.8638\n","Epoch 100/200, Loss: 2920.2810\n","Epoch 101/200, Loss: 2919.6919\n","Epoch 102/200, Loss: 2919.0950\n","Epoch 103/200, Loss: 2918.4922\n","Epoch 104/200, Loss: 2917.8816\n","Epoch 105/200, Loss: 2917.2639\n","Epoch 106/200, Loss: 2916.6399\n","Epoch 107/200, Loss: 2916.0076\n","Epoch 108/200, Loss: 2915.3687\n","Epoch 109/200, Loss: 2914.7219\n","Epoch 110/200, Loss: 2914.0686\n","Epoch 111/200, Loss: 2913.4082\n","Epoch 112/200, Loss: 2912.7397\n","Epoch 113/200, Loss: 2912.0645\n","Epoch 114/200, Loss: 2911.3826\n","Epoch 115/200, Loss: 2910.6938\n","Epoch 116/200, Loss: 2909.9980\n","Epoch 117/200, Loss: 2909.2957\n","Epoch 118/200, Loss: 2908.5869\n","Epoch 119/200, Loss: 2907.8721\n","Epoch 120/200, Loss: 2907.1504\n","Epoch 121/200, Loss: 2906.4238\n","Epoch 122/200, Loss: 2905.6909\n","Epoch 123/200, Loss: 2904.9536\n","Epoch 124/200, Loss: 2904.2100\n","Epoch 125/200, Loss: 2903.4624\n","Epoch 126/200, Loss: 2902.7095\n","Epoch 127/200, Loss: 2901.9531\n","Epoch 128/200, Loss: 2901.1919\n","Epoch 129/200, Loss: 2900.4272\n","Epoch 130/200, Loss: 2899.6587\n","Epoch 131/200, Loss: 2898.8879\n","Epoch 132/200, Loss: 2898.1133\n","Epoch 133/200, Loss: 2897.3367\n","Epoch 134/200, Loss: 2896.5574\n","Epoch 135/200, Loss: 2895.7764\n","Epoch 136/200, Loss: 2894.9939\n","Epoch 137/200, Loss: 2894.2095\n","Epoch 138/200, Loss: 2893.4243\n","Epoch 139/200, Loss: 2892.6387\n","Epoch 140/200, Loss: 2891.8516\n","Epoch 141/200, Loss: 2891.0645\n","Epoch 142/200, Loss: 2890.2778\n","Epoch 143/200, Loss: 2889.4912\n","Epoch 144/200, Loss: 2888.7056\n","Epoch 145/200, Loss: 2887.9209\n","Epoch 146/200, Loss: 2887.1377\n","Epoch 147/200, Loss: 2886.3555\n","Epoch 148/200, Loss: 2885.5757\n","Epoch 149/200, Loss: 2884.7979\n","Epoch 150/200, Loss: 2884.0222\n","Epoch 151/200, Loss: 2883.2500\n","Epoch 152/200, Loss: 2882.4807\n","Epoch 153/200, Loss: 2881.7148\n","Epoch 154/200, Loss: 2880.9521\n","Epoch 155/200, Loss: 2880.1934\n","Epoch 156/200, Loss: 2879.4382\n","Epoch 157/200, Loss: 2878.6880\n","Epoch 158/200, Loss: 2877.9419\n","Epoch 159/200, Loss: 2877.2012\n","Epoch 160/200, Loss: 2876.4644\n","Epoch 161/200, Loss: 2875.7327\n","Epoch 162/200, Loss: 2875.0054\n","Epoch 163/200, Loss: 2874.2842\n","Epoch 164/200, Loss: 2873.5676\n","Epoch 165/200, Loss: 2872.8564\n","Epoch 166/200, Loss: 2872.1504\n","Epoch 167/200, Loss: 2871.4492\n","Epoch 168/200, Loss: 2870.7532\n","Epoch 169/200, Loss: 2870.0635\n","Epoch 170/200, Loss: 2869.3779\n","Epoch 171/200, Loss: 2868.6980\n","Epoch 172/200, Loss: 2868.0234\n","Epoch 173/200, Loss: 2867.3540\n","Epoch 174/200, Loss: 2866.6899\n","Epoch 175/200, Loss: 2866.0300\n","Epoch 176/200, Loss: 2865.3774\n","Epoch 177/200, Loss: 2864.7358\n","Epoch 178/200, Loss: 2864.1416\n","Epoch 179/200, Loss: 2863.5303\n","Epoch 180/200, Loss: 2862.8442\n","Epoch 181/200, Loss: 2862.2310\n","Epoch 182/200, Loss: 2861.6394\n","Epoch 183/200, Loss: 2861.0059\n","Epoch 184/200, Loss: 2860.3923\n","Epoch 185/200, Loss: 2859.8110\n","Epoch 186/200, Loss: 2859.2126\n","Epoch 187/200, Loss: 2858.6064\n","Epoch 188/200, Loss: 2858.0244\n","Epoch 189/200, Loss: 2857.4514\n","Epoch 190/200, Loss: 2856.8811\n","Epoch 191/200, Loss: 2856.3003\n","Epoch 192/200, Loss: 2855.7144\n","Epoch 193/200, Loss: 2855.1489\n","Epoch 194/200, Loss: 2854.6125\n","Epoch 195/200, Loss: 2854.0796\n","Epoch 196/200, Loss: 2853.5332\n","Epoch 197/200, Loss: 2852.9800\n","Epoch 198/200, Loss: 2852.4272\n","Epoch 199/200, Loss: 2851.8811\n","Epoch 200/200, Loss: 2851.3469\n","NMF refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_NMF/matrix_factorization_default_loss_normalizing_flow_NMF_refined_embeddings.pt\n","Feature extraction and normalizing flow processing complete!\n"]}]}]}