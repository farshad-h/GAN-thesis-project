{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Embedding Generation Notebook\n","\n","## Objective\n","This notebook demonstrates how to use custom encoder models to generate embeddings from the MNIST dataset. These models are implemented in `encoder_models.py`, and the training processes are defined in `encoder_training.py`.\n","\n","## Workflow\n","1. **Load and Preprocess Data**:\n","   - Load the MNIST dataset for testing the embedding generation process.\n","   - Normalize and prepare the data.\n","2. **Model Selection and Training**:\n","   - Train selected encoder models from `encoder_models.py`.\n","   - Generate embeddings from the bottleneck layer.\n","3. **Feature Extraction**:\n","   - Generate embeddings using matrix factorization (PCA, SVD, NMF) and SIFT.\n","4. **Save Embeddings**:\n","   - Save all embeddings and trained models for reuse.\n","\n","## Models and Methods\n","### Supported Models\n","The following encoder models are available for training and embedding generation. Each model is implemented in `encoder_models.py`:\n","- **Encoder Models**:\n","  - BasicAutoencoder, IntermediateAutoencoder, AdvancedAutoencoder, EnhancedAutoencoder.\n","  - BasicVAE, VAEWithFCDecoder, ImprovedVAE, FlexibleVAE.\n","- **Feature Extraction**:\n","  - PCA, SVD, NMF.\n","  - SIFT, Kernel PCA.\n","\n","#### **Autoencoders**\n","1. **BasicAutoencoder**:\n","   - A simple autoencoder with:\n","     - **Encoder**: Two convolutional layers followed by max-pooling.\n","     - **Decoder**: Two transposed convolutional layers to reconstruct the input.\n","   - Designed for grayscale datasets like MNIST.\n","   - Suitable for basic dimensionality reduction and reconstruction tasks.\n","\n","2. **IntermediateAutoencoder**:\n","   - A deeper autoencoder with:\n","     - **Batch Normalization** for improved stability.\n","     - Additional feature maps for a more expressive latent space.\n","   - Designed for moderately complex embedding tasks requiring better feature extraction.\n","\n","3. **AdvancedAutoencoder**:\n","   - A sophisticated autoencoder with:\n","     - **Skip Connections** to improve gradient flow and reconstruction accuracy.\n","     - **LeakyReLU Activations** and Batch Normalization for robust performance.\n","   - Suitable for high-dimensional or structured data requiring detailed reconstruction.\n","\n","4. **EnhancedAutoencoder**:\n","   - A deeper autoencoder with:\n","     - Additional convolutional layers in the encoder.\n","     - Transposed convolutional layers in the decoder.\n","     - LeakyReLU activations and Batch Normalization for better embedding representation.\n","   - Designed for datasets requiring intricate reconstructions under noisy conditions.\n","\n","#### **Variational Autoencoders (VAEs)**\n","5. **BasicVAE**:\n","   - A simple VAE with:\n","     - **Encoder**: Two convolutional layers and a fully connected layer to parameterize the latent space.\n","     - **Decoder**: Fully connected and transposed convolution layers to reconstruct input images.\n","   - Suitable for generative tasks with simple latent spaces.\n","\n","6. **VAEWithFCDecoder**:\n","   - A VAE with a fully connected decoder for enhanced latent-to-feature mapping.\n","   - Features:\n","     - **Encoder**: Convolutional layers with Batch Normalization.\n","     - **Decoder**: A combination of fully connected and transposed convolutional layers.\n","\n","7. **ImprovedVAE**:\n","   - An advanced VAE with:\n","     - A bottleneck layer for enhanced feature extraction.\n","     - Transposed convolutions for smooth reconstructions.\n","     - KL divergence loss for latent space regularization.\n","   - Designed for datasets requiring expressive latent representations.\n","\n","8. **FlexibleVAE**:\n","   - A flexible VAE that supports dynamic input shapes and optional projection heads for contrastive learning.\n","   - Suitable for embedding tasks with varying input dimensions.\n","\n","9. **ImprovedFlexibleVAE**:\n","   - Combines convolutional and fully connected layers in the encoder.\n","   - Uses transposed convolutions in the decoder for better reconstruction.\n","   - Optional **Projection Head** for self-supervised contrastive learning tasks.\n","\n","#### **Denoising Autoencoders**\n","10. **DenoisingAutoencoder**:\n","    - A denoising autoencoder with:\n","      - **Encoder**: Convolutional layers for feature extraction.\n","      - **Decoder**: Transposed convolutional layers for reconstruction.\n","      - Optional **Projection Head** for contrastive learning.\n","    - Supports two architectures:\n","      - **Basic**: Simpler structure for standard denoising tasks.\n","      - **Strong**: Deeper architecture for challenging noisy datasets.\n","\n","#### **Feature Extraction and Normalizing Flow Models**\n","11. **Matrix Factorization**:\n","    - Embeddings generated using PCA, SVD, and NMF.\n","    - Useful for dimensionality reduction and compact representations.\n","\n","12. **SIFT (Scale-Invariant Feature Transform)**:\n","    - Extracts scale-invariant features from images.\n","    - Pads feature descriptors to ensure consistent dimensionality.\n","\n","13. **Kernel PCA**:\n","    - Nonlinear dimensionality reduction using Kernel PCA with adjustable kernels.\n","\n","14. **Normalizing Flow Models**:\n","    - Transforms embeddings into a latent space using invertible transformations.\n","    - Useful for embedding refinement and generative tasks.\n","\n","\n","**Training**:\n","   - Each model is trained using the corresponding training loop defined in `encoder_training.py`.\n","   - Training includes support for reconstruction loss, KL divergence (for VAE), and optional noise injection.\n","**Embedding Generation**:\n","   - Once the models are trained, embeddings are generated for the MNIST dataset.\n","   - Encodings from the bottleneck layer are extracted for downstream tasks.\n","**Results Storage**:\n","   - Save trained models to `.pth` files.\n","   - Save generated embeddings to `.pt` files for reuse in downstream applications.\n","\n","## Supported Features\n","- **Flexible Model Selection**:\n","  - Choose specific models to train and generate embeddings for, bypassing others if needed.\n","- **Custom Configuration**:\n","  - Easily modify parameters like the bottleneck size (`code_dim`), number of training epochs, and learning rates.\n","\n","## Outputs\n","- Trained models saved as `.pth` files.\n","- Generated embeddings saved as `.pt` files in a structured directory (`./embeddings`).\n","\n","## Notes\n","This notebook is designed for flexibility and reusability. You can:\n","- Add new encoder models in `encoder_models.py`.\n","- Customize training loops in `encoder_training.py`.\n","- Modify this notebook to train specific models or generate embeddings for specific datasets.\n"],"metadata":{"id":"VUQM0eOdRPMH"}},{"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","# Mount Google Drive and set repository path\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Repository path (adjust if needed)\n","repo_path = \"/content/drive/MyDrive/GAN-thesis-project\"\n","\n","# Add repository path to sys.path for module imports\n","if repo_path not in sys.path:\n","    sys.path.append(repo_path)\n","\n","# Change working directory to the repository\n","os.chdir(repo_path)\n","\n","# Verify the working directory\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","# Configuration\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLvDZtTSZFUb","executionInfo":{"status":"ok","timestamp":1737805898329,"user_tz":-210,"elapsed":2377,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"89a58ab2-4d19-4ded-869a-f2e9bb31aa4a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Current working directory: /content/drive/MyDrive/GAN-thesis-project\n","Using device: cpu\n"]}]},{"cell_type":"code","source":["import inspect\n","\n","# Import the entire modules\n","import src.data_utils as data_utils\n","import src.cl_loss_function as cl_loss\n","import src.losses as losses\n","import src.embeddings.encoder_models as encoder_models\n","import src.embeddings.encoder_training as encoder_training\n","\n","# Function to list functions and classes in a module\n","def list_functions_and_classes(module):\n","    members = inspect.getmembers(module)\n","    functions = [name for name, obj in members if inspect.isfunction(obj)]\n","    classes = [name for name, obj in members if inspect.isclass(obj)]\n","    return functions, classes\n","\n","# Function to print functions and classes in a readable format\n","def print_functions_and_classes(module_name, module):\n","    functions, classes = list_functions_and_classes(module)\n","    print(f\"Module: {module_name}\")\n","    print(\"  Functions:\")\n","    for func in functions:\n","        print(f\"    - {func}\")\n","    print(\"  Classes:\")\n","    for cls in classes:\n","        print(f\"    - {cls}\")\n","    print()  # Add a blank line for separation\n","\n","# Print functions and classes for each module\n","print_functions_and_classes(\"src.data_utils\", data_utils)\n","print_functions_and_classes(\"src.cl_loss_function\", cl_loss)\n","print_functions_and_classes(\"src.losses\", losses)\n","print_functions_and_classes(\"src.embeddings.encoder_models\", encoder_models)\n","print_functions_and_classes(\"src.embeddings.encoder_training\", encoder_training)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jah7kWk-SpUb","executionInfo":{"status":"ok","timestamp":1737805934549,"user_tz":-210,"elapsed":307,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"756db95a-6824-4b6b-e7f0-20d6766bee3a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Module: src.data_utils\n","  Functions:\n","    - analyze_embeddings\n","    - analyze_embeddings_v2\n","    - create_dataloader\n","    - create_embedding_loaders\n","    - kurtosis\n","    - load_data\n","    - load_embeddings\n","    - load_mnist_data\n","    - pdist\n","    - preprocess_images\n","    - save_embeddings\n","    - skew\n","    - split_dataset\n","    - train_test_split\n","    - visualize_embeddings\n","  Classes:\n","    - DataLoader\n","    - LocalOutlierFactor\n","    - TensorDataset\n","\n","Module: src.cl_loss_function\n","  Functions:\n","    - augment\n","    - compute_nt_xent_loss_with_augmentation\n","    - compute_triplet_loss_with_augmentation\n","    - contrastive_loss\n","    - hflip\n","    - info_nce_loss\n","    - resize\n","  Classes:\n","    - ContrastiveHead\n","    - DataLoader\n","    - NTXentLoss\n","    - PCA\n","    - TensorDataset\n","    - TripletLoss\n","    - VicRegLoss\n","\n","Module: src.losses\n","  Functions:\n","    - add_noise\n","    - cyclical_beta_schedule\n","    - linear_beta_schedule\n","    - loss_function_dae_ssim\n","    - vae_loss\n","    - vae_ssim_loss\n","  Classes:\n","\n","Module: src.embeddings.encoder_models\n","  Functions:\n","    - apply_dimensionality_reduction\n","    - apply_sift\n","    - init_weights\n","    - log_prob\n","    - process_feature_extraction\n","    - process_matrix_factorization\n","    - refine_embeddings_NF\n","    - train_nf_model\n","  Classes:\n","    - AdvancedAutoencoder\n","    - BasicAutoencoder\n","    - BasicVAE\n","    - DataLoader\n","    - DenoisingAutoencoder\n","    - EnhancedAutoencoder\n","    - FlexibleVAE\n","    - FlowLayer\n","    - ImprovedFlexibleVAE\n","    - ImprovedVAE\n","    - IntermediateAutoencoder\n","    - KernelPCA\n","    - MinMaxScaler\n","    - NMF\n","    - NormalizingFlowModel\n","    - PCA\n","    - ProjectionHead\n","    - SimCLR\n","    - StandardScaler\n","    - TensorDataset\n","    - TruncatedSVD\n","    - tqdm\n","\n","Module: src.embeddings.encoder_training\n","  Functions:\n","    - add_noise\n","    - ssim\n","    - train_autoencoder\n","    - train_dae\n","    - train_simclr\n","    - train_vae\n","  Classes:\n","    - DataLoader\n","    - MinMaxScaler\n","    - StandardScaler\n","    - TensorDataset\n","    - ToTensor\n","    - tqdm\n","\n"]}]},{"cell_type":"code","source":["# Load and Preprocess MNIST Data\n","fraction = 1  # Fraction of the dataset to use\n","batch_size = 64\n","\n","# Load Combined MNIST Dataset (Train + Test)\n","mnist_loader = data_utils.load_mnist_data(fraction=fraction, batch_size=batch_size, shuffle=True)\n","\n","# Inspect Combined Dataset\n","for batch in mnist_loader:\n","    images, labels = batch\n","    print(\"Batch Shape:\", images.shape, labels.shape)\n","    break\n","\n","# Visualize Original Images\n","n = 30\n","sample_indices = np.random.choice(len(mnist_loader.dataset), n, replace=False)\n","sampled_images = mnist_loader.dataset.tensors[0][sample_indices].numpy()\n","sampled_images = (sampled_images * 127.5 + 127.5).astype(np.uint8).squeeze()  # Denormalize for display\n","\n","plt.figure(figsize=(20, 6))\n","for i in range(n):\n","    ax = plt.subplot(3, 10, i + 1)\n","    plt.imshow(sampled_images[i], cmap=\"gray\")\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()\n"],"metadata":{"id":"9ByfVYCjeT7P","colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"status":"ok","timestamp":1737811164246,"user_tz":-210,"elapsed":2581,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"b9727555-7833-4859-b608-b7499e679c85"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (70000, 1, 28, 28) (70000,)\n","Batch Shape: torch.Size([64, 1, 28, 28]) torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 2000x600 with 30 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAABiEAAAHdCAYAAAB7dtr6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAY8VJREFUeJzt3Xn8XdO9P/6dRBISITKUBKFEDNccipYmUjWLeVZDzfMcNSYhaqiZGopS9FIzMdYliUvNQ9RMb2NKkCCEkCD5/fF93Pvr3u9Vn5OTs8/5DM/nf+/XY511VmvZ+5zPcva73ezZs2dnAAAAAAAANda+0QsAAAAAAABaJ4cQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUAqHEAAAAAAAQCkcQgAAAAAAAKVwCAEAAAAAAJRinkoGzZo1K5s4cWLWrVu3rF27dmWviWZs9uzZ2bRp07K+fftm7duXe4Zl3/G/6rXv7Dn+lX1HvbnH0giuddSbax2N4FpHI9h31Jt7LI1Q6b6r6BBi4sSJ2eKLL16zxdHyvffee9liiy1W6nvYdxSVve/sOVLsO+rNPZZGcK2j3lzraATXOhrBvqPe3GNphKb2XUXHYt26davZgmgd6rEn7DuKyt4T9hwp9h315h5LI7jWUW+udTSCax2NYN9Rb+6xNEJTe6KiQwg/q6GoHnvCvqOo7D1hz5Fi31Fv7rE0gmsd9eZaRyO41tEI9h315h5LIzS1JzSmBgAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAEoxT6MX0NK0bx/PbTp27JirTz/99DDm6KOPrmj+Sy65JFeffPLJYczUqVMrmgsAgB+2++67h+xPf/pTrv7www/DmNVWWy1kqXEAAABtnV9CAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUAqHEAAAAAAAQCk0pv4BHTp0CNmIESNCdsIJJzQ516xZsyp6z/322y9Xn3322WGMxtQAzImbb745V2+//fZhzOeffx6ytddeO1e//vrrtV0Y1Nm2224bsmuuuSZkxc9tCy+8cBiz4IILhkxjaorWXXfdkO2xxx4h+/Wvf13V/KnXFRurA8Cc6NatW8geffTRXL3yyiuHMVdddVXIRo4cmasnTpw4l6sDWiq/hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSaEz9A0466aSQVdKEulKpJqAHHXRQrn7vvfdq9n4Ac2PAgAG5erPNNgtj7r333pDtv//+ITvqqKNy9WKLLRbGfPDBB3O6RP6NYiPqYtPdLEs3oLvllltydarp7vXXXx+y22+/PWRffvllk+uEWtt1111z9XnnnVfR64pNE1ONFidNmlT9wmgVunfvHrK11147V1955ZVhTN++fUOWui5X4phjjglZ//79c/Xpp58exnzzzTdVvR8Ard+0adNC9uabb+bqlVZaKYzZe++9Q7bLLrvk6lGjRjU5d5Zl2QsvvBCyCRMmhAxqadlllw3ZI488kqsvvfTSMCb1WYvILyEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFG22MXXHjh1DVmwKveeee1Y194wZM0J29tlnh+yiiy4K2aefflrVewLU0sUXXxyyHXfcMVf36NEjjDnwwAND9vrrr4es2gacVOfzzz/P1akm1CkrrLDCD9ZZlmVDhgwJ2YUXXhiyYlO6Z599NoyZPHlyReuClFVXXTVkxUbUvXv3DmP+/Oc/h+zII4/M1VOmTJm7xdHipZpQ33bbbSH7+c9/XofV/P+WW265kJ1wwgm5+qGHHgpjHn300dLWBDRPa6+9dq7eeuutw5hUs/t33303ZEsuuWSufvnll8OYl156KWQ33HBDrr7//vuTa6X5KTbj7d+/fxgzYMCAkHXp0iVXV9rAd9KkSSF79dVXc3WqEfb7779f0fyQkvpc1adPn1x96qmnhjGPP/54yMaOHVuzdbUWfgkBAAAAAACUwiEEAAAAAABQCocQAAAAAABAKdpET4hU/4eTTz45ZCeeeGJN3q/4HOEsy7IrrriiJnPTNp1xxhkhGzZsWK5OPZd4hx12KG1NtB6HH354yIo9crIsy2bPnt3kXEsttVRFWdGIESNCdtRRR4Vs2rRpTc5FVHwG8KKLLlrR64r/7IYOHRrGbLrppiFbYIEFQnbPPffk6tTzhffZZ5+QPfzww02uE7Isy84888yQ9erVK1e/9957Yczw4cNDpgcERVdeeWXIKun/kOqL9PHHH4esuH+XX375MCb1HO155523yTUAbc+PfvSjkBV7wcwzT2V/Dpo5c2bIrr322lxd7BGRZVm2xhprhGy77bbL1W+//XYYs+GGG4bsgw8+aGKVlG3cuHG5et999w1jzjrrrJAVeyqtttpqFb1f8Tn8WZZlffv2zdXPP/98GHPNNdeEbNSoUbnad0rmRrt27UKW+huznhCRX0IAAAAAAAClcAgBAAAAAACUwiEEAAAAAABQCocQAAAAAABAKdrNrqDT6BdffJEtuOCC9VhPKY499tiQpZoXVmL69Okh++1vf5urU43rWluDw88//zzZeLSWWvq+q6U33ngjZEsvvXSTrxs5cmTITjvttJqsqRHK3netcc917tw5ZMsss0yuvuWWW8KYAQMGhKySxtSVKjZzSs39s5/9LGRPPfVUzdZQKfvu/9ehQ4eQrbLKKiE74YQTQrbJJpvk6lQz1YkTJ4Zs8803z9UvvfRSGFPLvdkcuMc2bddddw3ZVVddFbL27fP/vc0hhxwSxqQ+t7VFrnU/7JFHHgnZeuutF7JiI+qtttoqjPnHP/5R1RpSr+vXr1/Iivt+/fXXD2OKDWobwbWu8Yqfx1L7aZtttgnZE088EbJig9hddtkljEk1Qk5du8vUVq51qc9sO+ywQ64uNhr+dz7//POQffXVV02+rkePHiErfhc98MADw5jRo0eHbMstt2zy/ZqztrLvUrp165ar99lnnzCm+P00y7Jsp512CtlCCy2Uq2fNmlXVmlL76Z577qlqrubKPbY6nTp1ClnxbxCrrrpqGPPYY4+FLPU5sbVrat/5JQQAAAAAAFAKhxAAAAAAAEApHEIAAAAAAAClcAgBAAAAAACUInaGauF69+4dslSzo0q88847IUs19b3mmmuqmh/K1rNnz0YvgQZbbbXVQpZqmlQrf/rTn0K25JJLhmzw4MGlrYHyfP/99yErNqLMsizbbrvtQnbcccfl6hNPPDGM6du3b5Pzpxpaf/vtt3GxtGpnn312yFKN5C644IJcrQk11frrX/8asgkTJoTsjTfeyNXVNqFOKTacrjQrNh+m9Sk2IV5++eXDmGJT4izLsiWWWCJX/+pXv6rtwgqmTZsWsuL39/POOy+MmTFjRmlraq1Sn9luvPHGuq7h008/DdkhhxySq5daaqkwpkuXLqWtifor/nt//vnnV/S64me4LIv7J/Vdt1evXiErNr5O/btw3333hezkk08O2ZtvvhkyWo+ZM2eG7IwzzsjVN910U72W0+r4JQQAAAAAAFAKhxAAAAAAAEApHEIAAAAAAAClaPE9IXr06JGrr7vuujCm+KzLSh188MEhu//++6uaC6ARUr0XKnk2dOoZ06+//nquvuyyy8KYiy66qKJ1zZ49O1fPmjUrjDnnnHNCtt5661U0P83PWWedlatfeumlMOb2228PWfE5/6lnRR955JEh++677+Z0iTRTO+64Y8gWXnjhkKWeNX7LLbeUsibanjPPPLOu77fnnnuGLPWc69T9s6h4z6V5mmee+NX8P/7jP0K28cYbh2yzzTbL1euuu27tFlZD3bp1C9npp5+eq1PX7bfffru0NVFfK6ywQq5ec801w5gXXnihXsuhGUv1XjjssMOafF2qL+ZJJ53U5DzbbrttyJZbbrmQHXTQQbn68ccfb3JNtGzFfl9TpkwJY370ox+FrNjvcOLEibVdWAvklxAAAAAAAEApHEIAAAAAAAClcAgBAAAAAACUwiEEAAAAAABQihbfmHqPPfbI1RtuuGHVc40bNy5XP/fcc1XPBfV2xBFHhOySSy6p/0KoiwEDBoTs1ltvDVn//v1DVkmDymIT6izLsk022SRXT5gwocl5/p1iI01NM9ue+++/P2R77bVXyP785z/n6mIzuCxLN4S76aab5mJ1NFL37t1zdepe1q5du5AdeuihIXvyySdrti6opyWWWCJk8847bwNWQlmKTafvuOOOMKZPnz4h69q1a83W8NVXX+XqSZMmhTG33XZbyOaff/6QHXDAAbm6Q4cOVa2pU6dOVb2O5qfYhDrLsuyCCy7I1QsuuGAYc+WVV5a1JNqATz75JGR33nlnrl533XXDmNVWWy1kxet0lmXZaaedlquHDBkyhyukpRk/fnyufuutt8KYddZZJ2QrrbRSrtaY2i8hAAAAAACAkjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQtqjH1iiuuGLIjjzyyqrlSDVV32mmnXP3xxx9XNNeqq66aqw8//PAwZtFFF614bf/qhRdeCNnJJ58cspkzZ1Y1P83P7rvvHrJ+/fqFrLiHr7/++rKWRDOw5JJL5upUc95U87dKpK6HG2+8ccjeeeedquaHSt11110h+8tf/pKrd9xxxzDmsssuC9m9994bsmnTps3F6qiXRRZZJFf36NEjjEl9Pio2MYe26tZbb83VTz/9dINW0ja1bx//O7/U98Njjz02VxevfXPi+++/z9UPPfRQGHPLLbeErHjdrPQ7Zap5dLGB6+DBgyuaq6jY4DrLsuywww6rai7qZ4011gjZVVddFbLi33R23nnnMCa1V2FujBs3LlePGjUqjLn99tsrmmvQoEE/WKfeD/h//BICAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAAStGiGlPvvffeIau24fPf//73kBUbURcbTmdZuqnYVlttlasXWGCBqtaU8otf/CJk8803X8iOPvroXP3tt9/WbA3UV7FJXZZlWceOHUNW/Gf8+eefl7Ym6mueeeKl+fjjj8/V++yzTxgze/bsiuYvNi8844wzwphaNqHeY489ajYXrdvXX38dspEjR+bq7bffPoxJ3XeHDBkSslTjaxor1cD1hBNOaPJ1qevWrFmzarKmLMuydu3a5erFFlssjNlmm21C9qtf/SpkxSbat912WxjzwAMPzOkSaWV69eqVq4cOHVr1XBdddFGu/uabb6qeizl30kknhWzEiBFVzfXMM8+E7He/+13I3nvvvVz91FNPVfV+lTrwwANDVm0j6qJtt902ZBpTN9ZCCy0UsuJ31mHDhoUxxe8cWRYbUWtCTSM8//zzIav0u3RRnz595nY5tFIHHHBArn7wwQcbtJLmwy8hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKEWz7QmReib6kksuWdVcjzzySMhS/SUWXHDBXH3FFVeEMWussUZVa6ilgw8+OGQ33HBDrn766afrtRxqbIUVVghZtc8npGVKPVcydc2q1uWXX56rr7766prNnTJhwoRS56d1e+ONN3J16rn/qb4Cyy23XMj0hGh+Bg0aFLLddtutydc9/vjjNVvD2muvHbLi885TvR4qvTevvvrqufqXv/xlGLPBBhuE7O23365ofmpv3XXXDdljjz1Ws/n33HPPkFV7L06tq5b/ftC0Yg+IU045pap5xowZE7JUr7jUs8zr7d133w1ZsV9dqqddJe67776qXkdt9O/fP2R/+tOfQrbOOutUNf9xxx2Xq5deeukw5pprrgnZRx99VNX7QUpqn1frpptuqtlctC7V9jBuzfwSAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErRbBtTr7rqqiEbOnRoVXNdd911ISs2zsqy2HCp2ibU48aNC9kTTzwRsltuuSVkq6yySq6+5JJLwpguXbqE7O67787VSy21VBgzffr0uFgartiUM9VgtdJGrLQ8ffv2DVnx3+csy7J27drl6kr3yd/+9reQHX744XOyxFIU159aO61bjx49QrbIIos0+brivwv/zuabbx6y8ePH5+qHH344jEl9PqC+ig2fv/vuuybH/DsdOnTI1amGsYceemjIFlxwwVw9derUMObWW28NWWpcsXnnT3/60zBm+PDhIUs1w2bOdO/ePWRXXnllyHr27JmrU43tX3/99Yreszju7LPPbvL9sqz6+2C1Da2pTp8+fUJWbGSf+oz29ddfh+zUU0/N1RdffHEY01y/v91xxx0hu/fee3P1VlttVdXcqf+vqJ9nnnkmZMV7YpZl2YcffpirJ0+eXNH8xYbl+++/fxhz9NFHh2y//fYLWWofQsrAgQNz9Z133ln1XM8///xcrgbaLn/FBAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI028bU2223XVWvmzhxYshGjx4dsk022SRk1Ta+LjaEO+KII8KYSpuKvfjii7k61bR21KhRIevdu3euXnHFFcOYp59+uqI1UF9bb711rk41Jkw14NTIt3Xo169fyFZaaaWQFfdA6p9/6tqQam5fb1tssUXIiuuvtMksLUOnTp1CVmz+m2pEuPTSS9dsDanmv8WmmePGjQtjTjvttJCNGTOmZutizj344IMh++ijjyp67QEHHJCrTzrppDBm2rRpIbvgggtydapJZqV+9rOf5epHH300jFl77bWrnp9/7/DDDw9ZJc1yU42Fi5+1/5311lsvV++7774Vze9zXcuw9957h6zYrDr1zzL1uptuuql2C6uzddddN2SDBw+uydyvvfZaTeahOrvttlvI3nnnnZAV78OVNqYu+o//+I+QXX755SG77rrrQnbKKafk6vPPP7+qNdC6dOnSJWQjRozI1d26dQtjKv0+mvocB1TGLyEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAoRbPtCVGtzp07h2zJJZcMWapvQyWK/R+yLD5v9uuvv65q7pRFFlmkqtdNmjSpZmugebriiisavQRqIHV9qkSq/02q/0O1z2etVuoaXHxWMi3XAgssELLLLrssZJtttlnIUs9eLZo5c2bIivezJZZYosl5sizLvv/++5B9+umnuXrQoEFhTOo513/7299y9fDhw8OYVH8JqtOuXbtcvcwyy4Qx888/f8i+/PLLkP3mN7/J1akeXcXeTFlW2z4g3bt3r9lc/LAbb7wxV//yl78MY6ZOnRqy8ePH5+riHsyy9LOiU/fwSq9RtbLsssvW9f3aui233LLJMan7Ykvp/7DggguGbLXVVgtZqudi165dm5z/jTfeCNk666yTq1N9eqifYv+ssr3yyishS/WTu+uuu0J27LHH5uqbb745jPnggw/mYnU0N8XvmptuumkYc8wxx4RsrbXWytWpe3oq++STT0J2xx13NLlOyLIs+/Of/9zoJTQ7fgkBAAAAAACUwiEEAAAAAABQCocQAAAAAABAKRxCAAAAAAAApWi2jamLDeIq1bNnz5ClmgummmsW/fGPfwzZYYcdFrJvvvmmwtU1bezYsbn6pz/9aVXzfPbZZzVYDc3Za6+91uglUAOHHnpoVa+78sorQ9YcmlCfdNJJIdtxxx3rsRzqIPXPcqeddqrotc8991yuPv/888OYd955J2TFptAzZswIY+aZJ36cueKKK0I2atSoXJ26x1500UUhW2+99XL17bffHsakmlWnmsXTtGJjwAEDBoQxqcbUqYaqxabQt9xySxhTyybUKQMHDmxyzAMPPFDqGtqKhRdeOFen9kSqifwGG2xQ1ful/r1P3QfLNGzYsJD17ds3V5955plhTKpBME1bY401Qla8ZqXuU83VEUcckauPPPLIMGbxxRevau4PP/wwZFtvvXXIUs3iadtSe6LYhDrLsuzJJ5/M1am/1Rx33HE1W1dbV/xb25AhQyp6Xbt27XJ16jtr6rPY5ptvHrKhQ4fm6l//+tcVraFo+vTpIUs1Zb/00ktD9vjjj1f1nrQ9qcbmbZ1fQgAAAAAAAKVwCAEAAAAAAJTCIQQAAAAAAFAKhxAAAAAAAEApmm1j6nvuuadmc1XShDol1Thrs802m9vl/KBevXrl6g4dOpT6fjResVFT+/bxbPCf//xnyFINXGk7Tj311Lq/5ymnnJKrl1122TBGE+rWrdLmlF988UXIBg0alKu//vrriuaab775cnXxmvnvvP/++yH76KOPcvUdd9wRxowdOzZkxaZ3qc8VrsnVeeyxx0J266235urtt98+jPn9738fstQ/u65du+bqSvdPtbp06RKytddeO1d/9dVXYcxtt91W2ppaq3XXXTdkyy23XK5Ofabq06dPyB599NFcXWxGn2VZNmvWrDld4r+VWlct7bnnnrl60003DWNSzVqL/+59+eWXNV1Xa/Daa6+FrLjvVlhhhXot5/907tw5Vx911FFhTOpz22677Zar52ZvTpo0KVen9t3rr79e9fy0bZV8Bv3ggw/qsJK26+yzz87Ve+yxR0WvK372mjFjRhiTala9yCKLhGyeefJ/wpw9e3ZFayj69NNPQ7bTTjtVNRf8O4cddliuvuGGGxq0kubDLyEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFM22MXWqEdree+8dsquvvrq0NSy00EKlzV1ryyyzTK5ONT2keSo2U0o1PnzllVdCprFb61BshpllsYlpypFHHhmyN998M2QvvPBCyCppHl1sIpxlWbbFFls0+bpKFRsfpvZzsVkiLUexmXSWZdnAgQNzdaohcarh80033ZSrO3ToUNEaUo2pK/HZZ5+F7Nxzz61qLpr27bffhuyUU07J1euss04Ys9VWW4Vs6623DlnxHrvRRhuFMalrTfGa9Oyzz4YxqWviXnvtFbKNN944V1faEJ0flrqGFP+59e7dO4wZMGBAk3OnPotV25h6ypQpIfvrX/8asuJ3mi233DKM2WGHHUKWatxZ1KNHj5BdeeWVISs25D788MPDmLberPq0004L2Z///OdcXfx3Psuy7N133w3Zf/7nf1a1huL3vtR7pu7D1fr4449DtvPOO4fsiSeeyNXffPNNzdZA25Jq7n7ttdeGrNgMvfjvIrX15JNP5upddtkljOnUqVPIio2pO3fuHMYstthiFa2hOFe1Uo3Ov//++5AVv4dkWZbdd999udq+g8r5JQQAAAAAAFAKhxAAAAAAAEApHEIAAAAAAAClaDe7+LDchC+++CJbcMEF67GeH5R6/tvCCy+cq++///4wZuWVVy5tTWV76aWXQnb66aeH7LbbbsvVFfxjnSuff/558tndtdRc9l3ZLrjgglx96KGHhjHF5w5mWW2fz99SlL3vGrHnUv0fUs+5LkpdD6dPnx6y1LObU8/IrmT+Wl5XivOvtNJKYcyrr75as/ebG61x31VrueWWC1mqZ03Kiy++mKtTz8LecMMNQ7bBBhs0OXfqvjhy5MiQpZ712hy5x+alnh184IEHhiz1z7xbt25Vved3332Xq1O9K1LrSl0733nnnVyd2udvv/32nC6x5lrDta7Yw+Ccc86pap5i36Isq7wnxIQJE3L1dtttF8aMHz++qnUtueSSIbv11ltDttpqq+XqavtZpPrxXXfddVXNldISr3Wpa0qxv9cqq6xSs/crW7E/yRVXXBHGjBkzJmRTp04ta0mlaw3XutYk9beahx9+OGSpe26xN8m9995bu4XVWGvcdwcccEDIdt1115DNO++8ubp4j5oTxX6H1c5Vy++6xb/FZVl6LxZ7gk2bNq2q96tUS7zHthSPP/54yFI97CZPnpyrf/nLX4Yxqb/5tmRN7Tu/hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSzNPoBcyJVKOYDz/8MFevueaaYUyvXr1Cdsopp4Rs//33n4vVzblU469HHnkkV995551hTLFZIq3f7bff3uglUJJUU99UA8D111+/ybnmm2++irJ6KzbpzLIs22233XJ1c2jKStPefffdkD3wwAMh23jjjUO26qqr/mBdqfvuuy9kv//970PWUppQ07QZM2aE7IILLghZai8efPDBuXrFFVcMY37+85+HrEOHDj9YZ1m6IfCrr74asquvvjpXv//++2EMtVG8FqS+OwwdOrTJeVINK4vNBbMsy4477riQffPNN7n6o48+avL9KpW6n2600UYhK16DL7zwwjCmLTaSrIVUI9GtttoqVxevO1mWZZtttlnIll9++Zqt6/nnn8/VzzzzTBhz8803h6zYVNu9k1rq0aNHyIrNpE888cQwJvV5c9iwYSFLNbCmfi6//PKKsp49e+bqTTfdNIx56623KnrPWjWmHjRoUMhScw0ZMiRkxX293XbbhTHbbrttyI4++uhcnWpS/PHHH8fF0mL17t07Vy+55JJhTGtrTN0Uv4QAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUrSoxtSVSDVtLjavzrIsO+iggyrKoGzF5oft28ezwWLTOFqPVIPDc889N2RrrLFGrl5ggQVKW1Olnn766ZCdd955IXvxxRdDphF1yzR9+vSQFRsMZlmWjRw5MmSHHXZYVe952223Nfl+GmmSZVn2+uuvh+zQQw9twEpolOL3gIsuuiiMSWUt2SeffBKyP//5z02+7tprrw1Z8d+hxx9/vOp1tSXvvPNOrk410D3++ONDlvrMX61Zs2blavfF1q1Lly65OvX5rGwdO3bM1ZtvvnkYc+GFF4ZsoYUWytV///vfw5his/cs07C3JSvep66//vqazf3kk0+W+rqf/exnITvttNNy9cCBA8OYe+65J2Rvvvlmrk7dv2ldZs+enauL9+q2yC8hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKEWr6wkBLU0lz4nbbrvtQnbWWWeVtiYa64EHHghZ8dmohx9+eBgzdOjQmq3h6KOPbnLMX/7yl5BNmjSpZmugZfjiiy9CduSRR1aUAVC+VI+ISvpGUDupHg36NlCt0aNH5+pXX301jHn55ZebnGeJJZYIWbHHSZaln3m/2mqr5eoVV1wxjEn1e9h9991z9dixY5taJjRMqjfSkCFDGrASmpP/+q//Ctk666wTsuLf7FK9Qtoav4QAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUmhMDQ12+eWX5+pDDz00jBk3bly9lkMzVdwD9gQAALQ9xe+LhxxySBgzatSokPXs2bOq95swYULIxo8f3+QannrqqareD6A5Gz58eEUZkV9CAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUAqHEAAAAAAAQCk0poYGe/3113P14YcfHsbsv//+IXvyySdLWxMAAADNz6uvvpqrDzrooDAmlQFAI/klBAAAAAAAUAqHEAAAAAAAQCkcQgAAAAAAAKVwCAEAAAAAAJRCY2poZi655JJGLwEAAAAAoCb8EgIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUlR0CDF79uyy10ELU489Yd9RVPaesOdIse+oN/dYGsG1jnpzraMRXOtoBPuOenOPpRGa2hMVHUJMmzatJouh9ajHnrDvKCp7T9hzpNh31Jt7LI3gWke9udbRCK51NIJ9R725x9IITe2JdrMrOLqaNWtWNnHixKxbt25Zu3btarY4Wp7Zs2dn06ZNy/r27Zu1b1/u07zsO/5XvfadPce/su+oN/dYGsG1jnpzraMRXOtoBPuOenOPpREq3XcVHUIAAAAAAADMKY2pAQAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBTzVDJo1qxZ2cSJE7Nu3bpl7dq1K3tNNGOzZ8/Opk2blvXt2zdr377cMyz7jv9Vr31nz/Gv7DvqzT2WRnCto95c62gE1zoawb6j3txjaYRK911FhxATJ07MFl988ZotjpbvvffeyxZbbLFS38O+o6jsfWfPkWLfUW/usTSCax315lpHI7jW0Qj2HfXmHksjNLXvKjoW69atW80WROtQjz1h31FU9p6w50ix76g391gawbWOenOtoxFc62gE+456c4+lEZraExUdQvhZDUX12BP2HUVl7wl7jhT7jnpzj6URXOuoN9c6GsG1jkaw76g391gaoak9oTE1AAAAAABQCocQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUIp5Gr0AAACA1qhr164hGzZsWMhOOeWUXD106NAwZvTo0bVbGAAA1JFfQgAAAAAAAKVwCAEAAAAAAJTCIQQAAAAAAFAKhxAAAAAAAEApNKYGAKBNOPfcc0N21FFHhezUU0/9wTrLsuz777+v3cJotfbcc8+QnXjiiSGbNWtWHVYDAACN4ZcQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUAqNqaGFWnjhhXN1qvHhMsssU9Xcb7zxRshGjRoVsttuuy1X77LLLlW9H0A9LLDAArn6uOOOC2N23333kC2++OKlrYlyDR48OFcffPDBYUyqIfBJJ52Uq88///wwZurUqXO1NtqGrbbaqtFLAACAhvNLCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErhEAIAAAAAAChFq2tMPWLEiJANHz68oteOHDmyyTFjx46tKINKdenSJVcfffTRYcwOO+wQsp49e+bqPn36VPR+33//fZNjOnToUNFc3377bUXjAGqlV69eIdt4441DdtRRR4Vs+eWXz9WdO3cOYz744IO5WB2N1L59/G9ris3HO3bsWNXce+yxR8guvPDCquaiddt6661z9VJLLVXR64qfz2bMmFGzNdG6pK5jf/zjH3P1rrvuWtFcZ555Zq6ePXt2GHPfffeF7Omnnw6Z7wVAc9CuXbuQde/evcnXbbbZZiHbcccdQ3bbbbf9YJ1lWTZt2rQm34/Wr7gXt9pqqzBmzTXXDFnxs2SWZdlyyy2Xq//nf/4njNlwww1D9s9//jNXz5o1K7nWevFLCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErRonpCDB48OGTFfg+pMZWqpHdEakyxJ0Sqt4S+EWRZlv3oRz8KWfEZbcUeEVmWZV9++WXInnrqqVyd6iXx3XffhWzSpEkhKz5b9pZbbgljunbtGrLzzz8/ZLRexT3whz/8IYzZZZddQnbdddeFLPV8ddqWRRZZJGQbbLBByLbffvtcvf7664cx888/f+0WRou18sorhyz1bNRqPPvsszWZh9Yl1UNr9913z9VLLLFERXOddtppufqvf/1r9Quj1Uj1Lrr88stDVvz8lertkPKb3/ymydcVx2RZlj3wwAMhe/vtt3N16nnVl156acj0kgBqaYsttgjZHXfcUbP5N91001x90kknhTGpZ/+//PLLNVsDzU/q++g555yTq/fbb7+q5y/en3/84x+HMW+99VbIVltttVw9fvz4qtdQC34JAQAAAAAAlMIhBAAAAAAAUAqHEAAAAAAAQCkcQgAAAAAAAKVo8Y2p56YRda0U15BaU6oxdaq5Jq1H6p/vTTfdFLJvvvkmV1900UVhTKqJ23vvvTcXq8u7+uqrc3WqOXaqkfCLL75YszXQ/I0ePTpXp651M2bMCNmNN95Y1pJoplJNp0eNGpWrt9lmmzCme/fuZS0py7Is++qrr3L1LbfcEsacffbZpa6B8vTv37+0uQcMGBCyxx9/vLT3o2VYZZVVQpZqiFmJYmNqyLIs22677UJWbH5eqc8++yxkPXr0qGqujTfeuKrXpRp3nn766VXNBbQ9vXv3ztVDhgwJYy655JJ6LSfLsnSD4PXWWy9kGlO3HjvssEPIhg0bFrKBAwfm6mJz6TlRvIcvtNBCFb2u+H13ww03DGMmTJhQ9brmlF9CAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUAqHEAAAAAAAQClaVGPqarVr166q140YMSJkw4cPr2quVAPXMWPGhEyz6tZj6aWXDtmPfvSjkA0dOjRXF5v/1tqWW24Zsp133jlXf/HFF2HMAw88UNqaaH5+85vfhGydddbJ1ak9MXXq1JDZO61b586dQ3bUUUeF7Ne//nVpa/jggw9C9sgjj4TsjDPOyNWvv/56aWui/jbZZJMmx5xzzjkhS92vt95661y96KKLVr8wWq1bb721yTHff/99yA444IAylkMLl/q+eP7551c11+TJk0OWaia9/PLL5+rU578VV1yxqjWk7LfffiG7+uqrc/WHH35Ys/eDlK5du+bqTp06VfS6b7/9NmRffvllTdZEtOSSS4as2GR39dVXr9NqaCtSDZ/POuusXP2rX/0qjEl9J67E+PHjQ3b66aeHrPi9dbPNNgtjTjnllJD1798/V//2t78NY1L/e1KfX2vBLyEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAoRavrCVHLngqpnhCVZJX2jUg997M4V+r9aH5++ctfhiz1DNcbb7wxZPfee28pa8qyLOvbt2/I/vKXv4Tss88+y9UbbbRRGJPqE0HrsPbaa4csdR0rPuf/iSeeCGOKe4nWL9X/4ZhjjqnZ/I8++miuLj4LNsvS19ZPP/20Zmug+Uk9d3Xddddt8nWpnhDnnntuTdZE2zPffPM1Oea+++4L2TXXXBOyXr165eopU6ZUvzBapGHDhoWsZ8+eFb32o48+ytXFnnNZlmUvvvhik9ntt98exqS+X2+11VYh22677XJ16rna//znP0PmO0br0KVLl5AVey9kWZZNnz49V3/11VcVzd+xY8eQde/ePVdvvvnmYcxPf/rTkBWfpd6nT5+K1vDCCy+ETE+CObfCCiuELPX5PrV/Fl988VLWRNtU7AGXZem+CqusskpV88+aNStXP/jgg2FMqlfS+++/3+TcL7/8csj69esXsgMPPDBX77TTTmHMvvvuG7JKr81zyi8hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQtvjH12LFjf7Cuh0qaR1farLo4TmPqlmG55ZYL2ddffx2yQw89NGTFZjXVWnHFFUOWai6XauZZbMjz0ksv1WRNNE/F/ZpqBJZqNnzZZZdV9X6phoYrrbRSrr7ooouqmpv6SjWDS13XUopNy/fYY48w5sMPPwzZs88+W+HqaEtSTdX69+8fsvfeey9Xz5gxo6r323bbbUM2atSoquai+Ut9rrv44otDVmyKOjcqaXJN6zJgwIBc/ZOf/KTquXbZZZdcXe29M3WNfOCBByrK7rzzzlx98MEHhzHnnHNOyIqNiml+evXqFbI111wzV//+978PY3784x+HrNhM9corrwxjUt9ri++XZVm26qqrhqxWUp9JH3/88dLerzX75S9/mat32223MCZ1323Jttlmm5DNnDkzV1999dX1Wg5ZlvXo0SNkm2yySciqbUL9j3/8I2RXXHFFrk7dA2vpnnvuCVnxO9NCCy0UxqT+zrP77rvXbmH/wi8hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQtqjF1S2nSnGqOXWljalqmX/ziFyGbd955Q5Zq6vXJJ59U9Z4/+9nPcnWqGdjSSy8dsuOPPz5kTz/9dFVroPlLNR568MEHc/WZZ54ZxlTbhDrV8OnCCy8M2ZgxY6qan/pabLHFcnWqEeUiiywSslmzZoWseB9MNc6CShUboGZZlu2zzz4hu/XWW3P1F198UdX79enTp6rX0TKlGk6vv/76pb5ncW8efvjhYcwKK6wQslRj1rXWWqtm66I2+vXrF7K77rorV6c+s1VqpZVWytWnnXZaGNO1a9eQtWvXLlfPnj07jPnLX/4SsrPOOitkxc8Iqc8MNC/t28f/JnWvvfYK2XnnnReyBRZYoKr3LDadTn1PqNbEiRND9swzz4TslltuydX//Oc/w5gnn3wyZKnPt+SlGv3edNNNuXr++eev2ful/o6R2tdrrLFGzd6zEkOGDAlZ8e83n376aRhzxx13lLamtqZ4Tz3kkEPCmNR3h2o99NBDISu7EXXRe++9V9XrFl988Rqv5N/zSwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAoRYtqTN1cDR48OFfPTcPVkSNHzuVqaITRo0eHLNWsukOHDlXNv/baa4es2CBslVVWCWNSDdFTTYhpvf7whz+ErNhU7cYbb6zZ+6Uad6YafNa7SRPVOfroo3P18ssvX9HrPv/885C9+OKLtVgSZFmW3mPrrbdeVXN99NFHc7sc2oBiA99KpZqhL7jggiErfn9Ifa6r1KWXXpqrDzrooKrnojY6deoUsgEDBtRs/gsuuKCq11XSmDrVEH2dddYJ2VZbbVXVGqifYsPe1N8eTjrppHotJ8uyLPvss89CVmxknGVZ9u6774bskUceydWpz5ozZ86sfnH8oNT3vj/+8Y8hq7YRdfF7SJbFz3+rr756GFPJPe93v/tdyN54442Q9e3bN2Snnnpqk/OndO7cOVfPO++8Vc1DZbbccstcPWLEiKrneuutt3L1DjvsEMb8/e9/r3r+Wkndr4sNulMuu+yyMpaT5JcQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlEJPiH9RfBZrsddDrY0dOzZkc/OcMhpn8803D1nq2Yep58QVn+H6zDPPhDEnn3xyyIrPe7v//vvDmIsvvjhktF6LLbZYyDbaaKOQFffrp59+WtX7pXo9nH766SF7++23Q/bee+9V9Z7U1yKLLFLV61LPnvyv//qvXJ26Ph177LFVvR/MjebwDFeal8UXXzxkqWflpzz++OO5+owzzghj7rnnnpAVe0BU+n4pxWcVp/oFvPnmm1XPz5w77LDDGr2EqnXs2DFkm2yySciKz4efmz6JlKP4rPx693/Istjv4aijjgpjJk2aVK/lMAeKn+8feOCBMGaeeZr+E2Oqd8dPf/rTkKX6eRTvjdddd10Yc8wxxzS5hm+//TZkxb6JWRb7qGRZ7G2Y6t9Z7FeScu2114aslr0a25IePXqELNVTpBKvv/56yDbeeONcnepR05Lde++9dXsvv4QAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUmhM/S/KbESdakJdbN5Fy3XhhReGLNUQeL755gtZqhlXJYoNnTbddNOq5qH12GCDDUKWauj12muv1eT9tt5665D17ds3ZIccckhN3o/6u+qqq3L1aqutFsYMGDCgork6deqUq1PNwlKNsI8//vhc/f7771f0flCpBRZYoMkxvXr1ClnqGnjHHXfUZE3UV/HzWbWfzbIsy3784x/n6vvuuy+M6devX9XzV6J79+65uthMlPrr2rVrqfO/9NJLuTrV/Py8885rcp4HH3wwZAMHDgxZqvnsCSeckKs1pm5+br755ly9zTbbhDGpz2Kpf97LLLNMVWso3jvnn3/+MGa77bYL2YwZM6p6P2rn8MMPz9WpfVFsHJ1l8Z9d6lpU7T/f77//vqKsWqlm1cW1vvzyy2FMqml3sblxJU28qcxmm20WsuWWW66quVJ/K2mO3z833HDDkP3pT39q8nWXX355yKZPn16TNVXCLyEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFDqh/IuRI0fm6uHDh9dsbk2oW7dU4/G77747ZDvuuGNV81933XUhm5umibROqabBX3zxRcgmT55c1fwLLrhgrj7//PPDmFSj1ocffriq96Pxiv/s1l133TAmda88+OCDq3q/XXfdNWRLLrlkrh48eHAYU8sGdLQ9N954Y8gOO+ywXL300kuHMUsssURpa6K+Fl544Vy95pprVj1X3759q3rdBx98kKs33XTTMGa99dYL2cUXX9zk3JdddlnIjjvuuJA99NBDTc5FdX7729+GbPvtt2/ydal/Jttuu21N1pQyZMiQkP31r38N2TrrrBOyX/ziF7n6pJNOCmNGjRo1F6tjbn388ce5OvXPO6Vjx44hKzamTn0POeecc0JWbHy9+eabhzH77rtvyC655JIm10nt9O/fP2Sp73lFEyZMqGiu1iTVYHreeedtwErahm7duoXs2GOPDVkljb/vuuuukJV5j50bAwYMyNVXX311GJPad08++WSuHjZsWBiTaihfFr+EAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBR6QvyLESNG/GCdZelnUY8ZM6bJuVNj9IloPTbccMOQbbDBBjWb/8UXXwzZ9OnTazY/rUPqmc977rlnk9nbb78dxqSe3Vl8nvGzzz4bxuj/0LpNmTIlZEcccUTInnvuuZAVe4gUe4z8Oz/72c9y9dlnnx3GHH300RXNBSmffPJJyIrPqH7mmWfCmFNPPTVk77zzTsjuuOOOuVgd9bD33nvX9f0mTZoUsl/+8pe5+o033ghj1l577areb9FFFw1ZqmcU5fnHP/4RsgUWWKABK/lhX375Zch22WWXkL3yyishKz6LetCgQWHMWWedFbJvv/12TpZIQqqnVurZ/I8//nhV86f+Gb366qs/WGdZlk2bNi1kqWewF6We+U59bbTRRiFbccUVm3zd6aefXsZympXi/vzJT34SxqT+bljUFv6/KsM222wTstTeLPY5SN2HDzrooNotrEqp3hWp/z133nlnrk59tvv6669DtvPOO+fq1H2+nvwSAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAEqhMfUcGjt2bFWvq6QxDS3HCiuskKuvueaaMKZnz54hSzVrHT9+fK7+9a9/Hcacd955IVtmmWVydXNoqkNjvf766yG7+uqrQ3b55Zfn6lRzou7du4es2Ohou+22m8MV0hp9//33Ibv22mubHHfRRReFMZU0q956661DdvLJJ4ds+vTpTc4F/86bb76Zq1NNfPv27Ruy7bffPmTFptYzZswIYyZPnjynS6SF+OCDD0K22WabhSzViLqo+NmvUr///e9D9tRTT1U1F21PqsHx9ddfH7J99903Vw8ZMiSMGTZsWMg0Z517hx56aMjmm2++kK2yyir1WM7/uf/++0NWvNYtu+yyYczPf/7zkKU+N3711VdzsTp+yMUXXxyyYqPflNdee62M5TRM6m86N954Y67+xS9+UdFc//M//5Orb7/99uoX1oYNHDiwqtddeeWVIZs4ceLcLmeO9O7dO2QHHnhgyEaMGNHkXKnr3z777BOy1D28kfwSAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAEqhMTU0oXPnziG75ZZbcnWqYdHOO+8csjvvvDNkxWat06ZNC2MOP/zwkHXp0iVX9+rVK4yZMmVKyGhbjjrqqJBdccUVufrTTz8NY1LNkHbZZZdcPXXq1LlaG21LsYllu3btwphUQ+uiJZdcMmSp/ZpqfgkpCyywQMh23XXXXJ26N6fsuOOOTWape/OWW24ZsieffLKi96R5SzU132KLLZrMUmMqbcY4a9asXH3fffdV9Dqo1AUXXBCynXbaKVd369YtjEk1HD7nnHNy9YwZM+ZucWRZlmUrr7xyyM4888xc/eCDD4YxQ4cODdnbb78dshdffDFXpxqRL7HEEiFbaqmlQlb07bffhkwT6vpKfU4vNqb+61//Gsak9kpzsNBCC+XqPn36hDGbbrppyPbaa6+QLbfcck2+34cffhiyrbfeOle//PLLTc5Dls0333y5epNNNqnodV9//XWufuyxx2q2pkqttdZaufqGG24IY5ZeeumK5ho7dmyuPuWUU8KYRvxvnFN+CQEAAAAAAJTCIQQAAAAAAFAKhxAAAAAAAEApHEIAAAAAAACl0JgamnDqqaeGbIUVVsjVl156aRhz0003VfV+RxxxRMg233zzkO2xxx65+ic/+UkYU2yEk2WVN9ek9XrjjTeaHLPVVluF7LzzzithNbQkqQaDX375Zcg++eSTJudKNeZaY401QnbIIYc0OVfq+kfr1rFjx5DtvffeTb4u1QRws802C1mZjc179eoVslTDYY2p6+ehhx7K1QcddFAYs+CCC1Y19/LLLx+y0047LWTFJqDFBqD/znfffReyE088MVc/++yzFc1FeRZddNGQXXnllbk6dX0q81o0N15//fWQ/e1vf8vVG220URizwQYbhGzxxRfP1c21sW1ztv/++4cs9V30uOOO+8G6ESZMmBCyu+66q/4LIaeSe9CGG24YsrPPPjtkH3zwQU3WNDdWXXXVXJ1qQl2tc845J2Q333xzyDSirs6uu+6aqytt5Fz8O17xHjU3Uvf03XbbLWRnnnlmrk79e/XNN9+E7IQTTgjZn//851w9efLkJtfZHPklBAAAAAAAUAqHEAAAAAAAQCkcQgAAAAAAAKXQEwL+xTzzxH8lBg8eHLIZM2bk6uIzXbMsy3r06BGy1PPepk+f3uS6brvttpAVnxGbeuZw6rn+119/fZPvR9uy/fbbh2zWrFkhKz7TkNZv6623ztX/+Z//GcZMmTIlZGPGjAnZ+PHjm3y/VVZZZQ5W9/9bbbXVQta7d++QtdRnZ7ZmqfvuWWedlauL+zDL0s9iTc1VlHoWa/FZ/Cmpa2KqZ8Ppp58esmWWWSZX//Wvfw1j3nzzzSbXQHkeffTRXP3b3/42jDn++OND1r1797KWlPTuu++G7KKLLgrZBRdcUIfVMCfmm2++kBV7JqyzzjphzBNPPBGyO+64o3YLq0DXrl1DtsMOO4RMf6bGSX3G2nLLLUP2i1/8Ilf/+Mc/DmNWX331kHXu3DlkAwYMyNU/+tGPwphUv4d77rknV6fumx9++GHIqK+33norZP3792/ydcW+lc3VzJkzQ5bqnzRx4sSQFXupvP/++2FMql8T1Rk+fHiTY7766quQPf744zVbw6BBg3J16rPXSiut1OQ8zzzzTMhSfciee+65OVhdy+KXEAAAAAAAQCkcQgAAAAAAAKVwCAEAAAAAAJTCIQQAAAAAAFAKjal/QKohcSVNUWi5Ug2E/vGPf4Ss2Hgt1cAw1egr1QDpf/7nf3J1sel1lmXZt99+GxdbgVSTJCg68sgjQ5ZqhFjtPqTlWn/99XN1qjFhqkHwbrvtVlFWiWLT4FRj4W7duoWsY8eOVb0f9dW+ffzvYY444ojS3m/cuHEhS92biy655JKQ3XvvvRW95wMPPFDROJqPc889N2Q33nhjyA4++OCQFRsQb7PNNmHMYostFrJik8PUte6KK64I2RtvvBEymp9U08zi5/TUvrjmmmtCdvLJJ4fs8ssvz9VTp06dwxX+P6km1Mccc0zIlltuuarmTzXzfO+996qaix/25ptvVpRVInWvLl7rUp+7Ut9rv/7666rWQH1tscUWTWZnn312vZYzR1LfWYvfd1P78Nprry1rScyF4r0x9flo0qRJIXvssceanHufffYJWeqz3fLLL5+rO3Xq1OTcWRabTt96661hzOTJkyuaq7XwSwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAoRatrTJ1qUjJ27NiQpRoTltl0utjck5Zjzz33DFmxqdqZZ54ZxlTasG3llVeual1F06dPD9k///nPmsxN69anT5+QpZpf0vbU6vo0N4r39e+//z6Mueuuu0L2ySeflLYmGu8Pf/hDyFLNhItS98XUnoKiiRMnhuzEE09s8nVHHXVUGcuhhUk1zRw6dGiuHj16dBiz6KKLhmyVVVYJ2WWXXVbVutq1a5erU9+lq/Xoo4+G7De/+U3IUs2LaV5mzZoVslSzdVqPVBPz8847L1dfddVVYcyIESNC1r9//6rWkGoQ/Pnnn1c1V7Wvo/FeeumlXL3SSiuFMUsssUTI3nrrrSbnTt1j55133iZf98orr4TsggsuCNnVV1/d5FxtjV9CAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUIpW1xMiZfDgwRVl1Ro5cmSuTvWgSGW0DDNnzgzZk08+mau33nrrMGannXYK2eqrrx6y7t275+rtttuuonW99tprufpPf/pTGDNhwoSK5oKiMWPGNHoJNAPFa1vq2awrrLBCRXN16tQpV2+zzTYVve6mm27K1alnZt94440VzUXzk7rHdujQoQErAaiv8ePH5+pUr4d99tknZMccc0zIevXqVbuFVWDy5MkhO+ecc3J16lnY+j9Ay1XsGZPqs3DkkUfWazm0EcU+I7fddlsY07Fjx5AtvfTSNVvDddddl6uPP/74MCbV+4nILyEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFK2uMXWxSfS/M2jQoJCNGzeuydcVm6JAlmXZZ599FrLLLrusASuBOTdlypSQ9evXL2TvvvtuPZZDM1K8to0aNapBKwGA1i31feJ3v/tdyC655JKQtW+f/28LDzvssDCmS5cuIevcuXOuXnnllcOYhx56KGSXXnppyL7++uuQAcDcuO+++3L1c889F8YMHDiwyXnee++9kN1www0hu/rqq0M2YcKEXD1r1qwm3480v4QAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUrS6xtQaRwPMmVNOOSVko0ePDtmxxx6bq6+66qrS1gQAQFRJA+gzzjijDisBgHLNmDEjV6+55poNWgm14JcQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUIpW15gagDlz//33h2yhhRZqwEoAAAAAaG38EgIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUlR0CDF79uyy10ELU489Yd9RVPaesOdIse+oN/dYGsG1jnpzraMRXOtoBPuOenOPpRGa2hMVHUJMmzatJouh9ajHnrDvKCp7T9hzpNh31Jt7LI3gWke9udbRCK51NIJ9R725x9IITe2JdrMrOLqaNWtWNnHixKxbt25Zu3btarY4Wp7Zs2dn06ZNy/r27Zu1b1/u07zsO/5XvfadPce/su+oN/dYGsG1jnpzraMRXOtoBPuOenOPpREq3XcVHUIAAAAAAADMKY2pAQAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBTzVDJo1qxZ2cSJE7Nu3bpl7dq1K3tNNGOzZ8/Opk2blvXt2zdr377cMyz7jv9Vr31nz/Gv7DvqzT2WRnCto95c62gE1zoawb6j3txjaYRK911FhxATJ07MFl988ZotjpbvvffeyxZbbLFS38O+o6jsfWfPkWLfUW/usTSCax315lpHI7jW0Qj2HfXmHksjNLXvKjoW69atW80WROtQjz1h31FU9p6w50ix76g391gawbWOenOtoxFc62gE+456c4+lEZraExUdQvhZDUX12BP2HUVl7wl7jhT7jnpzj6URXOuoN9c6GsG1jkaw76g391gaoak9oTE1AAAAAABQCocQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUIp5Gr0AaK3+9re/hWyppZYKWb9+/XL1zJkzS1sTAAAAAEA9+SUEAAAAAABQCocQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlEJj6hoYMWJErh4+fHgYM3LkyCZfR8vVu3fvkA0YMCBkPXv2DNl6662Xqx9++OHaLQwAAAAAoIH8EgIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABKoTH1HBozZkzIBg8eXLP5i3ONHTu2ZnNTnt122y1kPXr0qOi1r732Wq2XA9Bqde7cOWTffPNNyK644oqQHXDAAaWsiZZjyy23DNkRRxwRsuLnsVmzZlX9nkOGDMnV48aNq3ougGrNnj07ZJdeemmuPvbYY8OY6dOnl7YmAKDt8EsIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIXG1D+g2ibUqWbSI0aMCFmqOVjRyJEjK5qLxtpuu+0qGvfFF1+EbMqUKbVeDkCbkrqfVnKPpeVaYYUVQrbEEks0OW748OFhTJcuXUJWbEQ9N/vpjjvuyNW33nprGLPffvtVPT/NS/fu3XP1r371qzDmoosuClm1zc+PPPLIkE2YMCFX33333VXNTct14IEHhiy1x4rXnvPOOy+M+cc//lG7hQHUUM+ePUO2zDLL1Gz+gQMHhmzQoEG5utrPiDvuuGNVr6P+BgwYkKs322yzMGb33XcP2corr5yr27ePvwN44YUXQnb99deH7Msvv8zVV155ZXqxzZxfQgAAAAAAAKVwCAEAAAAAAJTCIQQAAAAAAFAKPSH+RbHfQyX9H1LWX3/9uV8Mzdp8882XqxdZZJGKXnfZZZeFbObMmTVZE1Sq+OzM1PPQL7300pBNmjQpV6eef3nYYYeFbMaMGXO6RPi3ll9++UYvgTpbdNFFQzZ69OiQLbnkkiFrDr1BFlxwwVzdv3//MKZr164h++qrr0pbE7Xx0EMPhWzppZfO1f369QtjUs/mr3avnn/++SEr7p3x48eHMbvttlvI3nnnnarWQPOz6aabNnoJAKUbMmRIyG688caQtWvXLlfPzefDWs5F87PvvvuG7PTTT8/VPXr0qGiu4t5Iff4r9o3Isiz73e9+F7Lvv/8+V6+00kphzB//+MeQvfjii00ts678EgIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK0WYbU6eaTo8ZM6bJ140dOzZk1TaiHjlyZMiGDx/+g3WWZdmIESOqej9qZ4sttsjVSy21VEWvu/XWW8tYDmRZlmUDBw4M2bBhw0K29tpr5+pU09eUSppwfffddyE76qijQqZZNZXq1KlTrj7++OMbtBIaZb755gvZEkssUdFrX3311Vz9hz/8oSZryrIs69WrV8hOOumkJl/385//PGTrrrtuyB588MHqFkYpttxyy5CttdZaIevSpUtN3i/VTPqEE04IWfHenGVZduKJJ+bqvn37hjF77bVXyHzHAKqVuvalGqcuu+yyuXq11VYLY1L3xNQ999prr83Vqb+v0HJ169YtZPvss0+uTt3Lmqvnn3++0UugAqnvmpU2oi5Thw4dcvVBBx0UxsycOTNkGlMDAAAAAABtgkMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAAStFmG1OnGj5XYty4cTVbQ6rJdSXrSjXQrrY5NtUpNqJONehNmTBhQgmroS3o2bNnrr7iiivCmE033TRknTt3Dlml+7Ua+++/f8hSDdlT1zFIOfroo3P19ttv36CV0Chvv/12yHbccceKXnvbbbfVejn/p3///iGrpDE1LVO/fv1CVm0T6kcffTRkxb06evToMOadd96paP4HHnggV//mN78JY0aNGhWyhRZaKFefdtppYcyUKVMqWgPQMqWa3e+www4h23vvvXP1WmutFcakGgvX0h577JGrNaZuuQYOHBiyZ599NmSzZs2qav727fP//XW186Tm+uabb8KYjz/+OGQffPBB1e/J3Jtnnvjn79TfLpZYYomQFf9+MmPGjDAm9c+8qLh3siy9FxdeeOGQderUqcn5WwK/hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUbaInxODBgyvKilI9G0aMGDHX6/mh+StRydop19ChQxu9BFqxzTffPGRXXnllru7du3dFc6X62BTnWn311Sua66ijjqpoXNHJJ58cMj0hSOnQoUPIKt2fRWX2AqDx6v3PN3XNvfDCC6uaK3Vd/u///u+q5qIcgwYNCtn5559fs/kvueSSkJW5p9dbb72Kxh1yyCG5+qKLLgpj9IRoGVL309SzqGnbunbtGrKDDz44ZGeeeWZV86eem/7888/n6mWXXTaM6dGjR8i++OKLkKX6ztEyPffccyFL9YRYbbXVqpr/wQcfzNVXXXVVVfOkfPLJJyHzXbf56dOnT8iq/Syf6j9z9tlnVzVXyk477RSy5ZZbLle31D50PokAAAAAAAClcAgBAAAAAACUwiEEAAAAAABQCocQAAAAAABAKdpEY+pqm8Kkmo1ApT744IOQffPNN3Vdw7zzzhuyvfbaK2QdO3Zscq5HHnkkZC+//HJ1C+MHjR49OmSzZs3K1dOnTw9jUs2QTjvttCbf78Ybb6xoXcX3rLQZUqrBJ6Qce+yxIdt2222rmuupp56a2+XA/7nyyitDttFGG1X02qlTp+bq1LU6dU2nforNWY844oiq5/rqq6+anKvejdVfffXVkFW6f2mZvv/++5AVP0v+u4y244ILLgjZ3nvvXdFri42izzvvvDDm7rvvDtmLL76Yq1Of/c4666yQvfTSSyEbNmxYU8ukhRgwYEDIVl999ZDNnj27ybnefPPNkG288cbVLYxWrV27diFr3z7+t/ovvPBCrr7++utLW1OWZdlNN90Usr/85S+5OrX2o48+OmSLLbZYrk41va4nv4QAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUrSJxtTVGjt2bKOXQAuRagrzz3/+M2Rffvllzd6zc+fOufq4444LY3bZZZeQpZo+VWLmzJkhKzYmPuecc6qam7zU3ik2RR0zZkwY8+STT5a2pizLskmTJuXqShqDZVmWPf/882UshxauX79+Idtzzz2bfF2q2ebWW28dslpeb2l7Lrzwwly93nrrVT1XsXndgw8+WPVclGOVVVbJ1VtssUXVc02cODFXX3PNNVXPVStXXHFFyI488sgmX9e7d++Q/eMf/6jJmqit7t275+pis3XIsth0eq+99qrode+//37Ihg4dmquLDacrtcgii1Q07sMPP6xqfpqfnXfeOWRnnnlmVXPdeOONITv++OOrmovW7eSTTw5Z6u8Zs2bNCtnKK6+cq1PfY4t/K0nZYYcdQrbSSiuFbPfddw9Zjx49cnVq7anPeyeccEKT66onv4QAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUrS6xtSDBw+u+rXNoRH1yJEjc/Xw4cMrel3xf3dz+N/Smk2dOjVXV9qgt1qdOnUKWbFR8W677Vb1/J9++mmunjZtWhizxBJLhGzUqFG5+pVXXglj7r///qrX1Vb97Gc/C1kljY7KtswyyzQ55pFHHgnZ5ptvXsZyaOGWXXbZkA0YMKDJ13333Xchu+eee2qyJlq/YvPWLMuyO+64I2TFz1WpJnWVateuXdWvpT5OPPHERi+hWUr9/zI3Tbspz69+9atcvd566zVoJTQXCyywQMiKf1vo0KFDGHP11VeHLNU0+O23365qXSussEKu3m+//cKY1Pees88+u6r3o/H69u2bq3/729+GMYsttljIKvn8tOaaa4bsyCOPbPJ1qe8OL730Usg++eSTJueiZfjDH/4Qsk022SRkxf2aZXEfpPbm2muvHbItt9wyVw8bNiyMSf0tMfX3uOLfeIt/i8uyLPvwww9DVvxbX6P5JQQAAAAAAFAKhxAAAAAAAEApHEIAAAAAAAClaHU9ISrtoZCy/vrr13Al9aUHRH29/PLLuXrjjTcOY37yk5+ErEePHiErPqOtd+/eYcyxxx4bskqes1/sXZFlWXbwwQeH7MEHH8zVHTt2DGOK/5uzLMt69uyZq1dcccUwRk+IOdcc+j+krL766rk6tc7UMzhnzpxZ2ppouSp5XmvKBRdcUNuF0GqknutafPZ06h7Yr1+/kBV7QMxN76ey+0Yx94rP9p2bPh7t27eM/8Yr9b+xuPYjjjiiTqsBai3V72GeefJ//nn//ffDmNNOOy1k77zzTs3WVXxGeteuXcOYO++8M2TPPvtszdZAY6U+F1X6Wak4rn///mHM4YcfHrLiPS81ZsyYMSE7+eSTQ/buu+/m6okTJ6YXS7OSuoak/qb2/PPPh+zrr7/O1aleJOeff35V65o+fXrIUn02t91226rmb25axqdkAAAAAACgxXEIAQAAAAAAlMIhBAAAAAAAUAqHEAAAAAAAQClafGPqESNG5OrBgwdX9LqRI0fWfjE1MDeNtamfVAOkok6dOoUs1ayw2DTsiSeeCGOWWGKJql530EEHhWz8+PFxsQV77bVXyIpNqGm5OnfunKt/+tOfhjGphnDdunXL1ak998orr8zd4mi1dtttt1y97rrrVvS6KVOm5OpLL720Zmui5TrwwANDdskll4Ss2qbQb7/9dq4uNqrOsizr27dvyFINNmn+ivtkbpqJp/ZKo6WaHu6zzz4hKzbgTL2OxktdZ5ZffvlcnfrO0VKaplMbn332WchS961622abbZocc/zxx9dhJdRLsXHz0UcfHcbsuOOOIVt11VVDVsnfYaq1/vrrh+yxxx4L2XvvvZern3vuuTBm//33D9knn3wyF6ujDKm/jd11110hGzp0aK6utgl1au+/8cYbIbv//vurmr8l8EkEAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAAStHiG1O3RWPHjm30Etq8Cy+8MFdvueWWYUy7du1Cdthhh4VsmWWWydVLLbVURWt48MEHc/Vmm20WxlTaHHHQoEG5+vTTT6/odZ9++mmufuSRRyp6HfWz4oorhqzYSCnVhCul2Kgz1Rix2Lw6y7Js2rRpFc1P63bGGWfk6vnnn7+i111zzTW5utgMjrZp2WWXrep1kydPDlnqnpdqcl107733hmyjjTaqal1QS8X781FHHRXGFBsZZ1mWff7557n622+/re3CqIk+ffqEbN99983VlX4HuOWWW3L1lClTql8YFCy88MIhKzYWfuWVV8KYYiNjWpc77rijoqx79+4hK35/2GmnncKYe+65J2T77LNPri5+r82yLBswYEDIUn9j6devX65efPHFw5iHH344ZJdddlnIaH7++7//O2Spv/dV44UXXgjZuHHjajJ3S+GXEAAAAAAAQCkcQgAAAAAAAKVwCAEAAAAAAJSixfeEGD58eKOXULXBgwdX9bq29syw5qj4z+DVV18NY/7jP/4jZCeddFJV71d8XmuWZdl+++2Xqyt99mvqWYc33nhjrl5kkUXCmE8++SRke++9d65+7rnnKloD9bPXXnuFrNIeEE1J9Zs49dRTQ3bkkUfW5P1oOTp06BCy9u2b/u8eUs9ndV0h5bHHHgtZqhfTa6+9lqsvv/zymq0h9X6VZlCmXr165epUr5JUT4ghQ4bkav0BWr/is6+LfUGgUsXrTpZl2d133x2yeebJ/wlqbvoa0rpNnTq1yeycc86paK5jjjmmyTHrrrtuyFJ/syv2QHz55ZfDmDvvvLOiddH8rLzyyiFLfUetxhZbbBGytvb3Xb+EAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFK0+MbU1RoxYkRd3y/V0GbMmDFVzVXvtVNfX375Zcj233//kH399de5eskllwxjVl111ZBdfPHFISs2ok41A7vwwgtDlmo2Rtu2zz77hOzss88O2aRJk+qxHOog1eg81fytT58+Tc41fvz4kN18883VLYxW7dZbb60oq5XevXuHrGfPniFLNa6rVTM7ylNsHj43zcTbt2/8f+M1aNCgXH377beHMZMnTw7Zo48+WtqagNZtlVVWCdmaa64ZsmITX98JaIQBAwaEbO+99w5Z165dQ1b8XHfPPfeEMfZ1y7X77ruHrFaf5XfeeeeQnXXWWSFLfUZrLRr/KRkAAAAAAGiVHEIAAAAAAAClcAgBAAAAAACUwiEEAAAAAABQihbVmLrahsxjx46t6Tqaklrn8OHDq5pr5MiRc7ka6iHVhDXVtLljx45NzjXffPOF7Kqrrmryddtss02TY/6dzz//PFeffvrpYcw555xT9fw0ztFHH91klmrM9V//9V8hW2yxxZp8vy5duoSsQ4cOTb6OlmvIkCEh22STTZp83fvvvx+yvfbaqyZrglq78sorQzZw4MAGrIQynHrqqbl6ww03rHqufv365epf/epXYcz1119f9fxFqQaKV1xxRa4eP358GJP6rEfLkGp4evnll+fqAw44oF7LgSzLsmz55ZevaNxrr72Wq7/99tsylkMblvpuu+qqq+bqww8/PIxZa621Kpp/+vTpuXr06NGVL46GmWee+Ofv/fffv6q5TjrppJB99tlnIfv973+fqxdZZJEwZp999gnZGWecUdW6WgK/hAAAAAAAAErhEAIAAAAAACiFQwgAAAAAAKAUDiEAAAAAAIBStKjG1NUaPHhwqXMVs2qbUGdZbKJdbTNu6uvBBx8MWeqfXarReLFBTqqJ79w0nS56/PHHQ3bEEUfk6ueee65m70fz9+abb4Zsgw02CNnDDz+cq/v06VPammie5p9//pAdfPDBVc118803hyzVPBUaYdCgQbl6vfXWq3qu22+/fW6XQ8k+/PDDXD116tQwpnv37hXN1bFjx1w9bNiwMOaBBx4I2eTJk5ucu3fv3iE79thjQ1a8lm6++eZhzJQpU5p8P5qn1Oevgw46qMnXffLJJyF79913a7Im2p4ePXrk6kr/BnLcccfl6lSj1o8++ihks2fPnoPVMSdOOeWUkO29995VzXXBBRfM5Wp+WPH6t+OOO4Yxqe8rxXv43OynnXbaKVc/+eSTVc9F/aTunRdeeGFVc40ZMyZkEydOrGqulVdeuarXtVR+CQEAAAAAAJTCIQQAAAAAAFAKhxAAAAAAAEApWlRPiNQz9qvtv5B6hte4ceNqMneliv0fsizL1l9//VLfk/o544wzQrbiiiuGbOedd65q/qeffjpXP/HEE2FM6hl3qWdsfv3111WtgdYr1SfihRdeyNV6QrQ9hx9+eMgqfU76xx9/nKsvv/zyWiyJZmzLLbfM1f369QtjLr744pq932GHHRaySp75m+q7VOz3NWvWrDAm1Tdg6623Dtmjjz7a5BporAkTJuTqG264IYw55JBDqpp7+eWXD9mJJ54YsmJ/rpTU61ZYYYWQ3Xfffbla/4fWL3WNKnrqqadCNnr06DKWQyvTqVOnkBX/ntKzZ88w5uSTTw7ZO++8k6sHDhwYxhQ/M2aZnhBlWnPNNUO22GKLVTXXueeeG7Jq/9m1a9eutLlSPXJSvelGjRoVsuLfDWkZUnsztcfat4//rX7x34cPPvggjEl9TizOn5r7mGOOiYttxfwSAgAAAAAAKIVDCAAAAAAAoBQOIQAAAAAAgFI4hAAAAAAAAErRohpTpxSbOxcbCf47qXGVvrYaI0eODFmq0Tat2x577BGyYgOkVFPLpZdeOmR///vfc/WRRx45l6uDH7bZZpvl6lRjsFRD62nTppW2JspVbD6+7777Vj3Xww8/nKvffvvtquei+XnooYdCttZaa+XqLl26hDHVNvtN6d+/f8iqbWBYbPKaapJ59913h0wT6tYh1agwlVVr1VVXDdmhhx6aq/fbb78wJtWEuvh5MMuybK+99qp+cTR73377bcgmT56cq3v37h3GzD///CHr3r17rp46depcrY3WqXPnzhVlRY899ljIivflZ599tvqFURM77rhjyH7/+9+HrPh3itQ1JaWWTcWLc6UaTKe+j/73f/93rr700kvDmPfff38uV0dzltqHqaz4HSDLsuwPf/hDrr722mvDmE022aTJ+VNztzV+CQEAAAAAAJTCIQQAAAAAAFAKhxAAAAAAAEApHEIAAAAAAAClaPGNqYsNn8eNGxfGDB8+vGbvV2yEnXpPDaf5d7777ruQnX322T9YQ9lSzWL/8pe/hKzYlPO6664LY0444YSQff7553OxOhqp2HTwRz/6UdVzffnll3O7HJqxfv36hSx1bSlaeumly1jO/5kwYUKuTjV0TWnfPv/f6Rx22GFhzIMPPlj1umjeXnvttZC98sorIUs1iq7EeuutF7J11123ydelGihusMEGIZsyZUpV66JleOedd0J2xhln5OrzzjsvjPn5z38esl133TVXp5rR0rZ07do1ZFdddVXIBgwYkKtTTX1ff/312i2M0kyfPj1ke+21V8jOPffcXF1pY+oype53b7/9dgNWQmu20UYb/WBdqZtuuilkkydPrmqulsovIQAAAAAAgFI4hAAAAAAAAErhEAIAAAAAAChFi+8JUezRkOrZoEcD0BYMHDgwZB07dgxZnz59cvUxxxwTxqy11loh+/vf/56rTz755DBm0qRJTa6TlqP4PP0dd9wxjLnrrrsqmuvUU0+txZJopkaNGhWyYcOG5erll18+jEn18rrjjjtqtq7rr78+V+tRQyUuv/zykN1///0hu+GGG0K28sor5+rU89UrMXPmzJAVn8edZfo/8P/cfffdubrY6yHLYm+vLMuy0aNHl7YmWqZzzjknZFtttVXIjj322FydukZ+9NFHNVsXjffyyy83eglQtU8++SRk06ZNC9kCCyxQs/ecOnVqrk71f0193mvN/BICAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAAStFu9uzZs5sa9MUXX2QLLrhgPdZDC/H555/XtGFLin1HUdn7riXtuQEDBoTsiSeeCFn37t1DVsFlP7v33ntDdsABB+TqttKE2r6j3txjaQTXutoYOnRorj7iiCPCmFQz6VmzZuXqt956K4w5+eST525xzYxrHY3gWvfDUtenLl26hGy55ZbL1e+++25pa2oN7DvqzT22aYMGDQrZaqutFrKVV145V+++++5hTLEJdZZl2YYbbpirn3/++TlcYcvT1L7zSwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABK4RACAAAAAAAoxTyNXgAAc+7NN98M2QUXXBCyESNGhKzYdPq0004LY1544YWQff/995UvEADaoLvvvvsHa4DmYv311w9ZqsnszTffHDKNqIGWbty4cRVlRb/+9a/LWE6b4JcQAAAAAABAKRxCAAAAAAAApXAIAQAAAAAAlMIhBAAAAAAAUAqNqQFaiVSD6VQGAAC0bYMGDQpZ+/bxv1O99dZb67EcAFo5v4QAAAAAAABK4RACAAAAAAAohUMIAAAAAACgFHpCAAAAALQhI0aMqCgDgFrwSwgAAAAAAKAUDiEAAAAAAIBSOIQAAAAAAABKUdEhxOzZs8teBy1MPfaEfUdR2XvCniPFvqPe3GNpBNc66s21jkZwraMR7DvqzT2WRmhqT1R0CDFt2rSaLIbWox57wr6jqOw9Yc+RYt9Rb+6xNIJrHfXmWkcjuNbRCPYd9eYeSyM0tSfaza7g6GrWrFnZxIkTs27dumXt2rWr2eJoeWbPnp1NmzYt69u3b9a+fblP87Lv+F/12nf2HP/KvqPe3GNpBNc66s21jkZwraMR7DvqzT2WRqh031V0CAEAAAAAADCnNKYGAAAAAABK4RACAAAAAAAohUMIAAAAAACgFA4hAAAAAACAUjiEAAAAAAAASuEQAgAAAAAAKIVDCAAAAAAAoBT/H+ZXdolUrABcAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":["# from typing import Callable, Optional\n","\n","# def train_autoencoder(\n","#     model: nn.Module,\n","#     data_loader: DataLoader,\n","#     loss_fn: Callable,\n","#     optimizer: optim.Optimizer,\n","#     epochs: int = 10,\n","#     device: str = \"cpu\",\n","#     noise_factor: float = 0.0,\n","#     scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n","#     contrastive_loss_fn: Optional[Callable] = None,\n","#     temperature: float = 0.5,\n","#     triplet_data: bool = False,\n","#     augment_fn: Optional[Callable] = None,\n","#     patience: int = 5,\n","#     min_delta: float = 0.001,\n","# ):\n","#     \"\"\"\n","#     Unified training function for autoencoders with support for:\n","#     - Reconstruction loss\n","#     - Contrastive loss (e.g., NT-Xent, InfoNCE)\n","#     - Triplet loss\n","#     - Noise injection (for denoising autoencoders)\n","#     - Data augmentation\n","#     - Early stopping\n","\n","#     Args:\n","#         model (nn.Module): The autoencoder model.\n","#         data_loader (DataLoader): DataLoader for training data.\n","#         loss_fn (Callable): Primary loss function (e.g., reconstruction loss).\n","#         optimizer (optim.Optimizer): Optimizer for the model.\n","#         epochs (int): Number of epochs to train.\n","#         device (str): Device to train on ('cpu' or 'cuda').\n","#         noise_factor (float): Factor for adding noise to input images (denoising autoencoder).\n","#         scheduler (Optional[optim.lr_scheduler._LRScheduler]): Learning rate scheduler.\n","#         contrastive_loss_fn (Optional[Callable]): Contrastive loss function (e.g., NT-Xent, triplet loss).\n","#         temperature (float): Temperature parameter for NT-Xent loss.\n","#         triplet_data (bool): Whether the data_loader provides triplets (anchor, positive, negative).\n","#         augment_fn (Optional[Callable]): Augmentation function for contrastive learning.\n","#         patience (int): Number of epochs with no significant improvement before triggering early stopping.\n","#         min_delta (float): Minimum change in loss to qualify as an improvement.\n","\n","#     Returns:\n","#         None: Prints loss values for each epoch.\n","#     \"\"\"\n","#     model.to(device).train()\n","\n","#     # Initialize early stopping\n","#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n","\n","#     for epoch in range(epochs):\n","#         total_loss = 0\n","#         for batch in data_loader:\n","#             # Prepare data based on whether it's triplet data or not\n","#             if triplet_data:\n","#                 anchor, positive, negative = batch\n","#                 anchor, positive, negative = (\n","#                     anchor.to(device).float(),\n","#                     positive.to(device).float(),\n","#                     negative.to(device).float(),\n","#                 )\n","#                 images = anchor  # Use anchor as the primary input for reconstruction\n","#             else:\n","#                 images, _ = batch\n","#                 images = images.to(device).float()\n","\n","#             # Add noise if specified\n","#             if noise_factor > 0:\n","#                 noisy_images = images + noise_factor * torch.randn_like(images)\n","#                 noisy_images = torch.clamp(noisy_images, 0.0, 1.0)\n","#                 encoded, decoded = model(noisy_images)\n","#             else:\n","#                 encoded, decoded = model(images)\n","\n","#             # Compute reconstruction loss\n","#             reconstruction_loss = loss_fn(decoded, images)\n","\n","#             # Compute contrastive loss if specified\n","#             contrastive_loss_value = 0\n","#             if contrastive_loss_fn is not None:\n","#                 if triplet_data:\n","#                     # Triplet loss\n","#                     positive_encoded, _ = model(positive)\n","#                     negative_encoded, _ = model(negative)\n","#                     contrastive_loss_value = contrastive_loss_fn(encoded, positive_encoded, negative_encoded)\n","#                 else:\n","#                     # NT-Xent or other contrastive loss\n","#                     if augment_fn:\n","#                         augmented_1 = augment_fn(images)\n","#                         augmented_2 = augment_fn(images)\n","#                         z1, _ = model(augmented_1)\n","#                         z2, _ = model(augmented_2)\n","#                     else:\n","#                         z1, z2 = encoded, encoded  # Use the same embeddings if no augmentation\n","#                     contrastive_loss_value = contrastive_loss_fn(z1, z2, temperature)\n","\n","#             # Total loss\n","#             total_loss_value = reconstruction_loss + contrastive_loss_value\n","\n","#             # Backpropagation\n","#             optimizer.zero_grad()\n","#             total_loss_value.backward()\n","#             optimizer.step()\n","\n","#             total_loss += total_loss_value.item()\n","\n","#         # Step the scheduler if provided\n","#         if scheduler:\n","#             scheduler.step()\n","\n","#         # Compute average epoch loss\n","#         avg_loss = total_loss / len(data_loader)\n","#         print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_loss:.4f}\")\n","\n","#         # Check for early stopping\n","#         early_stopping(avg_loss)\n","#         if early_stopping.early_stop:\n","#             print(f\"Early stopping triggered at epoch {epoch + 1}.\")\n","#             break\n","\n","# class EarlyStopping:\n","#     \"\"\"\n","#     Early stopping to stop training when the loss does not improve after a specified number of epochs (patience).\n","#     \"\"\"\n","#     def __init__(self, patience=5, min_delta=0):\n","#         \"\"\"\n","#         Args:\n","#             patience (int): Number of epochs to wait for improvement before stopping.\n","#             min_delta (float): Minimum change in loss to qualify as an improvement.\n","#         \"\"\"\n","#         self.patience = patience\n","#         self.min_delta = min_delta\n","#         self.counter = 0\n","#         self.best_loss = float('inf')\n","#         self.early_stop = False\n","\n","#     def __call__(self, current_loss):\n","#         \"\"\"\n","#         Check if training should stop.\n","\n","#         Args:\n","#             current_loss (float): Current epoch's loss.\n","#         \"\"\"\n","#         if current_loss < self.best_loss - self.min_delta:\n","#             self.best_loss = current_loss\n","#             self.counter = 0\n","#         else:\n","#             self.counter += 1\n","#             if self.counter >= self.patience:\n","#                 self.early_stop = True\n","\n","# def train_vae(\n","#     vae: nn.Module,\n","#     train_loader: DataLoader,\n","#     optimizer: torch.optim.Optimizer,\n","#     loss_fn: Callable,\n","#     epochs: int = 10,\n","#     device: str = \"cpu\",\n","#     val_loader: Optional[DataLoader] = None,\n","#     scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n","#     save_best: bool = False,\n","#     save_path: str = \"best_vae_model.pth\",\n","#     beta: float = 1.0,\n","#     alpha: Optional[float] = None,\n","#     temperature: float = 0.5,\n","#     contrastive_loss_fn: Optional[Callable] = None,\n","#     patience: int = 5,\n","#     min_delta: float = 0.001,\n","# ):\n","#     \"\"\"\n","#     Unified training function for VAEs with support for:\n","#     - Reconstruction loss (e.g., MSE, SSIM).\n","#     - KL divergence.\n","#     - Contrastive learning (e.g., NT-Xent).\n","#     - Optional validation and model saving.\n","#     - Early stopping\n","\n","#     Args:\n","#         vae (nn.Module): VAE model.\n","#         train_loader (DataLoader): DataLoader for training data.\n","#         optimizer (torch.optim.Optimizer): Optimizer for training.\n","#         loss_fn (Callable): Loss function for reconstruction and KL divergence.\n","#         epochs (int): Number of epochs to train.\n","#         device (str): Device to train on ('cpu' or 'cuda').\n","#         val_loader (Optional[DataLoader]): DataLoader for validation data.\n","#         scheduler (Optional[torch.optim.lr_scheduler._LRScheduler]): Learning rate scheduler.\n","#         save_best (bool): Whether to save the best model based on validation loss.\n","#         save_path (str): Path to save the best model.\n","#         beta (float): Weight for the KL divergence term.\n","#         alpha (Optional[float]): Weight for the contrastive loss term.\n","#         temperature (float): Temperature parameter for contrastive loss.\n","#         contrastive_loss_fn (Optional[Callable]): Contrastive loss function (e.g., NT-Xent).\n","#         patience (int): Number of epochs with no significant improvement before triggering early stopping.\n","#         min_delta (float): Minimum change in loss to qualify as an improvement.\n","\n","#     Returns:\n","#         None: Prints loss values for each epoch.\n","#     \"\"\"\n","#     vae.to(device).train()\n","#     best_val_loss = float('inf') if save_best else None\n","\n","#     # Initialize early stopping\n","#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n","\n","#     for epoch in range(epochs):\n","#         # Training loop\n","#         vae.train()\n","#         total_train_loss = 0\n","#         for images, _ in train_loader:\n","#             images = images.to(device).float()\n","\n","#             # Forward pass\n","#             mu, logvar, decoded = vae(images)\n","\n","#             # Compute reconstruction and KL divergence loss\n","#             recon_loss = loss_fn(decoded, images, mu, logvar, beta)\n","\n","#             # Compute contrastive loss if specified\n","#             contrastive_loss_value = 0\n","#             if contrastive_loss_fn is not None and alpha is not None:\n","#                 if hasattr(vae, 'projection_head'):\n","#                     indices = torch.randperm(mu.size(0)).to(device)\n","#                     z1, z2 = mu, mu[indices]\n","#                     contrastive_loss_value = contrastive_loss_fn(z1, z2, temperature)\n","#                 else:\n","#                     raise ValueError(\"VAE model must have a projection head for contrastive loss.\")\n","\n","#             # Total loss\n","#             total_loss = recon_loss + (alpha * contrastive_loss_value if alpha is not None else 0)\n","\n","#             # Backpropagation\n","#             optimizer.zero_grad()\n","#             total_loss.backward()\n","#             optimizer.step()\n","\n","#             total_train_loss += total_loss.item()\n","\n","#         # Validation loop\n","#         if val_loader:\n","#             vae.eval()\n","#             total_val_loss = 0\n","#             with torch.no_grad():\n","#                 for images, _ in val_loader:\n","#                     images = images.to(device).float()\n","#                     mu, logvar, decoded = vae(images)\n","#                     val_loss = loss_fn(decoded, images, mu, logvar, beta)\n","#                     total_val_loss += val_loss.item()\n","\n","#             avg_val_loss = total_val_loss / len(val_loader)\n","#             print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {total_train_loss / len(train_loader):.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","#             # Save the best model\n","#             if save_best and avg_val_loss < best_val_loss:\n","#                 best_val_loss = avg_val_loss\n","#                 torch.save(vae.state_dict(), save_path)\n","#                 print(f\"Model saved at epoch {epoch + 1}\")\n","\n","#             # Check for early stopping\n","#             early_stopping(avg_val_loss)\n","#             if early_stopping.early_stop:\n","#                 print(f\"Early stopping triggered at epoch {epoch + 1}.\")\n","#                 break\n","#         else:\n","#             avg_train_loss = total_train_loss / len(train_loader)\n","#             print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.4f}\")\n","\n","#             # Check for early stopping (using training loss if no validation data)\n","#             early_stopping(avg_train_loss)\n","#             if early_stopping.early_stop:\n","#                 print(f\"Early stopping triggered at epoch {epoch + 1}.\")\n","#                 break\n","\n","#         # Step the scheduler if provided\n","#         if scheduler:\n","#             scheduler.step()"],"metadata":{"id":"qx6NgMUJPLv5","executionInfo":{"status":"ok","timestamp":1737810743044,"user_tz":-210,"elapsed":304,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoder\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"vae\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"dae\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels"],"metadata":{"id":"lC_8uAUF7t0r","executionInfo":{"status":"ok","timestamp":1737808763606,"user_tz":-210,"elapsed":296,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# class EnhancedAutoencoder(nn.Module):\n","#     \"\"\"\n","#     A deep autoencoder with advanced reconstruction capabilities for MNIST.\n","\n","#     Features:\n","#     - Deeper architecture with additional convolutional and transposed convolutional layers.\n","#     - Utilizes Batch Normalization and LeakyReLU activations.\n","#     - Capable of learning highly expressive embeddings.\n","\n","#     Designed for datasets requiring intricate reconstructions.\n","#     \"\"\"\n","#     def __init__(self, code_dim):\n","#         super(EnhancedAutoencoder, self).__init__()\n","\n","#         # Encoder\n","#         self.encoder = nn.Sequential(\n","#             nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: (1, 28, 28)\n","#             nn.LeakyReLU(0.2),\n","#             nn.BatchNorm2d(32),\n","#             nn.MaxPool2d(2, 2),  # Output: (32, 14, 14)\n","\n","#             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 14, 14)\n","#             nn.LeakyReLU(0.2),\n","#             nn.BatchNorm2d(64),\n","#             nn.MaxPool2d(2, 2),  # Output: (64, 7, 7)\n","\n","#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 7, 7)\n","#             nn.LeakyReLU(0.2),\n","#             nn.BatchNorm2d(128),\n","#             nn.MaxPool2d(2, 2)  # Output: (128, 3, 3)\n","#         )\n","#         self.fc_encoder = nn.Linear(128 * 3 * 3, code_dim)  # Flatten to embedding dimension\n","\n","#         # Decoder\n","#         self.fc_decoder = nn.Linear(code_dim, 128 * 3 * 3)  # Unflatten to feature map\n","#         self.decoder = nn.Sequential(\n","#             nn.ReLU(),\n","#             nn.Unflatten(1, (128, 3, 3)),  # Reshape to (batch_size, 128, 3, 3)\n","\n","#             nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (64, 7, 7)\n","#             nn.LeakyReLU(0.2),\n","#             nn.BatchNorm2d(64),\n","\n","#             nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=0, output_padding=1),  # Output: (32, 14, 14)\n","#             nn.LeakyReLU(0.2),\n","#             nn.BatchNorm2d(32),\n","\n","#             nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (16, 28, 28)\n","#             nn.LeakyReLU(0.2),\n","#             nn.BatchNorm2d(16),\n","\n","#             nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=1),  # Output: (1, 28, 28)\n","#             nn.Tanh()\n","#         )\n","\n","#     def forward(self, x):\n","#         batch_size = x.size(0)\n","\n","#         # Encoder\n","#         encoded = self.encoder(x)\n","#         encoded = encoded.view(batch_size, -1)  # Flatten to (batch_size, 128 * 3 * 3)\n","#         encoded = self.fc_encoder(encoded)  # Output: (batch_size, code_dim)\n","\n","#         # Decoder\n","#         decoded = self.fc_decoder(encoded)\n","#         decoded = self.decoder(decoded)  # Output: (batch_size, 1, 28, 28)\n","\n","#         return encoded, decoded"],"metadata":{"id":"a48gCSfl99zM","executionInfo":{"status":"ok","timestamp":1737805945375,"user_tz":-210,"elapsed":282,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# model = EnhancedAutoencoder(code_dim=50).to(device)\n","# dummy_input = torch.randn(1, 1, 28, 28).to(device)\n","# encoded, decoded = model(dummy_input)\n","# print(f\"Decoded shape: {decoded.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2NQsq0YAZFN","executionInfo":{"status":"ok","timestamp":1737805947294,"user_tz":-210,"elapsed":316,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"98b618e8-ee15-436a-a170-cd9e403fdc43"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded shape: torch.Size([1, 1, 28, 28])\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# class NTXentLoss(nn.Module):\n","#     \"\"\"\n","#     NT-Xent (Normalized Temperature-scaled Cross Entropy) Loss for contrastive learning.\n","\n","#     Args:\n","#         temperature (float): Scaling factor for similarity scores.\n","#     \"\"\"\n","#     def __init__(self, temperature=0.5):\n","#         super(NTXentLoss, self).__init__()\n","#         self.temperature = temperature\n","\n","#     def forward(self, z_i, z_j):\n","#         \"\"\"\n","#         Compute the NT-Xent loss between two sets of embeddings.\n","\n","#         Args:\n","#             z_i (torch.Tensor): First set of embeddings.\n","#             z_j (torch.Tensor): Second set of embeddings.\n","\n","#         Returns:\n","#             torch.Tensor: Computed NT-Xent loss.\n","#         \"\"\"\n","#         batch_size = z_i.size(0)\n","\n","#         # Normalize embeddings\n","#         z_i = F.normalize(z_i, dim=1)\n","#         z_j = F.normalize(z_j, dim=1)\n","\n","#         print(f\"Shape of z_i: {z_i.shape}\")\n","#         print(f\"Shape of z_j: {z_j.shape}\")\n","\n","#         # Concatenate embeddings\n","#         z = torch.cat([z_i, z_j], dim=0)\n","#         print(f\"Shape of z: {z.shape}\")\n","#         print(f\"Shape of zT: {z.T.shape}\")\n","#         # Compute similarity matrix\n","#         similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","\n","#         # Mask for positives and negatives\n","#         mask = torch.eye(2 * batch_size, device=z.device).bool()  # Mask for self-similarities\n","#         positives = torch.cat([\n","#             torch.diag(similarity_matrix, batch_size),  # Similarity between z_i and z_j\n","#             torch.diag(similarity_matrix, -batch_size)  # Similarity between z_j and z_i\n","#         ])\n","\n","#         # Mask out self-similarities and positives\n","#         negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n","\n","#         # Compute NT-Xent loss\n","#         numerator = torch.exp(positives)\n","#         denominator = torch.sum(torch.exp(negatives), dim=-1)\n","\n","#         # Avoid numerical instability by using log-sum-exp trick\n","#         loss = -torch.log(numerator / denominator).mean()\n","\n","#         return loss\n","\n","class NTXentLoss(nn.Module):\n","    def __init__(self, temperature=0.5):\n","        super(NTXentLoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, z_i, z_j):\n","        batch_size = z_i.size(0)\n","\n","        # Normalize embeddings\n","        z_i = F.normalize(z_i, dim=0)\n","        z_j = F.normalize(z_j, dim=0)\n","\n","        # Concatenate embeddings\n","        z = torch.cat([z_i, z_j], dim=0)\n","        print(f\"Shape of z: {z_i.shape}\")\n","        print(f\"Shape of zT: {z_j.shape}\")\n","        print(f\"Shape of z: {z.shape}\")\n","        print(f\"Shape of zT: {z.T.shape}\")\n","        # Compute similarity matrix\n","        similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","\n","        # Mask for positives and negatives\n","        mask = torch.eye(2 * batch_size, device=z.device).bool()  # Mask for self-similarities\n","        positives = torch.cat([\n","            torch.diag(similarity_matrix, batch_size),\n","            torch.diag(similarity_matrix, -batch_size)\n","        ])\n","\n","        # Mask out self-similarities and positives\n","        negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n","\n","        # Compute NT-Xent loss\n","        numerator = torch.exp(positives)\n","        denominator = torch.sum(torch.exp(negatives), dim=-1)\n","        loss = -torch.log(numerator / denominator).mean()\n","\n","        return loss\n"],"metadata":{"id":"Y8LPTX8iKf1j","executionInfo":{"status":"ok","timestamp":1737808538728,"user_tz":-210,"elapsed":283,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Define Configuration\n","# ------------------------------\n","\n","# Configuration\n","config = {\n","    \"model_type\": \"vae\",  # Options: \"autoencoder\", \"vae\", \"dae\"\n","    \"model_name\": \"ImprovedVAE\",  # Options: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\", \"BasicVAE\", \"ImprovedVAE\", \"FlexibleVAE\", \"ImprovedFlexibleVAE\", \"DenoisingAutoencoder\"\n","    \"code_dim\": 50,  # Dimensionality of the embedding\n","    \"loss_type\": \"mse\",  # Options: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","    \"noise_factor\": 0.1,  # Noise factor for denoising autoencoders\n","    \"temperature\": 0.5,  # Temperature parameter for NT-Xent loss\n","    \"margin\": 1.0,  # Margin for Triplet Loss\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-3,\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    \"save_best\": True,  # Whether to save the best model\n","    \"save_path\": \"best_model.pth\",  # Path to save the best model\n","    \"beta\": 1.0,  # Weight for KL divergence (VAE only)\n","    \"alpha\": 0.5,  # Weight for contrastive or triplet loss\n","    \"fraction\": 0.01,  # Fraction of the dataset to use\n","    \"projection_dim\": None,  # Optional projection head dimension for VAEs\n","    \"strong_architecture\": False,  # Whether to use a deeper architecture for DenoisingAutoencoder\n","    \"input_shape\": (1, 28, 28),  # Input shape for FlexibleVAE and ImprovedFlexibleVAE\n","    \"patience\": 5,\n","    \"min_delta\": 0.001,\n","}\n","\n","# ------------------------------\n","# Step 2: Load and Preprocess Data\n","# ------------------------------\n","\n","# Load Combined MNIST Dataset (Train + Test)\n","mnist_loader = data_utils.load_mnist_data(fraction=config[\"fraction\"], batch_size=config[\"batch_size\"], shuffle=True)\n","\n","# Inspect Combined Dataset\n","for batch in mnist_loader:\n","    images, labels = batch\n","    print(\"Batch Shape:\", images.shape, labels.shape)\n","    break\n","\n","# Visualize Original Images\n","n = 20\n","sample_indices = np.random.choice(len(mnist_loader.dataset), n, replace=False)\n","sampled_images = mnist_loader.dataset.tensors[0][sample_indices].numpy()\n","sampled_images = (sampled_images * 127.5 + 127.5).astype(np.uint8).squeeze()  # Denormalize for display\n","\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    ax = plt.subplot(2, 10, i + 1)\n","    plt.imshow(sampled_images[i], cmap=\"gray\")\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()\n","\n","# ------------------------------\n","# Step 3: Initialize Model, Loss, and Optimizer\n","# ------------------------------\n","\n","# Initialize the model\n","model_classes = {\n","    \"BasicAutoencoder\": encoder_models.BasicAutoencoder,\n","    \"IntermediateAutoencoder\": encoder_models.IntermediateAutoencoder,\n","    \"AdvancedAutoencoder\": encoder_models.AdvancedAutoencoder,\n","    \"EnhancedAutoencoder\": EnhancedAutoencoder,\n","    \"BasicVAE\": encoder_models.BasicVAE,\n","    \"ImprovedVAE\": encoder_models.ImprovedVAE,\n","    \"FlexibleVAE\": encoder_models.FlexibleVAE,\n","    \"ImprovedFlexibleVAE\": encoder_models.ImprovedFlexibleVAE,\n","    \"DenoisingAutoencoder\": encoder_models.DenoisingAutoencoder,\n","}\n","\n","# Initialize model with appropriate arguments\n","if config[\"model_name\"] in [\"FlexibleVAE\", \"ImprovedFlexibleVAE\"]:\n","    model = model_classes[config[\"model_name\"]](\n","        input_shape=config[\"input_shape\"],\n","        code_dim=config[\"code_dim\"],\n","        projection_dim=config[\"projection_dim\"]\n","    ).to(config[\"device\"])\n","elif config[\"model_name\"] == \"DenoisingAutoencoder\":\n","    model = model_classes[config[\"model_name\"]](\n","        code_dim=config[\"code_dim\"],\n","        projection_dim=config[\"projection_dim\"],\n","        strong_architecture=config[\"strong_architecture\"]\n","    ).to(config[\"device\"])\n","else:\n","    model = model_classes[config[\"model_name\"]](\n","        code_dim=config[\"code_dim\"]\n","    ).to(config[\"device\"])\n","\n","# # Define the loss function\n","# loss_functions = {\n","#     \"mse\": nn.MSELoss(),  # Reconstruction loss\n","#     \"vae_loss\": losses.vae_loss,  # VAE loss\n","#     \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","#     \"ntxent\": cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","#     \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","# }\n","# criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the loss function\n","if config[\"model_type\"] == \"vae\":\n","    criterion = losses.vae_loss  # Use VAE loss for VAEs\n","else:\n","    loss_functions = {\n","        \"mse\": nn.MSELoss(),  # Reconstruction loss\n","        \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","        \"ntxent\": NTXentLoss(temperature=config[\"temperature\"]),\n","        # cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","        \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","    }\n","    criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","\n","# Define scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 4: Train the Model\n","# ------------------------------\n","\n","if config[\"model_type\"] == \"autoencoder\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    train_autoencoder(\n","        model=model,\n","        data_loader=mnist_loader,\n","        loss_fn=criterion,\n","        optimizer=optimizer,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        noise_factor=config[\"noise_factor\"],\n","        scheduler=scheduler,\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\", \"triplet\"] else None,\n","        triplet_data=(config[\"loss_type\"] == \"triplet\"),\n","        augment_fn=cl_loss.augment if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","        patience=config[\"patience\"],\n","        min_delta=config[\"min_delta\"],\n","    )\n","\n","elif config[\"model_type\"] == \"vae\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    train_vae(\n","        vae=model,\n","        train_loader=mnist_loader,\n","        optimizer=optimizer,\n","        loss_fn=criterion,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        val_loader=None,  # No validation loader for simplicity\n","        scheduler=scheduler,\n","        save_best=config[\"save_best\"],\n","        save_path=config[\"save_path\"],\n","        beta=config[\"beta\"],\n","        alpha=config[\"alpha\"],\n","        temperature=config[\"temperature\"],\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","        patience=config[\"patience\"],\n","        min_delta=config[\"min_delta\"],\n","    )\n","\n","elif config[\"model_type\"] == \"dae\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    encoder_training.train_dae(\n","        dae=model,\n","        train_loader=mnist_loader,\n","        optimizer=optimizer,\n","        loss_fn=criterion,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        val_loader=None,  # No validation loader for simplicity\n","        scheduler=scheduler,\n","        save_best=config[\"save_best\"],\n","        save_path=config[\"save_path\"],\n","        noise_factor=config[\"noise_factor\"],\n","        alpha=config[\"alpha\"],\n","        temperature=config[\"temperature\"],\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","        triplet_loss_fn=criterion if config[\"loss_type\"] == \"triplet\" else None,\n","        ssim_func=losses.ssim if config[\"loss_type\"] == \"ssim\" else None,\n","    )\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Generate embeddings\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    embedding_type=config[\"model_type\"],\n","    data_loader=mnist_loader,\n","    device=config[\"device\"],\n",")\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{config['model_type']}_{config['model_name']}_{config['loss_type']}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n","\n","# ------------------------------\n","# Step 6: Visualize Embeddings\n","# ------------------------------\n","\n","# Visualize embeddings\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":866},"id":"zAKUO7472Gn7","executionInfo":{"status":"ok","timestamp":1737810835561,"user_tz":-210,"elapsed":44084,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"0f7d35e0-2735-4822-d54f-cf2003267ac8"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (700, 1, 28, 28) (700,)\n","Batch Shape: torch.Size([64, 1, 28, 28]) torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 2000x400 with 20 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training ImprovedVAE with mse loss...\n","Epoch [1/100], Train Loss: 3.8259\n","Epoch [2/100], Train Loss: 0.5691\n","Epoch [3/100], Train Loss: 0.4797\n","Epoch [4/100], Train Loss: 0.4421\n","Epoch [5/100], Train Loss: 0.4231\n","Epoch [6/100], Train Loss: 0.4108\n","Epoch [7/100], Train Loss: 0.4018\n","Epoch [8/100], Train Loss: 0.3956\n","Epoch [9/100], Train Loss: 0.3920\n","Epoch [10/100], Train Loss: 0.3887\n","Epoch [11/100], Train Loss: 0.3867\n","Epoch [12/100], Train Loss: 0.3855\n","Epoch [13/100], Train Loss: 0.3837\n","Epoch [14/100], Train Loss: 0.3815\n","Epoch [15/100], Train Loss: 0.3779\n","Epoch [16/100], Train Loss: 0.3772\n","Epoch [17/100], Train Loss: 0.3761\n","Epoch [18/100], Train Loss: 0.3754\n","Epoch [19/100], Train Loss: 0.3735\n","Epoch [20/100], Train Loss: 0.3696\n","Epoch [21/100], Train Loss: 0.3678\n","Epoch [22/100], Train Loss: 0.3672\n","Epoch [23/100], Train Loss: 0.3669\n","Epoch [24/100], Train Loss: 0.3667\n","Epoch [25/100], Train Loss: 0.3667\n","Epoch [26/100], Train Loss: 0.3667\n","Epoch [27/100], Train Loss: 0.3665\n","Epoch [28/100], Train Loss: 0.3666\n","Epoch [29/100], Train Loss: 0.3667\n","Early stopping triggered at epoch 29.\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/vae_ImprovedVAE_mse/ImprovedVAE_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/vae_ImprovedVAE_mse/ImprovedVAE_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Define Configuration\n","# ------------------------------\n","\n","# Configuration\n","config = {\n","    \"model_name\": \"IntermediateAutoencoder\",  # Options: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"\n","    \"code_dim\": 50,  # Dimensionality of the embedding\n","    \"embedding_type\": \"autoencoders\",  # Options: \"autoencoders\", \"VAEs\", \"denoising_autoencoders\"\n","    \"loss_type\": \"mse\",  # Options: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","    \"noise_factor\": 0.0,  # Noise factor for denoising autoencoders\n","    \"temperature\": 0.5,  # Temperature parameter for NT-Xent loss\n","    \"margin\": 1.0,  # Margin for Triplet Loss\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-3,\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","}\n","\n","# ------------------------------\n","# Step 2: Load and Preprocess Data\n","# ------------------------------\n","\n","# Load MNIST data\n","train_data, test_data = data_utils.load_mnist_data()\n","\n","# Preprocess images\n","train_data = data_utils.preprocess_images(train_data)\n","test_data = data_utils.preprocess_images(test_data)\n","\n","# Create DataLoader\n","train_loader = data_utils.create_dataloader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n","test_loader = data_utils.create_dataloader(test_data, batch_size=config[\"batch_size\"], shuffle=False)\n","\n","# ------------------------------\n","# Step 3: Initialize Model, Loss, and Optimizer\n","# ------------------------------\n","\n","# Initialize the model\n","model_classes = {\n","    \"BasicAutoencoder\": encoder_models.BasicAutoencoder,\n","    \"IntermediateAutoencoder\": encoder_models.IntermediateAutoencoder,\n","    \"AdvancedAutoencoder\": encoder_models.AdvancedAutoencoder,\n","    \"EnhancedAutoencoder\": encoder_models.EnhancedAutoencoder,\n","}\n","model = model_classes[config[\"model_name\"]](code_dim=config[\"code_dim\"]).to(config[\"device\"])\n","\n","# Define the loss function\n","loss_functions = {\n","    \"mse\": nn.MSELoss(),  # Reconstruction loss\n","    \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","    \"ntxent\": cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","    \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","}\n","criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","\n","# Define scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 4: Train the Model\n","# ------------------------------\n","\n","print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","encoder_training.train_autoencoder(\n","    model=model,\n","    data_loader=train_loader,\n","    loss_fn=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    epochs=config[\"epochs\"],\n","    device=config[\"device\"],\n","    noise_factor=config[\"noise_factor\"] if config[\"embedding_type\"] == \"denoising_autoencoders\" else 0.0,\n","    contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\", \"triplet\"] else None,\n","    augment_fn=cl_loss.augment if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","    triplet_data=(config[\"loss_type\"] == \"triplet\"),\n",")\n","\n","# ------------------------------\n","# Step 5: Generate and Save Embeddings\n","# ------------------------------\n","\n","# Generate embeddings\n","embeddings, labels = encoder_training.generate_embeddings(\n","    model=model,\n","    data_loader=test_loader,\n","    device=config[\"device\"],\n",")\n","\n","# Save embeddings\n","data_utils.save_embeddings(\n","    embeddings=embeddings,\n","    labels=labels,\n","    base_dir=\"./saved_embeddings\",\n","    model_name=config[\"model_name\"],\n","    loss_type=config[\"loss_type\"],\n","    embedding_type=config[\"embedding_type\"],\n","    save_format=\"pt\",  # Options: \"pt\" (PyTorch) or \"npy\" (NumPy)\n",")\n","\n","# Save the model\n","model_file = os.path.join(\"./saved_embeddings\", f\"{config['model_name']}_{config['loss_type']}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n","\n","# ------------------------------\n","# Step 6: Visualize Embeddings\n","# ------------------------------\n","\n","# Visualize embeddings\n","data_utils.visualize_embeddings(embeddings, labels, title=f\"{config['model_name']} Embeddings\")"],"metadata":{"id":"3tEV_wCe0DSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EnhancedAutoencoder(nn.Module):\n","    \"\"\"\n","    A deep autoencoder with advanced reconstruction capabilities.\n","\n","    Features:\n","    - Deeper architecture with additional convolutional and transposed convolutional layers.\n","    - Utilizes Batch Normalization and LeakyReLU activations.\n","    - Capable of learning highly expressive embeddings.\n","\n","    Designed for datasets requiring intricate reconstructions.\n","    \"\"\"\n","    def __init__(self, code_dim):\n","        super(EnhancedAutoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: (1, 28, 28)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","            nn.MaxPool2d(2, 2),  # Output: (32, 14, 14)\n","\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 14, 14)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d(2, 2),  # Output: (64, 7, 7)\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 7, 7)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d(2, 2)  # Output: (128, 3, 3)\n","        )\n","\n","        self.fc_encoder = nn.Linear(128 * 3 * 3, code_dim)\n","\n","        # Decoder with learned upsampling\n","        self.fc_decoder = nn.Linear(code_dim, 128 * 3 * 3)\n","        self.decoder = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Unflatten(1, (128, 3, 3)),\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Learned upsampling\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","\n","            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (1, 28, 28)\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Encoding\n","        encoded = self.encoder(x)\n","        encoded = encoded.view(batch_size, -1)\n","        encoded = self.fc_encoder(encoded)\n","\n","        # Decoding\n","        decoded = self.fc_decoder(encoded)\n","        decoded = self.decoder(decoded)\n","\n","        return encoded, decoded"],"metadata":{"id":"u56Uti9P5_ws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels"],"metadata":{"id":"_E5NCVi_6tOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","import numpy as np\n","from torch.utils.data import DataLoader\n","\n","# ------------------------------\n","# Step 1: Define Contrastive Loss Functions\n","# ------------------------------\n","\n","class NTXentLoss(nn.Module):\n","    def __init__(self, temperature=0.5):\n","        super(NTXentLoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, z_i, z_j):\n","        batch_size = z_i.size(0)\n","        z_i = F.normalize(z_i, dim=1)\n","        z_j = F.normalize(z_j, dim=1)\n","        z = torch.cat([z_i, z_j], dim=0)\n","        similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","        mask = ~torch.eye(2 * batch_size, device=z.device).bool()\n","        positives = torch.cat([torch.diag(similarity_matrix, batch_size), torch.diag(similarity_matrix, -batch_size)])\n","        negatives = similarity_matrix.masked_select(mask).view(2 * batch_size, -1)\n","        numerator = torch.exp(positives)\n","        denominator = torch.sum(torch.exp(negatives), dim=-1)\n","        return -torch.mean(torch.log(numerator / denominator))\n","\n","# VicReg Loss\n","class VicRegLoss(nn.Module):\n","    def __init__(self, lambda_var=25, mu_mean=25, nu_cov=1):\n","        super(VicRegLoss, self).__init__()\n","        self.lambda_var = lambda_var\n","        self.mu_mean = mu_mean\n","        self.nu_cov = nu_cov\n","\n","    def forward(self, z1, z2):\n","        variance_loss = torch.mean(torch.relu(1 - torch.std(z1, dim=0))) + torch.mean(torch.relu(1 - torch.std(z2, dim=0)))\n","        mean_loss = torch.mean((torch.mean(z1, dim=0) - torch.mean(z2, dim=0))**2)\n","        z1_centered = z1 - z1.mean(dim=0)\n","        z2_centered = z2 - z2.mean(dim=0)\n","        covariance_matrix_z1 = torch.mm(z1_centered.T, z1_centered) / (z1.size(0) - 1)\n","        covariance_matrix_z2 = torch.mm(z2_centered.T, z2_centered) / (z2.size(0) - 1)\n","        covariance_loss = torch.sum(covariance_matrix_z1 ** 2) - torch.sum(torch.diag(covariance_matrix_z1) ** 2) + \\\n","                          torch.sum(covariance_matrix_z2 ** 2) - torch.sum(torch.diag(covariance_matrix_z2) ** 2)\n","        return self.lambda_var * variance_loss + self.mu_mean * mean_loss + self.nu_cov * covariance_loss\n","\n","# Triplet Loss\n","class TripletLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(TripletLoss, self).__init__()\n","        self.margin = margin\n","        self.criterion = nn.TripletMarginWithDistanceLoss(\n","            distance_function=lambda a, b: 1.0 - F.cosine_similarity(a, b),\n","            margin=self.margin\n","        )\n","\n","    def forward(self, anchor, positive, negative):\n","        return self.criterion(anchor, positive, negative)\n","\n","# ------------------------------\n","# Step 2: Define Augmentation Function\n","# ------------------------------\n","\n","def augment(images):\n","    \"\"\"\n","    Apply tensor-based augmentations to images.\n","\n","    Args:\n","        images (torch.Tensor): Batch of images of shape (batch_size, 1, 28, 28).\n","\n","    Returns:\n","        torch.Tensor: Augmented images of the same shape as input.\n","    \"\"\"\n","    # Resize and crop (ensure output size matches input size)\n","    images = resize(images, size=[28, 28])\n","\n","    # Random horizontal flip\n","    if torch.rand(1) > 0.5:\n","        images = hflip(images)\n","\n","    return images\n","\n","# ------------------------------\n","# Step 3: Define Training Function\n","# ------------------------------\n","\n","def train_autoencoder(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    scheduler=None,\n","    epochs=10,\n","    device=\"cpu\",\n","    noise_factor=0.0,\n","    augment_fn=None,\n","    contrastive_loss_fn=None,\n","    temperature=0.5,\n","    triplet_data=False,\n","):\n","    \"\"\"\n","    Train an autoencoder model with support for contrastive learning, noise injection, and augmentations.\n","\n","    Args:\n","        model (nn.Module): The autoencoder model.\n","        data_loader (DataLoader): DataLoader for training data.\n","        loss_fn (callable): Primary loss function (e.g., reconstruction loss).\n","        optimizer (torch.optim.Optimizer): Optimizer for the model.\n","        scheduler (torch.optim.lr_scheduler._LRScheduler, optional): Learning rate scheduler.\n","        epochs (int): Number of epochs to train.\n","        device (str): Device to train on ('cpu' or 'cuda').\n","        noise_factor (float): Factor for adding noise to input images (denoising autoencoder).\n","        augment_fn (callable, optional): Augmentation function for contrastive learning.\n","        contrastive_loss_fn (callable, optional): Contrastive loss function (e.g., NT-Xent, triplet loss).\n","        temperature (float): Temperature parameter for NT-Xent loss.\n","        triplet_data (bool): Whether the data_loader provides triplets (anchor, positive, negative).\n","\n","    Returns:\n","        None: Prints loss values for each epoch.\n","    \"\"\"\n","    model.to(device).train()\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for batch in data_loader:\n","            # Prepare data based on whether it's triplet data or not\n","            if triplet_data:\n","                anchor, positive, negative = batch\n","                anchor, positive, negative = (\n","                    anchor.to(device).float(),\n","                    positive.to(device).float(),\n","                    negative.to(device).float(),\n","                )\n","                images = anchor  # Use anchor as the primary input for reconstruction\n","            else:\n","                images, _ = batch\n","                images = images.to(device).float()\n","\n","            # Add noise if specified\n","            if noise_factor > 0:\n","                noisy_images = images + noise_factor * torch.randn_like(images)\n","                noisy_images = torch.clamp(noisy_images, 0.0, 1.0)\n","                encoded, decoded = model(noisy_images)\n","            else:\n","                encoded, decoded = model(images)\n","\n","            # Compute reconstruction loss\n","            reconstruction_loss = loss_fn(decoded, images)\n","\n","            # Compute contrastive loss if specified\n","            contrastive_loss_value = 0\n","            if contrastive_loss_fn is not None:\n","                if triplet_data:\n","                    # Triplet loss\n","                    positive_encoded, _ = model(positive)\n","                    negative_encoded, _ = model(negative)\n","                    contrastive_loss_value = contrastive_loss_fn(encoded, positive_encoded, negative_encoded)\n","                else:\n","                    # NT-Xent or other contrastive loss\n","                    if augment_fn:\n","                        augmented_1 = augment_fn(images)\n","                        augmented_2 = augment_fn(images)\n","                        z1, _ = model(augmented_1)\n","                        z2, _ = model(augmented_2)\n","                    else:\n","                        z1, z2 = encoded, encoded  # Use the same embeddings if no augmentation\n","                    contrastive_loss_value = contrastive_loss_fn(z1, z2, temperature)\n","\n","            # Total loss\n","            total_loss_value = reconstruction_loss + contrastive_loss_value\n","\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            total_loss_value.backward()\n","            optimizer.step()\n","\n","            total_loss += total_loss_value.item()\n","\n","        # Step the scheduler if provided\n","        if scheduler:\n","            scheduler.step()\n","\n","        # Print epoch loss\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(data_loader):.4f}\")\n","\n","# ------------------------------\n","# Step 4: Configure Training Pipeline\n","# ------------------------------\n","\n","# ------------------------------\n","# Step 1: Model Selection and Parameters\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"IntermediateAutoencoder\"  # Choose from: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Choose from: \"autoencoders\", \"VAEs\", \"denoising_autoencoders\"\n","loss_type = \"mse\"  # Choose from: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","noise_factor = 0.0  # Noise factor for denoising autoencoders\n","temperature = 0.5  # Temperature parameter for NT-Xent loss\n","margin = 1.0  # Margin for Triplet Loss\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()  # Reconstruction loss\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1)  # VicReg loss\n","elif loss_type == \"ntxent\":\n","    criterion = NTXentLoss(temperature=temperature)  # NT-Xent loss\n","elif loss_type == \"triplet\":\n","    criterion = TripletLoss(margin=margin)  # Triplet loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 20 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 2: Training the Model\n","# ------------------------------\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_autoencoder(\n","    model=model,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    loss_fn=criterion,  # Primary loss function\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    epochs=100,\n","    device=device,\n","    noise_factor=noise_factor if embedding_type == \"denoising_autoencoders\" else 0.0,  # Add noise for denoising\n","    contrastive_loss_fn=criterion if loss_type in [\"vicreg\", \"ntxent\", \"triplet\"] else None,  # Optional: Contrastive loss\n","    augment_fn=augment if loss_type in [\"vicreg\", \"ntxent\"] else None,  # Optional: Augmentation function\n","    triplet_data=(loss_type == \"triplet\")  # Optional: Use triplet data\n",")\n","\n","# ------------------------------\n","# Step 3: Generate Embeddings\n","# ------------------------------\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    device=device\n",")\n","\n","# ------------------------------\n","# Step 4: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SDmzPRsGzDQp","executionInfo":{"status":"error","timestamp":1737562456314,"user_tz":-210,"elapsed":14409637,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"7135051e-61bf-4541-8aee-2f8252c3f03a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training IntermediateAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.0518\n","Epoch [2/100], Loss: 0.0172\n","Epoch [3/100], Loss: 0.0141\n","Epoch [4/100], Loss: 0.0125\n","Epoch [5/100], Loss: 0.0116\n","Epoch [6/100], Loss: 0.0108\n","Epoch [7/100], Loss: 0.0103\n","Epoch [8/100], Loss: 0.0100\n","Epoch [9/100], Loss: 0.0096\n","Epoch [10/100], Loss: 0.0093\n","Epoch [11/100], Loss: 0.0091\n","Epoch [12/100], Loss: 0.0089\n","Epoch [13/100], Loss: 0.0087\n","Epoch [14/100], Loss: 0.0086\n","Epoch [15/100], Loss: 0.0084\n","Epoch [16/100], Loss: 0.0083\n","Epoch [17/100], Loss: 0.0082\n","Epoch [18/100], Loss: 0.0081\n","Epoch [19/100], Loss: 0.0080\n","Epoch [20/100], Loss: 0.0080\n","Epoch [21/100], Loss: 0.0072\n","Epoch [22/100], Loss: 0.0071\n","Epoch [23/100], Loss: 0.0071\n","Epoch [24/100], Loss: 0.0071\n","Epoch [25/100], Loss: 0.0071\n","Epoch [26/100], Loss: 0.0071\n","Epoch [27/100], Loss: 0.0070\n","Epoch [28/100], Loss: 0.0070\n","Epoch [29/100], Loss: 0.0070\n","Epoch [30/100], Loss: 0.0070\n","Epoch [31/100], Loss: 0.0070\n","Epoch [32/100], Loss: 0.0070\n","Epoch [33/100], Loss: 0.0070\n","Epoch [34/100], Loss: 0.0070\n","Epoch [35/100], Loss: 0.0070\n","Epoch [36/100], Loss: 0.0069\n","Epoch [37/100], Loss: 0.0069\n","Epoch [38/100], Loss: 0.0069\n","Epoch [39/100], Loss: 0.0069\n","Epoch [40/100], Loss: 0.0069\n","Epoch [41/100], Loss: 0.0068\n","Epoch [42/100], Loss: 0.0068\n","Epoch [43/100], Loss: 0.0068\n","Epoch [44/100], Loss: 0.0068\n","Epoch [45/100], Loss: 0.0068\n","Epoch [46/100], Loss: 0.0068\n","Epoch [47/100], Loss: 0.0068\n","Epoch [48/100], Loss: 0.0068\n","Epoch [49/100], Loss: 0.0068\n","Epoch [50/100], Loss: 0.0068\n","Epoch [51/100], Loss: 0.0068\n","Epoch [52/100], Loss: 0.0068\n","Epoch [53/100], Loss: 0.0068\n","Epoch [54/100], Loss: 0.0068\n","Epoch [55/100], Loss: 0.0068\n","Epoch [56/100], Loss: 0.0068\n","Epoch [57/100], Loss: 0.0068\n","Epoch [58/100], Loss: 0.0068\n","Epoch [59/100], Loss: 0.0068\n","Epoch [60/100], Loss: 0.0068\n","Epoch [61/100], Loss: 0.0068\n","Epoch [62/100], Loss: 0.0068\n","Epoch [63/100], Loss: 0.0068\n","Epoch [64/100], Loss: 0.0068\n","Epoch [65/100], Loss: 0.0068\n","Epoch [66/100], Loss: 0.0068\n","Epoch [67/100], Loss: 0.0068\n","Epoch [68/100], Loss: 0.0068\n","Epoch [69/100], Loss: 0.0068\n","Epoch [70/100], Loss: 0.0068\n","Epoch [71/100], Loss: 0.0068\n","Epoch [72/100], Loss: 0.0068\n","Epoch [73/100], Loss: 0.0068\n","Epoch [74/100], Loss: 0.0068\n","Epoch [75/100], Loss: 0.0068\n","Epoch [76/100], Loss: 0.0068\n","Epoch [77/100], Loss: 0.0068\n","Epoch [78/100], Loss: 0.0068\n","Epoch [79/100], Loss: 0.0068\n","Epoch [80/100], Loss: 0.0068\n","Epoch [81/100], Loss: 0.0068\n","Epoch [82/100], Loss: 0.0068\n","Epoch [83/100], Loss: 0.0068\n","Epoch [84/100], Loss: 0.0068\n","Epoch [85/100], Loss: 0.0068\n","Epoch [86/100], Loss: 0.0068\n","Epoch [87/100], Loss: 0.0068\n","Epoch [88/100], Loss: 0.0068\n","Epoch [89/100], Loss: 0.0068\n","Epoch [90/100], Loss: 0.0068\n","Epoch [91/100], Loss: 0.0068\n","Epoch [92/100], Loss: 0.0068\n","Epoch [93/100], Loss: 0.0068\n","Epoch [94/100], Loss: 0.0068\n","Epoch [95/100], Loss: 0.0068\n","Epoch [96/100], Loss: 0.0068\n","Epoch [97/100], Loss: 0.0068\n","Epoch [98/100], Loss: 0.0068\n","Epoch [99/100], Loss: 0.0068\n","Epoch [100/100], Loss: 0.0068\n","Generating embeddings using IntermediateAutoencoder with mse loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"generate_embeddings() missing 1 required positional argument: 'embedding_type'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-540c87ac3aa1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;31m# Generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generating embeddings using {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m embeddings, labels = generate_embeddings(\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Your DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: generate_embeddings() missing 1 required positional argument: 'embedding_type'"]}]},{"cell_type":"code","source":["print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    embedding_type= embedding_type,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    device=device\n",")\n","\n","# ------------------------------\n","# Step 4: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhthd1DRy3w8","executionInfo":{"status":"ok","timestamp":1737562615217,"user_tz":-210,"elapsed":59878,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"8e829cd2-551c-4ea4-a2b0-ce45a1b4cd7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating embeddings using IntermediateAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"AdvancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHkQTUPaOsBs","executionInfo":{"status":"ok","timestamp":1737386767714,"user_tz":-210,"elapsed":1502224,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"2f26c37a-85ac-47d2-daaa-d53499521289"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training AdvancedAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.6372\n","Epoch [2/100], Loss: 0.3665\n","Epoch [3/100], Loss: 0.3064\n","Epoch [4/100], Loss: 0.2097\n","Epoch [5/100], Loss: 0.0671\n","Epoch [6/100], Loss: 0.0357\n","Epoch [7/100], Loss: 0.0296\n","Epoch [8/100], Loss: 0.0267\n","Epoch [9/100], Loss: 0.0248\n","Epoch [10/100], Loss: 0.0227\n","Epoch [11/100], Loss: 0.0216\n","Epoch [12/100], Loss: 0.0205\n","Epoch [13/100], Loss: 0.0197\n","Epoch [14/100], Loss: 0.0188\n","Epoch [15/100], Loss: 0.0180\n","Epoch [16/100], Loss: 0.0174\n","Epoch [17/100], Loss: 0.0167\n","Epoch [18/100], Loss: 0.0162\n","Epoch [19/100], Loss: 0.0158\n","Epoch [20/100], Loss: 0.0154\n","Epoch [21/100], Loss: 0.0149\n","Epoch [22/100], Loss: 0.0147\n","Epoch [23/100], Loss: 0.0144\n","Epoch [24/100], Loss: 0.0142\n","Epoch [25/100], Loss: 0.0138\n","Epoch [26/100], Loss: 0.0136\n","Epoch [27/100], Loss: 0.0135\n","Epoch [28/100], Loss: 0.0131\n","Epoch [29/100], Loss: 0.0128\n","Epoch [30/100], Loss: 0.0128\n","Epoch [31/100], Loss: 0.0124\n","Epoch [32/100], Loss: 0.0124\n","Epoch [33/100], Loss: 0.0121\n","Epoch [34/100], Loss: 0.0118\n","Epoch [35/100], Loss: 0.0120\n","Epoch [36/100], Loss: 0.0117\n","Epoch [37/100], Loss: 0.0117\n","Epoch [38/100], Loss: 0.0117\n","Epoch [39/100], Loss: 0.0112\n","Epoch [40/100], Loss: 0.0115\n","Epoch [41/100], Loss: 0.0111\n","Epoch [42/100], Loss: 0.0112\n","Epoch [43/100], Loss: 0.0109\n","Epoch [44/100], Loss: 0.0106\n","Epoch [45/100], Loss: 0.0105\n","Epoch [46/100], Loss: 0.0103\n","Epoch [47/100], Loss: 0.0103\n","Epoch [48/100], Loss: 0.0103\n","Epoch [49/100], Loss: 0.0101\n","Epoch [50/100], Loss: 0.0099\n","Epoch [51/100], Loss: 0.0099\n","Epoch [52/100], Loss: 0.0098\n","Epoch [53/100], Loss: 0.0098\n","Epoch [54/100], Loss: 0.0098\n","Epoch [55/100], Loss: 0.0097\n","Epoch [56/100], Loss: 0.0097\n","Epoch [57/100], Loss: 0.0094\n","Epoch [58/100], Loss: 0.0094\n","Epoch [59/100], Loss: 0.0092\n","Epoch [60/100], Loss: 0.0092\n","Epoch [61/100], Loss: 0.0092\n","Epoch [62/100], Loss: 0.0090\n","Epoch [63/100], Loss: 0.0090\n","Epoch [64/100], Loss: 0.0090\n","Epoch [65/100], Loss: 0.0089\n","Epoch [66/100], Loss: 0.0090\n","Epoch [67/100], Loss: 0.0088\n","Epoch [68/100], Loss: 0.0088\n","Epoch [69/100], Loss: 0.0088\n","Epoch [70/100], Loss: 0.0086\n","Epoch [71/100], Loss: 0.0086\n","Epoch [72/100], Loss: 0.0085\n","Epoch [73/100], Loss: 0.0084\n","Epoch [74/100], Loss: 0.0084\n","Epoch [75/100], Loss: 0.0085\n","Epoch [76/100], Loss: 0.0084\n","Epoch [77/100], Loss: 0.0083\n","Epoch [78/100], Loss: 0.0082\n","Epoch [79/100], Loss: 0.0082\n","Epoch [80/100], Loss: 0.0082\n","Epoch [81/100], Loss: 0.0079\n","Epoch [82/100], Loss: 0.0080\n","Epoch [83/100], Loss: 0.0080\n","Epoch [84/100], Loss: 0.0081\n","Epoch [85/100], Loss: 0.0078\n","Epoch [86/100], Loss: 0.0078\n","Epoch [87/100], Loss: 0.0079\n","Epoch [88/100], Loss: 0.0078\n","Epoch [89/100], Loss: 0.0078\n","Epoch [90/100], Loss: 0.0077\n","Epoch [91/100], Loss: 0.0077\n","Epoch [92/100], Loss: 0.0076\n","Epoch [93/100], Loss: 0.0076\n","Epoch [94/100], Loss: 0.0076\n","Epoch [95/100], Loss: 0.0076\n","Epoch [96/100], Loss: 0.0075\n","Epoch [97/100], Loss: 0.0074\n","Epoch [98/100], Loss: 0.0075\n","Epoch [99/100], Loss: 0.0074\n","Epoch [100/100], Loss: 0.0074\n","Generating embeddings using AdvancedAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"IntermediateAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOYys9LMQ4Bc","executionInfo":{"status":"ok","timestamp":1737388309401,"user_tz":-210,"elapsed":1080757,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"ea235020-6123-42e9-b127-eadaca700bca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training IntermediateAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.3440\n","Epoch [2/100], Loss: 0.0693\n","Epoch [3/100], Loss: 0.0423\n","Epoch [4/100], Loss: 0.0327\n","Epoch [5/100], Loss: 0.0277\n","Epoch [6/100], Loss: 0.0246\n","Epoch [7/100], Loss: 0.0222\n","Epoch [8/100], Loss: 0.0205\n","Epoch [9/100], Loss: 0.0191\n","Epoch [10/100], Loss: 0.0181\n","Epoch [11/100], Loss: 0.0174\n","Epoch [12/100], Loss: 0.0165\n","Epoch [13/100], Loss: 0.0157\n","Epoch [14/100], Loss: 0.0149\n","Epoch [15/100], Loss: 0.0147\n","Epoch [16/100], Loss: 0.0139\n","Epoch [17/100], Loss: 0.0136\n","Epoch [18/100], Loss: 0.0136\n","Epoch [19/100], Loss: 0.0130\n","Epoch [20/100], Loss: 0.0128\n","Epoch [21/100], Loss: 0.0124\n","Epoch [22/100], Loss: 0.0122\n","Epoch [23/100], Loss: 0.0116\n","Epoch [24/100], Loss: 0.0116\n","Epoch [25/100], Loss: 0.0114\n","Epoch [26/100], Loss: 0.0111\n","Epoch [27/100], Loss: 0.0111\n","Epoch [28/100], Loss: 0.0109\n","Epoch [29/100], Loss: 0.0106\n","Epoch [30/100], Loss: 0.0103\n","Epoch [31/100], Loss: 0.0106\n","Epoch [32/100], Loss: 0.0103\n","Epoch [33/100], Loss: 0.0101\n","Epoch [34/100], Loss: 0.0098\n","Epoch [35/100], Loss: 0.0099\n","Epoch [36/100], Loss: 0.0095\n","Epoch [37/100], Loss: 0.0095\n","Epoch [38/100], Loss: 0.0095\n","Epoch [39/100], Loss: 0.0094\n","Epoch [40/100], Loss: 0.0093\n","Epoch [41/100], Loss: 0.0091\n","Epoch [42/100], Loss: 0.0091\n","Epoch [43/100], Loss: 0.0088\n","Epoch [44/100], Loss: 0.0088\n","Epoch [45/100], Loss: 0.0087\n","Epoch [46/100], Loss: 0.0086\n","Epoch [47/100], Loss: 0.0088\n","Epoch [48/100], Loss: 0.0086\n","Epoch [49/100], Loss: 0.0084\n","Epoch [50/100], Loss: 0.0085\n","Epoch [51/100], Loss: 0.0084\n","Epoch [52/100], Loss: 0.0082\n","Epoch [53/100], Loss: 0.0082\n","Epoch [54/100], Loss: 0.0082\n","Epoch [55/100], Loss: 0.0079\n","Epoch [56/100], Loss: 0.0079\n","Epoch [57/100], Loss: 0.0079\n","Epoch [58/100], Loss: 0.0079\n","Epoch [59/100], Loss: 0.0079\n","Epoch [60/100], Loss: 0.0076\n","Epoch [61/100], Loss: 0.0076\n","Epoch [62/100], Loss: 0.0076\n","Epoch [63/100], Loss: 0.0077\n","Epoch [64/100], Loss: 0.0075\n","Epoch [65/100], Loss: 0.0075\n","Epoch [66/100], Loss: 0.0075\n","Epoch [67/100], Loss: 0.0074\n","Epoch [68/100], Loss: 0.0073\n","Epoch [69/100], Loss: 0.0073\n","Epoch [70/100], Loss: 0.0072\n","Epoch [71/100], Loss: 0.0071\n","Epoch [72/100], Loss: 0.0072\n","Epoch [73/100], Loss: 0.0071\n","Epoch [74/100], Loss: 0.0071\n","Epoch [75/100], Loss: 0.0069\n","Epoch [76/100], Loss: 0.0069\n","Epoch [77/100], Loss: 0.0070\n","Epoch [78/100], Loss: 0.0070\n","Epoch [79/100], Loss: 0.0068\n","Epoch [80/100], Loss: 0.0069\n","Epoch [81/100], Loss: 0.0069\n","Epoch [82/100], Loss: 0.0067\n","Epoch [83/100], Loss: 0.0066\n","Epoch [84/100], Loss: 0.0066\n","Epoch [85/100], Loss: 0.0067\n","Epoch [86/100], Loss: 0.0066\n","Epoch [87/100], Loss: 0.0066\n","Epoch [88/100], Loss: 0.0066\n","Epoch [89/100], Loss: 0.0065\n","Epoch [90/100], Loss: 0.0065\n","Epoch [91/100], Loss: 0.0065\n","Epoch [92/100], Loss: 0.0065\n","Epoch [93/100], Loss: 0.0067\n","Epoch [94/100], Loss: 0.0066\n","Epoch [95/100], Loss: 0.0064\n","Epoch [96/100], Loss: 0.0063\n","Epoch [97/100], Loss: 0.0063\n","Epoch [98/100], Loss: 0.0063\n","Epoch [99/100], Loss: 0.0063\n","Epoch [100/100], Loss: 0.0063\n","Generating embeddings using IntermediateAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"BasicAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztH2i-oLRBUm","executionInfo":{"status":"ok","timestamp":1737388913536,"user_tz":-210,"elapsed":604139,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"2b5c963a-fc06-4b83-d253-fb74b3f2e0b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training BasicAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.3403\n","Epoch [2/100], Loss: 0.2061\n","Epoch [3/100], Loss: 0.1146\n","Epoch [4/100], Loss: 0.0868\n","Epoch [5/100], Loss: 0.0737\n","Epoch [6/100], Loss: 0.0653\n","Epoch [7/100], Loss: 0.0595\n","Epoch [8/100], Loss: 0.0546\n","Epoch [9/100], Loss: 0.0507\n","Epoch [10/100], Loss: 0.0478\n","Epoch [11/100], Loss: 0.0455\n","Epoch [12/100], Loss: 0.0433\n","Epoch [13/100], Loss: 0.0413\n","Epoch [14/100], Loss: 0.0398\n","Epoch [15/100], Loss: 0.0387\n","Epoch [16/100], Loss: 0.0373\n","Epoch [17/100], Loss: 0.0362\n","Epoch [18/100], Loss: 0.0352\n","Epoch [19/100], Loss: 0.0340\n","Epoch [20/100], Loss: 0.0332\n","Epoch [21/100], Loss: 0.0327\n","Epoch [22/100], Loss: 0.0319\n","Epoch [23/100], Loss: 0.0312\n","Epoch [24/100], Loss: 0.0305\n","Epoch [25/100], Loss: 0.0299\n","Epoch [26/100], Loss: 0.0295\n","Epoch [27/100], Loss: 0.0291\n","Epoch [28/100], Loss: 0.0286\n","Epoch [29/100], Loss: 0.0280\n","Epoch [30/100], Loss: 0.0278\n","Epoch [31/100], Loss: 0.0274\n","Epoch [32/100], Loss: 0.0269\n","Epoch [33/100], Loss: 0.0267\n","Epoch [34/100], Loss: 0.0263\n","Epoch [35/100], Loss: 0.0262\n","Epoch [36/100], Loss: 0.0257\n","Epoch [37/100], Loss: 0.0254\n","Epoch [38/100], Loss: 0.0251\n","Epoch [39/100], Loss: 0.0248\n","Epoch [40/100], Loss: 0.0246\n","Epoch [41/100], Loss: 0.0244\n","Epoch [42/100], Loss: 0.0241\n","Epoch [43/100], Loss: 0.0238\n","Epoch [44/100], Loss: 0.0235\n","Epoch [45/100], Loss: 0.0234\n","Epoch [46/100], Loss: 0.0234\n","Epoch [47/100], Loss: 0.0230\n","Epoch [48/100], Loss: 0.0228\n","Epoch [49/100], Loss: 0.0227\n","Epoch [50/100], Loss: 0.0225\n","Epoch [51/100], Loss: 0.0225\n","Epoch [52/100], Loss: 0.0223\n","Epoch [53/100], Loss: 0.0221\n","Epoch [54/100], Loss: 0.0219\n","Epoch [55/100], Loss: 0.0217\n","Epoch [56/100], Loss: 0.0216\n","Epoch [57/100], Loss: 0.0214\n","Epoch [58/100], Loss: 0.0212\n","Epoch [59/100], Loss: 0.0212\n","Epoch [60/100], Loss: 0.0211\n","Epoch [61/100], Loss: 0.0210\n","Epoch [62/100], Loss: 0.0207\n","Epoch [63/100], Loss: 0.0205\n","Epoch [64/100], Loss: 0.0206\n","Epoch [65/100], Loss: 0.0204\n","Epoch [66/100], Loss: 0.0204\n","Epoch [67/100], Loss: 0.0202\n","Epoch [68/100], Loss: 0.0202\n","Epoch [69/100], Loss: 0.0199\n","Epoch [70/100], Loss: 0.0199\n","Epoch [71/100], Loss: 0.0199\n","Epoch [72/100], Loss: 0.0197\n","Epoch [73/100], Loss: 0.0197\n","Epoch [74/100], Loss: 0.0195\n","Epoch [75/100], Loss: 0.0195\n","Epoch [76/100], Loss: 0.0194\n","Epoch [77/100], Loss: 0.0192\n","Epoch [78/100], Loss: 0.0193\n","Epoch [79/100], Loss: 0.0192\n","Epoch [80/100], Loss: 0.0190\n","Epoch [81/100], Loss: 0.0189\n","Epoch [82/100], Loss: 0.0187\n","Epoch [83/100], Loss: 0.0189\n","Epoch [84/100], Loss: 0.0188\n","Epoch [85/100], Loss: 0.0187\n","Epoch [86/100], Loss: 0.0187\n","Epoch [87/100], Loss: 0.0186\n","Epoch [88/100], Loss: 0.0185\n","Epoch [89/100], Loss: 0.0185\n","Epoch [90/100], Loss: 0.0184\n","Epoch [91/100], Loss: 0.0183\n","Epoch [92/100], Loss: 0.0182\n","Epoch [93/100], Loss: 0.0182\n","Epoch [94/100], Loss: 0.0180\n","Epoch [95/100], Loss: 0.0181\n","Epoch [96/100], Loss: 0.0180\n","Epoch [97/100], Loss: 0.0179\n","Epoch [98/100], Loss: 0.0179\n","Epoch [99/100], Loss: 0.0179\n","Epoch [100/100], Loss: 0.0178\n","Generating embeddings using BasicAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"EnhancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=20, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"MAaK-hZjah3-","executionInfo":{"status":"error","timestamp":1737472116552,"user_tz":-210,"elapsed":921,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"1b3b5306-3d86-44c6-cb6f-db17e3b68ccf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training EnhancedAutoencoder with mse loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"train_autoencoder() got an unexpected keyword argument 'scheduler'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-53762437c672>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: train_autoencoder() got an unexpected keyword argument 'scheduler'"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"FlexibleVAE\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"vae\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"Y_eXsL-sYuUl","executionInfo":{"status":"error","timestamp":1737455119844,"user_tz":-210,"elapsed":22324,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"08b18305-34ca-4681-8483-06f3db77f65d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training FlexibleVAE with vae loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"'function' object is not iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-615e66d254db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/embeddings/encoder_training.py\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(vae, train_loader, val_loader, optimizer, scheduler, loss_fn, epochs, device, save_best, save_path)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mtotal_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"AdvancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"ntxent\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"c4lXegQrZEhF","executionInfo":{"status":"error","timestamp":1737471735460,"user_tz":-210,"elapsed":16599432,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"c5a121af-1603-4ac2-c017-94c8870c0526"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training AdvancedAutoencoder with ntxent loss...\n","Epoch [1/200], Loss: 3.2167\n","Epoch [2/200], Loss: 3.1131\n","Epoch [3/200], Loss: 3.0889\n","Epoch [4/200], Loss: 3.0769\n","Epoch [5/200], Loss: 3.0689\n","Epoch [6/200], Loss: 3.0646\n","Epoch [7/200], Loss: 3.0587\n","Epoch [8/200], Loss: 3.0553\n","Epoch [9/200], Loss: 3.0534\n","Epoch [10/200], Loss: 3.0497\n","Epoch [11/200], Loss: 3.0399\n","Epoch [12/200], Loss: 3.0386\n","Epoch [13/200], Loss: 3.0380\n","Epoch [14/200], Loss: 3.0374\n","Epoch [15/200], Loss: 3.0367\n","Epoch [16/200], Loss: 3.0362\n","Epoch [17/200], Loss: 3.0360\n","Epoch [18/200], Loss: 3.0358\n","Epoch [19/200], Loss: 3.0352\n","Epoch [20/200], Loss: 3.0348\n","Epoch [21/200], Loss: 3.0337\n","Epoch [22/200], Loss: 3.0331\n","Epoch [23/200], Loss: 3.0336\n","Epoch [24/200], Loss: 3.0332\n","Epoch [25/200], Loss: 3.0330\n","Epoch [26/200], Loss: 3.0332\n","Epoch [27/200], Loss: 3.0334\n","Epoch [28/200], Loss: 3.0329\n","Epoch [29/200], Loss: 3.0338\n","Epoch [30/200], Loss: 3.0334\n","Epoch [31/200], Loss: 3.0331\n","Epoch [32/200], Loss: 3.0330\n","Epoch [33/200], Loss: 3.0332\n","Epoch [34/200], Loss: 3.0333\n","Epoch [35/200], Loss: 3.0332\n","Epoch [36/200], Loss: 3.0328\n","Epoch [37/200], Loss: 3.0333\n","Epoch [38/200], Loss: 3.0334\n","Epoch [39/200], Loss: 3.0329\n","Epoch [40/200], Loss: 3.0332\n","Epoch [41/200], Loss: 3.0327\n","Epoch [42/200], Loss: 3.0332\n","Epoch [43/200], Loss: 3.0328\n","Epoch [44/200], Loss: 3.0327\n","Epoch [45/200], Loss: 3.0331\n","Epoch [46/200], Loss: 3.0333\n","Epoch [47/200], Loss: 3.0329\n","Epoch [48/200], Loss: 3.0332\n","Epoch [49/200], Loss: 3.0330\n","Epoch [50/200], Loss: 3.0325\n","Epoch [51/200], Loss: 3.0330\n","Epoch [52/200], Loss: 3.0334\n","Epoch [53/200], Loss: 3.0337\n","Epoch [54/200], Loss: 3.0332\n","Epoch [55/200], Loss: 3.0332\n","Epoch [56/200], Loss: 3.0329\n","Epoch [57/200], Loss: 3.0328\n","Epoch [58/200], Loss: 3.0329\n","Epoch [59/200], Loss: 3.0331\n","Epoch [60/200], Loss: 3.0331\n","Epoch [61/200], Loss: 3.0329\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fd37e98c6c71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/embeddings/encoder_training.py\u001b[0m in \u001b[0;36mtrain_with_ntxent_loss\u001b[0;34m(model, data_loader, ntxent_loss_fn, optimizer, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"FlexibleVAE\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"vae\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"id":"d3HnUC18Y12q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Configuration\n","# ------------------------------\n","\n","save_format = \"pt\"  # Change to \"npy\" if needed\n","\n","# Ensure valid save format\n","if save_format not in ['pt', 'npy']:\n","    print(f\"Invalid save format: {save_format}. Defaulting to 'pt'.\")\n","    save_format = 'pt'\n","\n","# Define model and loss type for naming conventions (update as needed)\n","model_name = \"matrix_factorization\"  # Example model name\n","loss_type = \"default_loss\"  # Update this as necessary (e.g., \"mse\", \"contrastive\", etc.)\n","\n","# ------------------------------\n","# Step 2: Matrix Factorization\n","# ------------------------------\n","\n","# Extract flattened images and labels\n","sampled_x, sampled_y = mnist_loader.dataset.tensors[0].numpy(), mnist_loader.dataset.tensors[1].numpy()\n","\n","print(\"Processing matrix factorization models (PCA, SVD, NMF)...\")\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","factorized_embeddings, factorized_labels = process_matrix_factorization(\n","    sampled_x, sampled_y, n_components=50\n",")\n","for method, embeddings in factorized_embeddings.items():\n","    embedding_subdir = f\"matrix_factorization_{method}\"\n","    embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","    os.makedirs(embedding_dir, exist_ok=True)\n","\n","    if save_format == \"pt\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_{method}_embeddings.pt\")\n","        torch.save({\"embeddings\": embeddings, \"labels\": factorized_labels}, embedding_file)\n","        print(f\"{method} embeddings saved in PyTorch format: {embedding_file}\")\n","    elif save_format == \"npy\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_{method}_embeddings.npy\")\n","        np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": factorized_labels.numpy()})\n","        print(f\"{method} embeddings saved in NumPy format: {embedding_file}\")\n","    else:\n","        raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 3: SIFT Features\n","# ------------------------------\n","\n","print(\"Processing SIFT features...\")\n","sift_features = apply_sift(sampled_x, n_features=50)\n","sift_labels = torch.tensor(sampled_y, dtype=torch.long)\n","\n","embedding_subdir = \"sift_features\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_sift_embeddings.pt\")\n","    torch.save({\"embeddings\": torch.tensor(sift_features), \"labels\": sift_labels}, embedding_file)\n","    print(f\"SIFT embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_sift_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": sift_features, \"labels\": sift_labels.numpy()})\n","    print(f\"SIFT embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 4: Kernel PCA\n","# ------------------------------\n","\n","print(\"Processing Kernel PCA...\")\n","kernel_pca_features, kernel_pca_labels = process_feature_extraction(\n","    sampled_x, sampled_y, n_features=50, kernel=\"rbf\", n_components=50\n",")\n","for method, embeddings in kernel_pca_features.items():\n","    embedding_subdir = f\"kernel_pca_{method}\"\n","    embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","    os.makedirs(embedding_dir, exist_ok=True)\n","\n","    if save_format == \"pt\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_kernel_pca_{method}_embeddings.pt\")\n","        torch.save({\"embeddings\": embeddings, \"labels\": kernel_pca_labels}, embedding_file)\n","        print(f\"{method} Kernel PCA embeddings saved in PyTorch format: {embedding_file}\")\n","    elif save_format == \"npy\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_kernel_pca_{method}_embeddings.npy\")\n","        np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": kernel_pca_labels.numpy()})\n","        print(f\"{method} Kernel PCA embeddings saved in NumPy format: {embedding_file}\")\n","    else:\n","        raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 5: Normalizing Flow\n","# ------------------------------\n","\n","print(\"Processing Normalizing Flow...\")\n","for method, embeddings in factorized_embeddings.items():\n","    # Initialize Normalizing Flow model\n","    input_dim = embeddings.size(1)\n","    nf_model = NormalizingFlowModel(input_dim=input_dim, num_flows=4)\n","    nf_model.to(device)\n","\n","    # Train Normalizing Flow model\n","    trained_nf_model = train_nf_model(\n","        nf_model, embeddings, num_epochs=200, lr=1e-3, batch_size=128\n","    )\n","\n","    # Refine embeddings\n","    with torch.no_grad():\n","        refined_embeddings, _ = trained_nf_model(embeddings)\n","\n","        embedding_subdir = f\"normalizing_flow_{method}\"\n","        embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","        os.makedirs(embedding_dir, exist_ok=True)\n","\n","        if save_format == \"pt\":\n","            embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_normalizing_flow_{method}_refined_embeddings.pt\")\n","            torch.save({\"embeddings\": refined_embeddings, \"labels\": factorized_labels}, embedding_file)\n","            print(f\"{method} refined embeddings (Normalizing Flow) saved in PyTorch format: {embedding_file}\")\n","        elif save_format == \"npy\":\n","            embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_normalizing_flow_{method}_refined_embeddings.npy\")\n","            np.save(embedding_file, {\"embeddings\": refined_embeddings.numpy(), \"labels\": factorized_labels.numpy()})\n","            print(f\"{method} refined embeddings (Normalizing Flow) saved in NumPy format: {embedding_file}\")\n","        else:\n","            raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","print(\"Feature extraction and normalizing flow processing complete!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BavfjvO2VAjZ","executionInfo":{"status":"ok","timestamp":1737454621474,"user_tz":-210,"elapsed":6311,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"e0a053ea-f4c2-44cb-ab01-5b8c07c45d6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing matrix factorization models (PCA, SVD, NMF)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_PCA/matrix_factorization_default_loss_PCA_embeddings.pt\n","SVD embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_SVD/matrix_factorization_default_loss_SVD_embeddings.pt\n","NMF embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_NMF/matrix_factorization_default_loss_NMF_embeddings.pt\n","Processing SIFT features...\n","SIFT embeddings saved in PyTorch format: ./saved_embeddings/embeddings/sift_features/matrix_factorization_default_loss_sift_embeddings.pt\n","Processing Kernel PCA...\n","SIFT Kernel PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/kernel_pca_SIFT/matrix_factorization_default_loss_kernel_pca_SIFT_embeddings.pt\n","Kernel PCA Kernel PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/kernel_pca_Kernel PCA/matrix_factorization_default_loss_kernel_pca_Kernel PCA_embeddings.pt\n","Processing Normalizing Flow...\n","Epoch 1/200, Loss: 2105532.0000\n","Epoch 2/200, Loss: 1371861.2500\n","Epoch 3/200, Loss: 1126910.5000\n","Epoch 4/200, Loss: 959351.8750\n","Epoch 5/200, Loss: 864164.3125\n","Epoch 6/200, Loss: 786244.5000\n","Epoch 7/200, Loss: 721899.8125\n","Epoch 8/200, Loss: 664512.6250\n","Epoch 9/200, Loss: 609381.1875\n","Epoch 10/200, Loss: 559293.9375\n","Epoch 11/200, Loss: 510937.0938\n","Epoch 12/200, Loss: 470874.3438\n","Epoch 13/200, Loss: 436454.6250\n","Epoch 14/200, Loss: 404345.8750\n","Epoch 15/200, Loss: 379761.9062\n","Epoch 16/200, Loss: 357777.1250\n","Epoch 17/200, Loss: 335547.7500\n","Epoch 18/200, Loss: 314221.4062\n","Epoch 19/200, Loss: 294646.0000\n","Epoch 20/200, Loss: 277729.7500\n","Epoch 21/200, Loss: 262339.3750\n","Epoch 22/200, Loss: 248801.9062\n","Epoch 23/200, Loss: 237040.6875\n","Epoch 24/200, Loss: 225382.1250\n","Epoch 25/200, Loss: 213554.0469\n","Epoch 26/200, Loss: 203046.3906\n","Epoch 27/200, Loss: 192870.7812\n","Epoch 28/200, Loss: 184185.5625\n","Epoch 29/200, Loss: 175942.1875\n","Epoch 30/200, Loss: 167710.6562\n","Epoch 31/200, Loss: 159133.3750\n","Epoch 32/200, Loss: 151090.0625\n","Epoch 33/200, Loss: 143847.4219\n","Epoch 34/200, Loss: 136919.7656\n","Epoch 35/200, Loss: 130742.0703\n","Epoch 36/200, Loss: 125214.0391\n","Epoch 37/200, Loss: 120406.5625\n","Epoch 38/200, Loss: 116063.0234\n","Epoch 39/200, Loss: 111900.5469\n","Epoch 40/200, Loss: 107834.9375\n","Epoch 41/200, Loss: 103941.9062\n","Epoch 42/200, Loss: 100229.7344\n","Epoch 43/200, Loss: 96637.6250\n","Epoch 44/200, Loss: 93144.7109\n","Epoch 45/200, Loss: 89765.5312\n","Epoch 46/200, Loss: 86543.3594\n","Epoch 47/200, Loss: 83587.8281\n","Epoch 48/200, Loss: 80868.2891\n","Epoch 49/200, Loss: 78263.9922\n","Epoch 50/200, Loss: 75727.1953\n","Epoch 51/200, Loss: 73238.8438\n","Epoch 52/200, Loss: 70822.8906\n","Epoch 53/200, Loss: 68554.4844\n","Epoch 54/200, Loss: 66264.5625\n","Epoch 55/200, Loss: 64225.9844\n","Epoch 56/200, Loss: 62319.0078\n","Epoch 57/200, Loss: 60492.9102\n","Epoch 58/200, Loss: 58792.5703\n","Epoch 59/200, Loss: 57237.7578\n","Epoch 60/200, Loss: 55801.0859\n","Epoch 61/200, Loss: 54346.9727\n","Epoch 62/200, Loss: 52881.1914\n","Epoch 63/200, Loss: 51527.9141\n","Epoch 64/200, Loss: 50278.5625\n","Epoch 65/200, Loss: 49091.9766\n","Epoch 66/200, Loss: 47941.2852\n","Epoch 67/200, Loss: 46817.4375\n","Epoch 68/200, Loss: 45729.6016\n","Epoch 69/200, Loss: 44685.0000\n","Epoch 70/200, Loss: 43661.2500\n","Epoch 71/200, Loss: 42656.9375\n","Epoch 72/200, Loss: 41696.7031\n","Epoch 73/200, Loss: 40776.6484\n","Epoch 74/200, Loss: 39884.7383\n","Epoch 75/200, Loss: 39019.6680\n","Epoch 76/200, Loss: 38188.8594\n","Epoch 77/200, Loss: 37394.0898\n","Epoch 78/200, Loss: 36625.8203\n","Epoch 79/200, Loss: 35882.1875\n","Epoch 80/200, Loss: 35169.9102\n","Epoch 81/200, Loss: 34486.4688\n","Epoch 82/200, Loss: 33823.5742\n","Epoch 83/200, Loss: 33177.8164\n","Epoch 84/200, Loss: 32551.8242\n","Epoch 85/200, Loss: 31946.8691\n","Epoch 86/200, Loss: 31359.3652\n","Epoch 87/200, Loss: 30787.3340\n","Epoch 88/200, Loss: 30233.3398\n","Epoch 89/200, Loss: 29697.4609\n","Epoch 90/200, Loss: 29175.7383\n","Epoch 91/200, Loss: 28666.6055\n","Epoch 92/200, Loss: 28172.3789\n","Epoch 93/200, Loss: 27692.1426\n","Epoch 94/200, Loss: 27222.4727\n","Epoch 95/200, Loss: 26765.9258\n","Epoch 96/200, Loss: 26324.6719\n","Epoch 97/200, Loss: 25896.7305\n","Epoch 98/200, Loss: 25481.9746\n","Epoch 99/200, Loss: 25080.5391\n","Epoch 100/200, Loss: 24689.1719\n","Epoch 101/200, Loss: 24305.9316\n","Epoch 102/200, Loss: 23931.9062\n","Epoch 103/200, Loss: 23566.7090\n","Epoch 104/200, Loss: 23209.2207\n","Epoch 105/200, Loss: 22859.9180\n","Epoch 106/200, Loss: 22518.7441\n","Epoch 107/200, Loss: 22184.3594\n","Epoch 108/200, Loss: 21856.7676\n","Epoch 109/200, Loss: 21537.1270\n","Epoch 110/200, Loss: 21225.8438\n","Epoch 111/200, Loss: 20922.8809\n","Epoch 112/200, Loss: 20628.1504\n","Epoch 113/200, Loss: 20341.0020\n","Epoch 114/200, Loss: 20060.4609\n","Epoch 115/200, Loss: 19785.9141\n","Epoch 116/200, Loss: 19516.7344\n","Epoch 117/200, Loss: 19252.1523\n","Epoch 118/200, Loss: 18992.1680\n","Epoch 119/200, Loss: 18737.8105\n","Epoch 120/200, Loss: 18490.2461\n","Epoch 121/200, Loss: 18250.1562\n","Epoch 122/200, Loss: 18017.6680\n","Epoch 123/200, Loss: 17792.3457\n","Epoch 124/200, Loss: 17573.5000\n","Epoch 125/200, Loss: 17360.6152\n","Epoch 126/200, Loss: 17153.1270\n","Epoch 127/200, Loss: 16950.0996\n","Epoch 128/200, Loss: 16750.5918\n","Epoch 129/200, Loss: 16554.3926\n","Epoch 130/200, Loss: 16361.8965\n","Epoch 131/200, Loss: 16173.4922\n","Epoch 132/200, Loss: 15989.2266\n","Epoch 133/200, Loss: 15808.9131\n","Epoch 134/200, Loss: 15632.2549\n","Epoch 135/200, Loss: 15458.9541\n","Epoch 136/200, Loss: 15288.8477\n","Epoch 137/200, Loss: 15121.9639\n","Epoch 138/200, Loss: 14958.4229\n","Epoch 139/200, Loss: 14798.3320\n","Epoch 140/200, Loss: 14641.7598\n","Epoch 141/200, Loss: 14488.6709\n","Epoch 142/200, Loss: 14338.8359\n","Epoch 143/200, Loss: 14191.9453\n","Epoch 144/200, Loss: 14047.8545\n","Epoch 145/200, Loss: 13906.6152\n","Epoch 146/200, Loss: 13768.2539\n","Epoch 147/200, Loss: 13632.6572\n","Epoch 148/200, Loss: 13499.6143\n","Epoch 149/200, Loss: 13368.9805\n","Epoch 150/200, Loss: 13240.6631\n","Epoch 151/200, Loss: 13114.6094\n","Epoch 152/200, Loss: 12990.8145\n","Epoch 153/200, Loss: 12869.2852\n","Epoch 154/200, Loss: 12750.0078\n","Epoch 155/200, Loss: 12632.9160\n","Epoch 156/200, Loss: 12517.9258\n","Epoch 157/200, Loss: 12404.9639\n","Epoch 158/200, Loss: 12294.0127\n","Epoch 159/200, Loss: 12185.0859\n","Epoch 160/200, Loss: 12078.1631\n","Epoch 161/200, Loss: 11973.1680\n","Epoch 162/200, Loss: 11870.0000\n","Epoch 163/200, Loss: 11768.5840\n","Epoch 164/200, Loss: 11668.8877\n","Epoch 165/200, Loss: 11570.8867\n","Epoch 166/200, Loss: 11474.5254\n","Epoch 167/200, Loss: 11379.7246\n","Epoch 168/200, Loss: 11286.4268\n","Epoch 169/200, Loss: 11194.6035\n","Epoch 170/200, Loss: 11104.2188\n","Epoch 171/200, Loss: 11015.2275\n","Epoch 172/200, Loss: 10927.5889\n","Epoch 173/200, Loss: 10841.2588\n","Epoch 174/200, Loss: 10756.1904\n","Epoch 175/200, Loss: 10672.3398\n","Epoch 176/200, Loss: 10589.6748\n","Epoch 177/200, Loss: 10508.1709\n","Epoch 178/200, Loss: 10427.7969\n","Epoch 179/200, Loss: 10348.5186\n","Epoch 180/200, Loss: 10270.3154\n","Epoch 181/200, Loss: 10193.1895\n","Epoch 182/200, Loss: 10117.1660\n","Epoch 183/200, Loss: 10042.2715\n","Epoch 184/200, Loss: 9968.5381\n","Epoch 185/200, Loss: 9896.0156\n","Epoch 186/200, Loss: 9824.7305\n","Epoch 187/200, Loss: 9754.6768\n","Epoch 188/200, Loss: 9685.8164\n","Epoch 189/200, Loss: 9618.0967\n","Epoch 190/200, Loss: 9551.4570\n","Epoch 191/200, Loss: 9485.8428\n","Epoch 192/200, Loss: 9421.2070\n","Epoch 193/200, Loss: 9357.5127\n","Epoch 194/200, Loss: 9294.7295\n","Epoch 195/200, Loss: 9232.8203\n","Epoch 196/200, Loss: 9171.7578\n","Epoch 197/200, Loss: 9111.5137\n","Epoch 198/200, Loss: 9052.0703\n","Epoch 199/200, Loss: 8993.4043\n","Epoch 200/200, Loss: 8935.5127\n","PCA refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_PCA/matrix_factorization_default_loss_normalizing_flow_PCA_refined_embeddings.pt\n","Epoch 1/200, Loss: 1729079.7500\n","Epoch 2/200, Loss: 1269361.7500\n","Epoch 3/200, Loss: 1074794.2500\n","Epoch 4/200, Loss: 965479.5000\n","Epoch 5/200, Loss: 857789.2500\n","Epoch 6/200, Loss: 751496.5000\n","Epoch 7/200, Loss: 628635.3750\n","Epoch 8/200, Loss: 575189.8125\n","Epoch 9/200, Loss: 540162.9375\n","Epoch 10/200, Loss: 488456.0938\n","Epoch 11/200, Loss: 459694.4688\n","Epoch 12/200, Loss: 449375.0000\n","Epoch 13/200, Loss: 415592.5000\n","Epoch 14/200, Loss: 390766.3125\n","Epoch 15/200, Loss: 368766.2812\n","Epoch 16/200, Loss: 349098.9688\n","Epoch 17/200, Loss: 332017.5938\n","Epoch 18/200, Loss: 316723.5938\n","Epoch 19/200, Loss: 302317.8125\n","Epoch 20/200, Loss: 288276.3750\n","Epoch 21/200, Loss: 274500.5312\n","Epoch 22/200, Loss: 261038.2969\n","Epoch 23/200, Loss: 247624.7188\n","Epoch 24/200, Loss: 233180.6250\n","Epoch 25/200, Loss: 219732.7031\n","Epoch 26/200, Loss: 206750.2500\n","Epoch 27/200, Loss: 197098.7812\n","Epoch 28/200, Loss: 188452.6250\n","Epoch 29/200, Loss: 179932.4844\n","Epoch 30/200, Loss: 171514.6406\n","Epoch 31/200, Loss: 164366.2969\n","Epoch 32/200, Loss: 157405.9219\n","Epoch 33/200, Loss: 150652.1406\n","Epoch 34/200, Loss: 144091.1875\n","Epoch 35/200, Loss: 137732.9062\n","Epoch 36/200, Loss: 131878.7031\n","Epoch 37/200, Loss: 126356.6875\n","Epoch 38/200, Loss: 121095.7188\n","Epoch 39/200, Loss: 116108.7031\n","Epoch 40/200, Loss: 111570.0000\n","Epoch 41/200, Loss: 107226.4844\n","Epoch 42/200, Loss: 103056.3359\n","Epoch 43/200, Loss: 99171.6719\n","Epoch 44/200, Loss: 95610.7344\n","Epoch 45/200, Loss: 92332.8594\n","Epoch 46/200, Loss: 89287.8906\n","Epoch 47/200, Loss: 86436.2188\n","Epoch 48/200, Loss: 83686.1250\n","Epoch 49/200, Loss: 80969.9219\n","Epoch 50/200, Loss: 78286.3906\n","Epoch 51/200, Loss: 75665.6250\n","Epoch 52/200, Loss: 73122.6562\n","Epoch 53/200, Loss: 70650.6875\n","Epoch 54/200, Loss: 68278.5469\n","Epoch 55/200, Loss: 66032.1719\n","Epoch 56/200, Loss: 63903.3477\n","Epoch 57/200, Loss: 61912.3945\n","Epoch 58/200, Loss: 60011.9414\n","Epoch 59/200, Loss: 58107.3555\n","Epoch 60/200, Loss: 56300.2188\n","Epoch 61/200, Loss: 54520.9023\n","Epoch 62/200, Loss: 52711.2383\n","Epoch 63/200, Loss: 50970.0469\n","Epoch 64/200, Loss: 49512.9688\n","Epoch 65/200, Loss: 48227.9961\n","Epoch 66/200, Loss: 47020.4219\n","Epoch 67/200, Loss: 45882.2031\n","Epoch 68/200, Loss: 44753.2812\n","Epoch 69/200, Loss: 43619.6289\n","Epoch 70/200, Loss: 42507.7773\n","Epoch 71/200, Loss: 41419.2734\n","Epoch 72/200, Loss: 40364.6250\n","Epoch 73/200, Loss: 39372.5312\n","Epoch 74/200, Loss: 38441.9258\n","Epoch 75/200, Loss: 37558.5469\n","Epoch 76/200, Loss: 36725.7539\n","Epoch 77/200, Loss: 35931.5156\n","Epoch 78/200, Loss: 35162.0234\n","Epoch 79/200, Loss: 34424.8867\n","Epoch 80/200, Loss: 33711.4453\n","Epoch 81/200, Loss: 33019.6836\n","Epoch 82/200, Loss: 32354.8418\n","Epoch 83/200, Loss: 31709.5078\n","Epoch 84/200, Loss: 31081.6172\n","Epoch 85/200, Loss: 30475.1504\n","Epoch 86/200, Loss: 29885.5156\n","Epoch 87/200, Loss: 29311.1289\n","Epoch 88/200, Loss: 28754.8945\n","Epoch 89/200, Loss: 28213.1367\n","Epoch 90/200, Loss: 27683.5391\n","Epoch 91/200, Loss: 27168.1055\n","Epoch 92/200, Loss: 26666.0156\n","Epoch 93/200, Loss: 26176.6797\n","Epoch 94/200, Loss: 25698.3984\n","Epoch 95/200, Loss: 25222.3105\n","Epoch 96/200, Loss: 24744.2773\n","Epoch 97/200, Loss: 24283.6523\n","Epoch 98/200, Loss: 23864.8809\n","Epoch 99/200, Loss: 23471.5195\n","Epoch 100/200, Loss: 23086.2910\n","Epoch 101/200, Loss: 22707.4648\n","Epoch 102/200, Loss: 22335.4727\n","Epoch 103/200, Loss: 21970.8633\n","Epoch 104/200, Loss: 21613.3086\n","Epoch 105/200, Loss: 21262.4492\n","Epoch 106/200, Loss: 20919.8320\n","Epoch 107/200, Loss: 20588.4941\n","Epoch 108/200, Loss: 20272.1270\n","Epoch 109/200, Loss: 19967.5312\n","Epoch 110/200, Loss: 19666.4375\n","Epoch 111/200, Loss: 19371.8691\n","Epoch 112/200, Loss: 19087.2324\n","Epoch 113/200, Loss: 18811.8652\n","Epoch 114/200, Loss: 18546.0430\n","Epoch 115/200, Loss: 18292.3867\n","Epoch 116/200, Loss: 18051.0723\n","Epoch 117/200, Loss: 17816.7227\n","Epoch 118/200, Loss: 17583.8086\n","Epoch 119/200, Loss: 17351.0234\n","Epoch 120/200, Loss: 17120.6172\n","Epoch 121/200, Loss: 16896.7012\n","Epoch 122/200, Loss: 16682.3027\n","Epoch 123/200, Loss: 16476.0645\n","Epoch 124/200, Loss: 16273.7061\n","Epoch 125/200, Loss: 16073.8555\n","Epoch 126/200, Loss: 15877.9326\n","Epoch 127/200, Loss: 15686.4023\n","Epoch 128/200, Loss: 15498.8008\n","Epoch 129/200, Loss: 15314.8877\n","Epoch 130/200, Loss: 15134.8193\n","Epoch 131/200, Loss: 14958.9785\n","Epoch 132/200, Loss: 14787.5879\n","Epoch 133/200, Loss: 14620.3809\n","Epoch 134/200, Loss: 14456.5010\n","Epoch 135/200, Loss: 14294.8496\n","Epoch 136/200, Loss: 14135.3115\n","Epoch 137/200, Loss: 13979.0918\n","Epoch 138/200, Loss: 13827.0576\n","Epoch 139/200, Loss: 13678.8662\n","Epoch 140/200, Loss: 13533.6719\n","Epoch 141/200, Loss: 13390.9814\n","Epoch 142/200, Loss: 13250.8604\n","Epoch 143/200, Loss: 13113.5928\n","Epoch 144/200, Loss: 12979.1807\n","Epoch 145/200, Loss: 12847.3379\n","Epoch 146/200, Loss: 12718.1055\n","Epoch 147/200, Loss: 12591.7930\n","Epoch 148/200, Loss: 12468.2969\n","Epoch 149/200, Loss: 12347.1289\n","Epoch 150/200, Loss: 12227.9580\n","Epoch 151/200, Loss: 12110.8643\n","Epoch 152/200, Loss: 11996.0781\n","Epoch 153/200, Loss: 11883.5645\n","Epoch 154/200, Loss: 11773.1113\n","Epoch 155/200, Loss: 11664.7139\n","Epoch 156/200, Loss: 11558.4287\n","Epoch 157/200, Loss: 11454.0859\n","Epoch 158/200, Loss: 11351.4814\n","Epoch 159/200, Loss: 11250.6113\n","Epoch 160/200, Loss: 11151.6025\n","Epoch 161/200, Loss: 11054.4502\n","Epoch 162/200, Loss: 10959.0186\n","Epoch 163/200, Loss: 10865.2744\n","Epoch 164/200, Loss: 10773.2354\n","Epoch 165/200, Loss: 10682.7891\n","Epoch 166/200, Loss: 10593.8076\n","Epoch 167/200, Loss: 10506.3105\n","Epoch 168/200, Loss: 10420.3359\n","Epoch 169/200, Loss: 10335.8193\n","Epoch 170/200, Loss: 10252.6895\n","Epoch 171/200, Loss: 10170.9512\n","Epoch 172/200, Loss: 10090.5752\n","Epoch 173/200, Loss: 10011.4775\n","Epoch 174/200, Loss: 9933.6318\n","Epoch 175/200, Loss: 9857.0508\n","Epoch 176/200, Loss: 9781.7051\n","Epoch 177/200, Loss: 9707.5400\n","Epoch 178/200, Loss: 9634.5430\n","Epoch 179/200, Loss: 9562.6924\n","Epoch 180/200, Loss: 9491.9375\n","Epoch 181/200, Loss: 9422.2441\n","Epoch 182/200, Loss: 9353.6221\n","Epoch 183/200, Loss: 9286.0420\n","Epoch 184/200, Loss: 9219.4717\n","Epoch 185/200, Loss: 9153.8994\n","Epoch 186/200, Loss: 9089.3047\n","Epoch 187/200, Loss: 9025.6475\n","Epoch 188/200, Loss: 8962.9111\n","Epoch 189/200, Loss: 8901.0820\n","Epoch 190/200, Loss: 8840.1367\n","Epoch 191/200, Loss: 8780.0547\n","Epoch 192/200, Loss: 8720.8291\n","Epoch 193/200, Loss: 8662.4316\n","Epoch 194/200, Loss: 8604.8438\n","Epoch 195/200, Loss: 8548.0508\n","Epoch 196/200, Loss: 8492.0332\n","Epoch 197/200, Loss: 8436.7705\n","Epoch 198/200, Loss: 8382.2559\n","Epoch 199/200, Loss: 8328.4707\n","Epoch 200/200, Loss: 8275.3965\n","SVD refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_SVD/matrix_factorization_default_loss_normalizing_flow_SVD_refined_embeddings.pt\n","Epoch 1/200, Loss: 3338.7485\n","Epoch 2/200, Loss: 3224.9641\n","Epoch 3/200, Loss: 3159.9268\n","Epoch 4/200, Loss: 3114.6672\n","Epoch 5/200, Loss: 3082.0623\n","Epoch 6/200, Loss: 3058.4419\n","Epoch 7/200, Loss: 3041.1448\n","Epoch 8/200, Loss: 3028.2073\n","Epoch 9/200, Loss: 3018.2388\n","Epoch 10/200, Loss: 3010.2837\n","Epoch 11/200, Loss: 3003.7068\n","Epoch 12/200, Loss: 2998.1006\n","Epoch 13/200, Loss: 2993.2102\n","Epoch 14/200, Loss: 2988.8782\n","Epoch 15/200, Loss: 2985.0061\n","Epoch 16/200, Loss: 2981.5303\n","Epoch 17/200, Loss: 2978.4077\n","Epoch 18/200, Loss: 2975.6042\n","Epoch 19/200, Loss: 2973.0889\n","Epoch 20/200, Loss: 2970.8335\n","Epoch 21/200, Loss: 2968.8108\n","Epoch 22/200, Loss: 2966.9912\n","Epoch 23/200, Loss: 2965.3506\n","Epoch 24/200, Loss: 2963.8628\n","Epoch 25/200, Loss: 2962.5071\n","Epoch 26/200, Loss: 2961.2622\n","Epoch 27/200, Loss: 2960.1118\n","Epoch 28/200, Loss: 2959.0422\n","Epoch 29/200, Loss: 2958.0398\n","Epoch 30/200, Loss: 2957.0974\n","Epoch 31/200, Loss: 2956.2058\n","Epoch 32/200, Loss: 2955.3608\n","Epoch 33/200, Loss: 2954.5566\n","Epoch 34/200, Loss: 2953.7896\n","Epoch 35/200, Loss: 2953.0562\n","Epoch 36/200, Loss: 2952.3540\n","Epoch 37/200, Loss: 2951.6809\n","Epoch 38/200, Loss: 2951.0344\n","Epoch 39/200, Loss: 2950.4111\n","Epoch 40/200, Loss: 2949.8086\n","Epoch 41/200, Loss: 2949.2263\n","Epoch 42/200, Loss: 2948.6609\n","Epoch 43/200, Loss: 2948.1123\n","Epoch 44/200, Loss: 2947.5759\n","Epoch 45/200, Loss: 2947.0527\n","Epoch 46/200, Loss: 2946.5405\n","Epoch 47/200, Loss: 2946.0378\n","Epoch 48/200, Loss: 2945.5442\n","Epoch 49/200, Loss: 2945.0588\n","Epoch 50/200, Loss: 2944.5796\n","Epoch 51/200, Loss: 2944.1072\n","Epoch 52/200, Loss: 2943.6406\n","Epoch 53/200, Loss: 2943.1775\n","Epoch 54/200, Loss: 2942.7190\n","Epoch 55/200, Loss: 2942.2634\n","Epoch 56/200, Loss: 2941.8105\n","Epoch 57/200, Loss: 2941.3596\n","Epoch 58/200, Loss: 2940.9097\n","Epoch 59/200, Loss: 2940.4614\n","Epoch 60/200, Loss: 2940.0139\n","Epoch 61/200, Loss: 2939.5664\n","Epoch 62/200, Loss: 2939.1187\n","Epoch 63/200, Loss: 2938.6714\n","Epoch 64/200, Loss: 2938.2236\n","Epoch 65/200, Loss: 2937.7749\n","Epoch 66/200, Loss: 2937.3264\n","Epoch 67/200, Loss: 2936.8760\n","Epoch 68/200, Loss: 2936.4248\n","Epoch 69/200, Loss: 2935.9724\n","Epoch 70/200, Loss: 2935.5181\n","Epoch 71/200, Loss: 2935.0623\n","Epoch 72/200, Loss: 2934.6050\n","Epoch 73/200, Loss: 2934.1448\n","Epoch 74/200, Loss: 2933.6826\n","Epoch 75/200, Loss: 2933.2178\n","Epoch 76/200, Loss: 2932.7505\n","Epoch 77/200, Loss: 2932.2800\n","Epoch 78/200, Loss: 2931.8066\n","Epoch 79/200, Loss: 2931.3306\n","Epoch 80/200, Loss: 2930.8503\n","Epoch 81/200, Loss: 2930.3669\n","Epoch 82/200, Loss: 2929.8794\n","Epoch 83/200, Loss: 2929.3879\n","Epoch 84/200, Loss: 2928.8936\n","Epoch 85/200, Loss: 2928.3940\n","Epoch 86/200, Loss: 2927.8906\n","Epoch 87/200, Loss: 2927.3818\n","Epoch 88/200, Loss: 2926.8691\n","Epoch 89/200, Loss: 2926.3506\n","Epoch 90/200, Loss: 2925.8281\n","Epoch 91/200, Loss: 2925.2993\n","Epoch 92/200, Loss: 2924.7659\n","Epoch 93/200, Loss: 2924.2266\n","Epoch 94/200, Loss: 2923.6812\n","Epoch 95/200, Loss: 2923.1299\n","Epoch 96/200, Loss: 2922.5732\n","Epoch 97/200, Loss: 2922.0093\n","Epoch 98/200, Loss: 2921.4397\n","Epoch 99/200, Loss: 2920.8638\n","Epoch 100/200, Loss: 2920.2810\n","Epoch 101/200, Loss: 2919.6919\n","Epoch 102/200, Loss: 2919.0950\n","Epoch 103/200, Loss: 2918.4922\n","Epoch 104/200, Loss: 2917.8816\n","Epoch 105/200, Loss: 2917.2639\n","Epoch 106/200, Loss: 2916.6399\n","Epoch 107/200, Loss: 2916.0076\n","Epoch 108/200, Loss: 2915.3687\n","Epoch 109/200, Loss: 2914.7219\n","Epoch 110/200, Loss: 2914.0686\n","Epoch 111/200, Loss: 2913.4082\n","Epoch 112/200, Loss: 2912.7397\n","Epoch 113/200, Loss: 2912.0645\n","Epoch 114/200, Loss: 2911.3826\n","Epoch 115/200, Loss: 2910.6938\n","Epoch 116/200, Loss: 2909.9980\n","Epoch 117/200, Loss: 2909.2957\n","Epoch 118/200, Loss: 2908.5869\n","Epoch 119/200, Loss: 2907.8721\n","Epoch 120/200, Loss: 2907.1504\n","Epoch 121/200, Loss: 2906.4238\n","Epoch 122/200, Loss: 2905.6909\n","Epoch 123/200, Loss: 2904.9536\n","Epoch 124/200, Loss: 2904.2100\n","Epoch 125/200, Loss: 2903.4624\n","Epoch 126/200, Loss: 2902.7095\n","Epoch 127/200, Loss: 2901.9531\n","Epoch 128/200, Loss: 2901.1919\n","Epoch 129/200, Loss: 2900.4272\n","Epoch 130/200, Loss: 2899.6587\n","Epoch 131/200, Loss: 2898.8879\n","Epoch 132/200, Loss: 2898.1133\n","Epoch 133/200, Loss: 2897.3367\n","Epoch 134/200, Loss: 2896.5574\n","Epoch 135/200, Loss: 2895.7764\n","Epoch 136/200, Loss: 2894.9939\n","Epoch 137/200, Loss: 2894.2095\n","Epoch 138/200, Loss: 2893.4243\n","Epoch 139/200, Loss: 2892.6387\n","Epoch 140/200, Loss: 2891.8516\n","Epoch 141/200, Loss: 2891.0645\n","Epoch 142/200, Loss: 2890.2778\n","Epoch 143/200, Loss: 2889.4912\n","Epoch 144/200, Loss: 2888.7056\n","Epoch 145/200, Loss: 2887.9209\n","Epoch 146/200, Loss: 2887.1377\n","Epoch 147/200, Loss: 2886.3555\n","Epoch 148/200, Loss: 2885.5757\n","Epoch 149/200, Loss: 2884.7979\n","Epoch 150/200, Loss: 2884.0222\n","Epoch 151/200, Loss: 2883.2500\n","Epoch 152/200, Loss: 2882.4807\n","Epoch 153/200, Loss: 2881.7148\n","Epoch 154/200, Loss: 2880.9521\n","Epoch 155/200, Loss: 2880.1934\n","Epoch 156/200, Loss: 2879.4382\n","Epoch 157/200, Loss: 2878.6880\n","Epoch 158/200, Loss: 2877.9419\n","Epoch 159/200, Loss: 2877.2012\n","Epoch 160/200, Loss: 2876.4644\n","Epoch 161/200, Loss: 2875.7327\n","Epoch 162/200, Loss: 2875.0054\n","Epoch 163/200, Loss: 2874.2842\n","Epoch 164/200, Loss: 2873.5676\n","Epoch 165/200, Loss: 2872.8564\n","Epoch 166/200, Loss: 2872.1504\n","Epoch 167/200, Loss: 2871.4492\n","Epoch 168/200, Loss: 2870.7532\n","Epoch 169/200, Loss: 2870.0635\n","Epoch 170/200, Loss: 2869.3779\n","Epoch 171/200, Loss: 2868.6980\n","Epoch 172/200, Loss: 2868.0234\n","Epoch 173/200, Loss: 2867.3540\n","Epoch 174/200, Loss: 2866.6899\n","Epoch 175/200, Loss: 2866.0300\n","Epoch 176/200, Loss: 2865.3774\n","Epoch 177/200, Loss: 2864.7358\n","Epoch 178/200, Loss: 2864.1416\n","Epoch 179/200, Loss: 2863.5303\n","Epoch 180/200, Loss: 2862.8442\n","Epoch 181/200, Loss: 2862.2310\n","Epoch 182/200, Loss: 2861.6394\n","Epoch 183/200, Loss: 2861.0059\n","Epoch 184/200, Loss: 2860.3923\n","Epoch 185/200, Loss: 2859.8110\n","Epoch 186/200, Loss: 2859.2126\n","Epoch 187/200, Loss: 2858.6064\n","Epoch 188/200, Loss: 2858.0244\n","Epoch 189/200, Loss: 2857.4514\n","Epoch 190/200, Loss: 2856.8811\n","Epoch 191/200, Loss: 2856.3003\n","Epoch 192/200, Loss: 2855.7144\n","Epoch 193/200, Loss: 2855.1489\n","Epoch 194/200, Loss: 2854.6125\n","Epoch 195/200, Loss: 2854.0796\n","Epoch 196/200, Loss: 2853.5332\n","Epoch 197/200, Loss: 2852.9800\n","Epoch 198/200, Loss: 2852.4272\n","Epoch 199/200, Loss: 2851.8811\n","Epoch 200/200, Loss: 2851.3469\n","NMF refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_NMF/matrix_factorization_default_loss_normalizing_flow_NMF_refined_embeddings.pt\n","Feature extraction and normalizing flow processing complete!\n"]}]}]}