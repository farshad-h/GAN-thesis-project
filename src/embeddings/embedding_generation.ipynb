{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Embedding Generation Notebook\n","\n","## Objective\n","This notebook demonstrates how to use custom encoder models to generate embeddings from the MNIST dataset. These models are implemented in `encoder_models.py`, and the training processes are defined in `encoder_training.py`.\n","\n","## Workflow\n","1. **Load and Preprocess Data**:\n","   - Load the MNIST dataset for testing the embedding generation process.\n","   - Normalize and prepare the data.\n","2. **Model Selection and Training**:\n","   - Train selected encoder models from `encoder_models.py`.\n","   - Generate embeddings from the bottleneck layer.\n","3. **Feature Extraction**:\n","   - Generate embeddings using matrix factorization (PCA, SVD, NMF) and SIFT.\n","4. **Save Embeddings**:\n","   - Save all embeddings and trained models for reuse.\n","\n","## Models and Methods\n","### Supported Models\n","The following encoder models are available for training and embedding generation. Each model is implemented in `encoder_models.py`:\n","- **Encoder Models**:\n","  - BasicAutoencoder, IntermediateAutoencoder, AdvancedAutoencoder, EnhancedAutoencoder.\n","  - BasicVAE, VAEWithFCDecoder, ImprovedVAE, FlexibleVAE.\n","- **Feature Extraction**:\n","  - PCA, SVD, NMF.\n","  - SIFT, Kernel PCA.\n","\n","#### **Autoencoders**\n","1. **BasicAutoencoder**:\n","   - A simple autoencoder with:\n","     - **Encoder**: Two convolutional layers followed by max-pooling.\n","     - **Decoder**: Two transposed convolutional layers to reconstruct the input.\n","   - Designed for grayscale datasets like MNIST.\n","   - Suitable for basic dimensionality reduction and reconstruction tasks.\n","\n","2. **IntermediateAutoencoder**:\n","   - A deeper autoencoder with:\n","     - **Batch Normalization** for improved stability.\n","     - Additional feature maps for a more expressive latent space.\n","   - Designed for moderately complex embedding tasks requiring better feature extraction.\n","\n","3. **AdvancedAutoencoder**:\n","   - A sophisticated autoencoder with:\n","     - **Skip Connections** to improve gradient flow and reconstruction accuracy.\n","     - **LeakyReLU Activations** and Batch Normalization for robust performance.\n","   - Suitable for high-dimensional or structured data requiring detailed reconstruction.\n","\n","4. **EnhancedAutoencoder**:\n","   - A deeper autoencoder with:\n","     - Additional convolutional layers in the encoder.\n","     - Transposed convolutional layers in the decoder.\n","     - LeakyReLU activations and Batch Normalization for better embedding representation.\n","   - Designed for datasets requiring intricate reconstructions under noisy conditions.\n","\n","#### **Variational Autoencoders (VAEs)**\n","5. **BasicVAE**:\n","   - A simple VAE with:\n","     - **Encoder**: Two convolutional layers and a fully connected layer to parameterize the latent space.\n","     - **Decoder**: Fully connected and transposed convolution layers to reconstruct input images.\n","   - Suitable for generative tasks with simple latent spaces.\n","\n","6. **VAEWithFCDecoder**:\n","   - A VAE with a fully connected decoder for enhanced latent-to-feature mapping.\n","   - Features:\n","     - **Encoder**: Convolutional layers with Batch Normalization.\n","     - **Decoder**: A combination of fully connected and transposed convolutional layers.\n","\n","7. **ImprovedVAE**:\n","   - An advanced VAE with:\n","     - A bottleneck layer for enhanced feature extraction.\n","     - Transposed convolutions for smooth reconstructions.\n","     - KL divergence loss for latent space regularization.\n","   - Designed for datasets requiring expressive latent representations.\n","\n","8. **FlexibleVAE**:\n","   - A flexible VAE that supports dynamic input shapes and optional projection heads for contrastive learning.\n","   - Suitable for embedding tasks with varying input dimensions.\n","\n","9. **ImprovedFlexibleVAE**:\n","   - Combines convolutional and fully connected layers in the encoder.\n","   - Uses transposed convolutions in the decoder for better reconstruction.\n","   - Optional **Projection Head** for self-supervised contrastive learning tasks.\n","\n","#### **Denoising Autoencoders**\n","10. **DenoisingAutoencoder**:\n","    - A denoising autoencoder with:\n","      - **Encoder**: Convolutional layers for feature extraction.\n","      - **Decoder**: Transposed convolutional layers for reconstruction.\n","      - Optional **Projection Head** for contrastive learning.\n","    - Supports two architectures:\n","      - **Basic**: Simpler structure for standard denoising tasks.\n","      - **Strong**: Deeper architecture for challenging noisy datasets.\n","\n","#### **Feature Extraction and Normalizing Flow Models**\n","11. **Matrix Factorization**:\n","    - Embeddings generated using PCA, SVD, and NMF.\n","    - Useful for dimensionality reduction and compact representations.\n","\n","12. **SIFT (Scale-Invariant Feature Transform)**:\n","    - Extracts scale-invariant features from images.\n","    - Pads feature descriptors to ensure consistent dimensionality.\n","\n","13. **Kernel PCA**:\n","    - Nonlinear dimensionality reduction using Kernel PCA with adjustable kernels.\n","\n","14. **Normalizing Flow Models**:\n","    - Transforms embeddings into a latent space using invertible transformations.\n","    - Useful for embedding refinement and generative tasks.\n","\n","\n","**Training**:\n","   - Each model is trained using the corresponding training loop defined in `encoder_training.py`.\n","   - Training includes support for reconstruction loss, KL divergence (for VAE), and optional noise injection.\n","**Embedding Generation**:\n","   - Once the models are trained, embeddings are generated for the MNIST dataset.\n","   - Encodings from the bottleneck layer are extracted for downstream tasks.\n","**Results Storage**:\n","   - Save trained models to `.pth` files.\n","   - Save generated embeddings to `.pt` files for reuse in downstream applications.\n","\n","## Supported Features\n","- **Flexible Model Selection**:\n","  - Choose specific models to train and generate embeddings for, bypassing others if needed.\n","- **Custom Configuration**:\n","  - Easily modify parameters like the bottleneck size (`code_dim`), number of training epochs, and learning rates.\n","\n","## Outputs\n","- Trained models saved as `.pth` files.\n","- Generated embeddings saved as `.pt` files in a structured directory (`./embeddings`).\n","\n","## Notes\n","This notebook is designed for flexibility and reusability. You can:\n","- Add new encoder models in `encoder_models.py`.\n","- Customize training loops in `encoder_training.py`.\n","- Modify this notebook to train specific models or generate embeddings for specific datasets.\n"],"metadata":{"id":"VUQM0eOdRPMH"}},{"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","# Mount Google Drive and set repository path\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Repository path (adjust if needed)\n","repo_path = \"/content/drive/MyDrive/GAN-thesis-project\"\n","\n","# Add repository path to sys.path for module imports\n","if repo_path not in sys.path:\n","    sys.path.append(repo_path)\n","\n","# Change working directory to the repository\n","os.chdir(repo_path)\n","\n","# Verify the working directory\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","# Configuration\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLvDZtTSZFUb","executionInfo":{"status":"ok","timestamp":1737805898329,"user_tz":-210,"elapsed":2377,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"89a58ab2-4d19-4ded-869a-f2e9bb31aa4a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Current working directory: /content/drive/MyDrive/GAN-thesis-project\n","Using device: cpu\n"]}]},{"cell_type":"code","source":["import inspect\n","\n","# Import the entire modules\n","import src.data_utils as data_utils\n","import src.cl_loss_function as cl_loss\n","import src.losses as losses\n","import src.embeddings.encoder_models as encoder_models\n","import src.embeddings.encoder_training as encoder_training\n","\n","# Function to list functions and classes in a module\n","def list_functions_and_classes(module):\n","    members = inspect.getmembers(module)\n","    functions = [name for name, obj in members if inspect.isfunction(obj)]\n","    classes = [name for name, obj in members if inspect.isclass(obj)]\n","    return functions, classes\n","\n","# Function to print functions and classes in a readable format\n","def print_functions_and_classes(module_name, module):\n","    functions, classes = list_functions_and_classes(module)\n","    print(f\"Module: {module_name}\")\n","    print(\"  Functions:\")\n","    for func in functions:\n","        print(f\"    - {func}\")\n","    print(\"  Classes:\")\n","    for cls in classes:\n","        print(f\"    - {cls}\")\n","    print()  # Add a blank line for separation\n","\n","# Print functions and classes for each module\n","print_functions_and_classes(\"src.data_utils\", data_utils)\n","print_functions_and_classes(\"src.cl_loss_function\", cl_loss)\n","print_functions_and_classes(\"src.losses\", losses)\n","print_functions_and_classes(\"src.embeddings.encoder_models\", encoder_models)\n","print_functions_and_classes(\"src.embeddings.encoder_training\", encoder_training)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jah7kWk-SpUb","executionInfo":{"status":"ok","timestamp":1737805934549,"user_tz":-210,"elapsed":307,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"756db95a-6824-4b6b-e7f0-20d6766bee3a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Module: src.data_utils\n","  Functions:\n","    - analyze_embeddings\n","    - analyze_embeddings_v2\n","    - create_dataloader\n","    - create_embedding_loaders\n","    - kurtosis\n","    - load_data\n","    - load_embeddings\n","    - load_mnist_data\n","    - pdist\n","    - preprocess_images\n","    - save_embeddings\n","    - skew\n","    - split_dataset\n","    - train_test_split\n","    - visualize_embeddings\n","  Classes:\n","    - DataLoader\n","    - LocalOutlierFactor\n","    - TensorDataset\n","\n","Module: src.cl_loss_function\n","  Functions:\n","    - augment\n","    - compute_nt_xent_loss_with_augmentation\n","    - compute_triplet_loss_with_augmentation\n","    - contrastive_loss\n","    - hflip\n","    - info_nce_loss\n","    - resize\n","  Classes:\n","    - ContrastiveHead\n","    - DataLoader\n","    - NTXentLoss\n","    - PCA\n","    - TensorDataset\n","    - TripletLoss\n","    - VicRegLoss\n","\n","Module: src.losses\n","  Functions:\n","    - add_noise\n","    - cyclical_beta_schedule\n","    - linear_beta_schedule\n","    - loss_function_dae_ssim\n","    - vae_loss\n","    - vae_ssim_loss\n","  Classes:\n","\n","Module: src.embeddings.encoder_models\n","  Functions:\n","    - apply_dimensionality_reduction\n","    - apply_sift\n","    - init_weights\n","    - log_prob\n","    - process_feature_extraction\n","    - process_matrix_factorization\n","    - refine_embeddings_NF\n","    - train_nf_model\n","  Classes:\n","    - AdvancedAutoencoder\n","    - BasicAutoencoder\n","    - BasicVAE\n","    - DataLoader\n","    - DenoisingAutoencoder\n","    - EnhancedAutoencoder\n","    - FlexibleVAE\n","    - FlowLayer\n","    - ImprovedFlexibleVAE\n","    - ImprovedVAE\n","    - IntermediateAutoencoder\n","    - KernelPCA\n","    - MinMaxScaler\n","    - NMF\n","    - NormalizingFlowModel\n","    - PCA\n","    - ProjectionHead\n","    - SimCLR\n","    - StandardScaler\n","    - TensorDataset\n","    - TruncatedSVD\n","    - tqdm\n","\n","Module: src.embeddings.encoder_training\n","  Functions:\n","    - add_noise\n","    - ssim\n","    - train_autoencoder\n","    - train_dae\n","    - train_simclr\n","    - train_vae\n","  Classes:\n","    - DataLoader\n","    - MinMaxScaler\n","    - StandardScaler\n","    - TensorDataset\n","    - ToTensor\n","    - tqdm\n","\n"]}]},{"cell_type":"code","source":["# Load and Preprocess MNIST Data\n","fraction = 1  # Fraction of the dataset to use\n","batch_size = 64\n","\n","# Load Combined MNIST Dataset (Train + Test)\n","mnist_loader = data_utils.load_mnist_data(fraction=fraction, batch_size=batch_size, shuffle=True)\n","\n","# Inspect Combined Dataset\n","for batch in mnist_loader:\n","    images, labels = batch\n","    print(\"Batch Shape:\", images.shape, labels.shape)\n","    break\n","\n","# Visualize Original Images\n","n = 20\n","sample_indices = np.random.choice(len(mnist_loader.dataset), n, replace=False)\n","sampled_images = mnist_loader.dataset.tensors[0][sample_indices].numpy()\n","sampled_images = (sampled_images * 127.5 + 127.5).astype(np.uint8).squeeze()  # Denormalize for display\n","\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    ax = plt.subplot(2, 10, i + 1)\n","    plt.imshow(sampled_images[i], cmap=\"gray\")\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()\n"],"metadata":{"id":"9ByfVYCjeT7P","colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"status":"ok","timestamp":1737805940249,"user_tz":-210,"elapsed":1502,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"71616d83-21b6-4b28-c56c-560e8211e0a9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (70000, 1, 28, 28) (70000,)\n","Batch Shape: torch.Size([64, 1, 28, 28]) torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 2000x400 with 20 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["from typing import Callable, Optional\n","\n","def train_autoencoder(\n","    model: nn.Module,\n","    data_loader: DataLoader,\n","    loss_fn: Callable,\n","    optimizer: optim.Optimizer,\n","    epochs: int = 10,\n","    device: str = \"cpu\",\n","    noise_factor: float = 0.0,\n","    scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,\n","    contrastive_loss_fn: Optional[Callable] = None,\n","    temperature: float = 0.5,\n","    triplet_data: bool = False,\n","    augment_fn: Optional[Callable] = None,\n","):\n","    \"\"\"\n","    Unified training function for autoencoders with support for:\n","    - Reconstruction loss\n","    - Contrastive loss (e.g., NT-Xent, InfoNCE)\n","    - Triplet loss\n","    - Noise injection (for denoising autoencoders)\n","    - Data augmentation\n","\n","    Args:\n","        model (nn.Module): The autoencoder model.\n","        data_loader (DataLoader): DataLoader for training data.\n","        loss_fn (Callable): Primary loss function (e.g., reconstruction loss).\n","        optimizer (optim.Optimizer): Optimizer for the model.\n","        epochs (int): Number of epochs to train.\n","        device (str): Device to train on ('cpu' or 'cuda').\n","        noise_factor (float): Factor for adding noise to input images (denoising autoencoder).\n","        scheduler (Optional[optim.lr_scheduler._LRScheduler]): Learning rate scheduler.\n","        contrastive_loss_fn (Optional[Callable]): Contrastive loss function (e.g., NT-Xent, triplet loss).\n","        temperature (float): Temperature parameter for NT-Xent loss.\n","        triplet_data (bool): Whether the data_loader provides triplets (anchor, positive, negative).\n","        augment_fn (Optional[Callable]): Augmentation function for contrastive learning.\n","\n","    Returns:\n","        None: Prints loss values for each epoch.\n","    \"\"\"\n","    model.to(device).train()\n","\n","    # Initialize early stopping\n","    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for batch in data_loader:\n","            # Prepare data based on whether it's triplet data or not\n","            if triplet_data:\n","                anchor, positive, negative = batch\n","                anchor, positive, negative = (\n","                    anchor.to(device).float(),\n","                    positive.to(device).float(),\n","                    negative.to(device).float(),\n","                )\n","                images = anchor  # Use anchor as the primary input for reconstruction\n","            else:\n","                images, _ = batch\n","                images = images.to(device).float()\n","\n","            # Add noise if specified\n","            if noise_factor > 0:\n","                noisy_images = images + noise_factor * torch.randn_like(images)\n","                noisy_images = torch.clamp(noisy_images, 0.0, 1.0)\n","                encoded, decoded = model(noisy_images)\n","            else:\n","                encoded, decoded = model(images)\n","\n","            # Compute reconstruction loss\n","            reconstruction_loss = loss_fn(decoded, images)\n","\n","            # Compute contrastive loss if specified\n","            contrastive_loss_value = 0\n","            if contrastive_loss_fn is not None:\n","                if triplet_data:\n","                    # Triplet loss\n","                    positive_encoded, _ = model(positive)\n","                    negative_encoded, _ = model(negative)\n","                    contrastive_loss_value = contrastive_loss_fn(encoded, positive_encoded, negative_encoded)\n","                else:\n","                    # NT-Xent or other contrastive loss\n","                    if augment_fn:\n","                        augmented_1 = augment_fn(images)\n","                        augmented_2 = augment_fn(images)\n","                        z1, _ = model(augmented_1)\n","                        z2, _ = model(augmented_2)\n","                    else:\n","                        z1, z2 = encoded, encoded  # Use the same embeddings if no augmentation\n","                    contrastive_loss_value = contrastive_loss_fn(z1, z2, temperature)\n","\n","            # Total loss\n","            total_loss_value = reconstruction_loss + contrastive_loss_value\n","\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            total_loss_value.backward()\n","            optimizer.step()\n","\n","            total_loss += total_loss_value.item()\n","\n","        # Step the scheduler if provided\n","        if scheduler:\n","            scheduler.step()\n","\n","        # Compute average epoch loss\n","        avg_loss = total_loss / len(data_loader)\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_loss:.4f}\")\n","\n","        # Check for early stopping\n","        early_stopping(avg_loss)\n","        if early_stopping.early_stop:\n","            print(f\"Early stopping triggered at epoch {epoch + 1}.\")\n","            break\n","\n","class EarlyStopping:\n","    \"\"\"\n","    Early stopping to stop training when the loss does not improve after a specified number of epochs (patience).\n","    \"\"\"\n","    def __init__(self, patience=5, min_delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): Number of epochs to wait for improvement before stopping.\n","            min_delta (float): Minimum change in loss to qualify as an improvement.\n","        \"\"\"\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_loss = float('inf')\n","        self.early_stop = False\n","\n","    def __call__(self, current_loss):\n","        \"\"\"\n","        Check if training should stop.\n","\n","        Args:\n","            current_loss (float): Current epoch's loss.\n","        \"\"\"\n","        if current_loss < self.best_loss - self.min_delta:\n","            self.best_loss = current_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True"],"metadata":{"id":"qx6NgMUJPLv5","executionInfo":{"status":"ok","timestamp":1737805940605,"user_tz":-210,"elapsed":3,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"vae\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"dae\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels"],"metadata":{"id":"lC_8uAUF7t0r","executionInfo":{"status":"ok","timestamp":1737805943692,"user_tz":-210,"elapsed":328,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class EnhancedAutoencoder(nn.Module):\n","    \"\"\"\n","    A deep autoencoder with advanced reconstruction capabilities for MNIST.\n","\n","    Features:\n","    - Deeper architecture with additional convolutional and transposed convolutional layers.\n","    - Utilizes Batch Normalization and LeakyReLU activations.\n","    - Capable of learning highly expressive embeddings.\n","\n","    Designed for datasets requiring intricate reconstructions.\n","    \"\"\"\n","    def __init__(self, code_dim):\n","        super(EnhancedAutoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: (1, 28, 28)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","            nn.MaxPool2d(2, 2),  # Output: (32, 14, 14)\n","\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 14, 14)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d(2, 2),  # Output: (64, 7, 7)\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 7, 7)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d(2, 2)  # Output: (128, 3, 3)\n","        )\n","        self.fc_encoder = nn.Linear(128 * 3 * 3, code_dim)  # Flatten to embedding dimension\n","\n","        # Decoder\n","        self.fc_decoder = nn.Linear(code_dim, 128 * 3 * 3)  # Unflatten to feature map\n","        self.decoder = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Unflatten(1, (128, 3, 3)),  # Reshape to (batch_size, 128, 3, 3)\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (64, 7, 7)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=0, output_padding=1),  # Output: (32, 14, 14)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","\n","            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (16, 28, 28)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(16),\n","\n","            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=1),  # Output: (1, 28, 28)\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Encoder\n","        encoded = self.encoder(x)\n","        encoded = encoded.view(batch_size, -1)  # Flatten to (batch_size, 128 * 3 * 3)\n","        encoded = self.fc_encoder(encoded)  # Output: (batch_size, code_dim)\n","\n","        # Decoder\n","        decoded = self.fc_decoder(encoded)\n","        decoded = self.decoder(decoded)  # Output: (batch_size, 1, 28, 28)\n","\n","        return encoded, decoded"],"metadata":{"id":"a48gCSfl99zM","executionInfo":{"status":"ok","timestamp":1737805945375,"user_tz":-210,"elapsed":282,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = EnhancedAutoencoder(code_dim=50).to(device)\n","dummy_input = torch.randn(1, 1, 28, 28).to(device)\n","encoded, decoded = model(dummy_input)\n","print(f\"Decoded shape: {decoded.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2NQsq0YAZFN","executionInfo":{"status":"ok","timestamp":1737805947294,"user_tz":-210,"elapsed":316,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"98b618e8-ee15-436a-a170-cd9e403fdc43"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoded shape: torch.Size([1, 1, 28, 28])\n"]}]},{"cell_type":"code","source":["class NTXentLoss(nn.Module):\n","    \"\"\"\n","    NT-Xent (Normalized Temperature-scaled Cross Entropy) Loss for contrastive learning.\n","\n","    Args:\n","        temperature (float): Scaling factor for similarity scores.\n","    \"\"\"\n","    def __init__(self, temperature=0.5):\n","        super(NTXentLoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, z_i, z_j):\n","        \"\"\"\n","        Compute the NT-Xent loss between two sets of embeddings.\n","\n","        Args:\n","            z_i (torch.Tensor): First set of embeddings.\n","            z_j (torch.Tensor): Second set of embeddings.\n","\n","        Returns:\n","            torch.Tensor: Computed NT-Xent loss.\n","        \"\"\"\n","        batch_size = z_i.size(0)\n","\n","        # Normalize embeddings\n","        z_i = F.normalize(z_i, dim=1)\n","        z_j = F.normalize(z_j, dim=1)\n","\n","        # Concatenate embeddings\n","        z = torch.cat([z_i, z_j], dim=0)\n","\n","        # Compute similarity matrix\n","        similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","\n","        # Mask for positives and negatives\n","        mask = torch.eye(2 * batch_size, device=z.device).bool()  # Mask for self-similarities\n","        positives = torch.cat([\n","            torch.diag(similarity_matrix, batch_size),  # Similarity between z_i and z_j\n","            torch.diag(similarity_matrix, -batch_size)  # Similarity between z_j and z_i\n","        ])\n","\n","        # Mask out self-similarities and positives\n","        negatives = similarity_matrix[~mask].view(2 * batch_size, -1)\n","\n","        # Compute NT-Xent loss\n","        numerator = torch.exp(positives)\n","        denominator = torch.sum(torch.exp(negatives), dim=-1)\n","\n","        # Avoid numerical instability by using log-sum-exp trick\n","        loss = -torch.log(numerator / denominator).mean()\n","\n","        return loss"],"metadata":{"id":"Y8LPTX8iKf1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Define Configuration\n","# ------------------------------\n","\n","# Configuration\n","config = {\n","    \"model_type\": \"dae\",  # Options: \"autoencoder\", \"vae\", \"dae\"\n","    \"model_name\": \"DenoisingAutoencoder\",  # Options: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\", \"BasicVAE\", \"ImprovedVAE\", \"FlexibleVAE\", \"ImprovedFlexibleVAE\", \"DenoisingAutoencoder\"\n","    \"code_dim\": 50,  # Dimensionality of the embedding\n","    \"loss_type\": \"mse\",  # Options: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","    \"noise_factor\": 0.1,  # Noise factor for denoising autoencoders\n","    \"temperature\": 0.5,  # Temperature parameter for NT-Xent loss\n","    \"margin\": 1.0,  # Margin for Triplet Loss\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-3,\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","    \"save_best\": True,  # Whether to save the best model\n","    \"save_path\": \"best_model.pth\",  # Path to save the best model\n","    \"beta\": 1.0,  # Weight for KL divergence (VAE only)\n","    \"alpha\": 0.5,  # Weight for contrastive or triplet loss\n","    \"fraction\": 0.01,  # Fraction of the dataset to use\n","    \"projection_dim\": None,  # Optional projection head dimension for VAEs\n","    \"strong_architecture\": False,  # Whether to use a deeper architecture for DenoisingAutoencoder\n","    \"input_shape\": (1, 28, 28),  # Input shape for FlexibleVAE and ImprovedFlexibleVAE\n","}\n","\n","# ------------------------------\n","# Step 2: Load and Preprocess Data\n","# ------------------------------\n","\n","# Load Combined MNIST Dataset (Train + Test)\n","mnist_loader = data_utils.load_mnist_data(fraction=config[\"fraction\"], batch_size=config[\"batch_size\"], shuffle=True)\n","\n","# Inspect Combined Dataset\n","for batch in mnist_loader:\n","    images, labels = batch\n","    print(\"Batch Shape:\", images.shape, labels.shape)\n","    break\n","\n","# Visualize Original Images\n","n = 20\n","sample_indices = np.random.choice(len(mnist_loader.dataset), n, replace=False)\n","sampled_images = mnist_loader.dataset.tensors[0][sample_indices].numpy()\n","sampled_images = (sampled_images * 127.5 + 127.5).astype(np.uint8).squeeze()  # Denormalize for display\n","\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    ax = plt.subplot(2, 10, i + 1)\n","    plt.imshow(sampled_images[i], cmap=\"gray\")\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()\n","\n","# ------------------------------\n","# Step 3: Initialize Model, Loss, and Optimizer\n","# ------------------------------\n","\n","# Initialize the model\n","model_classes = {\n","    \"BasicAutoencoder\": encoder_models.BasicAutoencoder,\n","    \"IntermediateAutoencoder\": encoder_models.IntermediateAutoencoder,\n","    \"AdvancedAutoencoder\": encoder_models.AdvancedAutoencoder,\n","    \"EnhancedAutoencoder\": encoder_models.EnhancedAutoencoder,\n","    \"BasicVAE\": encoder_models.BasicVAE,\n","    \"ImprovedVAE\": encoder_models.ImprovedVAE,\n","    \"FlexibleVAE\": encoder_models.FlexibleVAE,\n","    \"ImprovedFlexibleVAE\": encoder_models.ImprovedFlexibleVAE,\n","    \"DenoisingAutoencoder\": encoder_models.DenoisingAutoencoder,\n","}\n","\n","# Initialize model with appropriate arguments\n","if config[\"model_name\"] in [\"FlexibleVAE\", \"ImprovedFlexibleVAE\"]:\n","    model = model_classes[config[\"model_name\"]](\n","        input_shape=config[\"input_shape\"],\n","        code_dim=config[\"code_dim\"],\n","        projection_dim=config[\"projection_dim\"]\n","    ).to(config[\"device\"])\n","elif config[\"model_name\"] == \"DenoisingAutoencoder\":\n","    model = model_classes[config[\"model_name\"]](\n","        code_dim=config[\"code_dim\"],\n","        projection_dim=config[\"projection_dim\"],\n","        strong_architecture=config[\"strong_architecture\"]\n","    ).to(config[\"device\"])\n","else:\n","    model = model_classes[config[\"model_name\"]](\n","        code_dim=config[\"code_dim\"]\n","    ).to(config[\"device\"])\n","\n","# # Define the loss function\n","# loss_functions = {\n","#     \"mse\": nn.MSELoss(),  # Reconstruction loss\n","#     \"vae_loss\": losses.vae_loss,  # VAE loss\n","#     \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","#     \"ntxent\": cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","#     \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","# }\n","# criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the loss function\n","if config[\"model_type\"] == \"vae\":\n","    criterion = losses.vae_loss  # Use VAE loss for VAEs\n","else:\n","    loss_functions = {\n","        \"mse\": nn.MSELoss(),  # Reconstruction loss\n","        \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","        \"ntxent\": cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","        \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","    }\n","    criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","\n","# Define scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 4: Train the Model\n","# ------------------------------\n","\n","if config[\"model_type\"] == \"autoencoder\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    encoder_training.train_autoencoder(\n","        model=model,\n","        data_loader=mnist_loader,\n","        loss_fn=criterion,\n","        optimizer=optimizer,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        noise_factor=config[\"noise_factor\"],\n","        scheduler=scheduler,\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\", \"triplet\"] else None,\n","        triplet_data=(config[\"loss_type\"] == \"triplet\"),\n","        augment_fn=cl_loss.augment if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","    )\n","\n","elif config[\"model_type\"] == \"vae\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    encoder_training.train_vae(\n","        vae=model,\n","        train_loader=mnist_loader,\n","        optimizer=optimizer,\n","        loss_fn=criterion,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        val_loader=None,  # No validation loader for simplicity\n","        scheduler=scheduler,\n","        save_best=config[\"save_best\"],\n","        save_path=config[\"save_path\"],\n","        beta=config[\"beta\"],\n","        alpha=config[\"alpha\"],\n","        temperature=config[\"temperature\"],\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","    )\n","\n","elif config[\"model_type\"] == \"dae\":\n","    print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","    encoder_training.train_dae(\n","        dae=model,\n","        train_loader=mnist_loader,\n","        optimizer=optimizer,\n","        loss_fn=criterion,\n","        epochs=config[\"epochs\"],\n","        device=config[\"device\"],\n","        val_loader=None,  # No validation loader for simplicity\n","        scheduler=scheduler,\n","        save_best=config[\"save_best\"],\n","        save_path=config[\"save_path\"],\n","        noise_factor=config[\"noise_factor\"],\n","        alpha=config[\"alpha\"],\n","        temperature=config[\"temperature\"],\n","        contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","        triplet_loss_fn=criterion if config[\"loss_type\"] == \"triplet\" else None,\n","        ssim_func=losses.ssim if config[\"loss_type\"] == \"ssim\" else None,\n","    )\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Generate embeddings\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    embedding_type=config[\"model_type\"],\n","    data_loader=mnist_loader,\n","    device=config[\"device\"],\n",")\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{config['model_type']}_{config['model_name']}_{config['loss_type']}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{config['model_name']}_{config['loss_type']}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n","\n","# ------------------------------\n","# Step 6: Visualize Embeddings\n","# ------------------------------\n","\n","# Visualize embeddings\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":643},"id":"zAKUO7472Gn7","executionInfo":{"status":"error","timestamp":1737806004290,"user_tz":-210,"elapsed":1432,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"15917a6c-2409-4a29-c8fd-585f2a059db8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (700, 1, 28, 28) (700,)\n","Batch Shape: torch.Size([64, 1, 28, 28]) torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 2000x400 with 20 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training DenoisingAutoencoder with mse loss...\n"]},{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5ff438b1cf3e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dae\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {config['model_name']} with {config['loss_type']} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     encoder_training.train_dae(\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mdae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/embeddings/encoder_training.py\u001b[0m in \u001b[0;36mtrain_dae\u001b[0;34m(dae, train_loader, optimizer, loss_fn, epochs, device, val_loader, scheduler, save_best, save_path, noise_factor, alpha, temperature, contrastive_loss_fn, triplet_loss_fn, ssim_func)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Compute reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Define Configuration\n","# ------------------------------\n","\n","# Configuration\n","config = {\n","    \"model_name\": \"IntermediateAutoencoder\",  # Options: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"\n","    \"code_dim\": 50,  # Dimensionality of the embedding\n","    \"embedding_type\": \"autoencoders\",  # Options: \"autoencoders\", \"VAEs\", \"denoising_autoencoders\"\n","    \"loss_type\": \"mse\",  # Options: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","    \"noise_factor\": 0.0,  # Noise factor for denoising autoencoders\n","    \"temperature\": 0.5,  # Temperature parameter for NT-Xent loss\n","    \"margin\": 1.0,  # Margin for Triplet Loss\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-3,\n","    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","}\n","\n","# ------------------------------\n","# Step 2: Load and Preprocess Data\n","# ------------------------------\n","\n","# Load MNIST data\n","train_data, test_data = data_utils.load_mnist_data()\n","\n","# Preprocess images\n","train_data = data_utils.preprocess_images(train_data)\n","test_data = data_utils.preprocess_images(test_data)\n","\n","# Create DataLoader\n","train_loader = data_utils.create_dataloader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n","test_loader = data_utils.create_dataloader(test_data, batch_size=config[\"batch_size\"], shuffle=False)\n","\n","# ------------------------------\n","# Step 3: Initialize Model, Loss, and Optimizer\n","# ------------------------------\n","\n","# Initialize the model\n","model_classes = {\n","    \"BasicAutoencoder\": encoder_models.BasicAutoencoder,\n","    \"IntermediateAutoencoder\": encoder_models.IntermediateAutoencoder,\n","    \"AdvancedAutoencoder\": encoder_models.AdvancedAutoencoder,\n","    \"EnhancedAutoencoder\": encoder_models.EnhancedAutoencoder,\n","}\n","model = model_classes[config[\"model_name\"]](code_dim=config[\"code_dim\"]).to(config[\"device\"])\n","\n","# Define the loss function\n","loss_functions = {\n","    \"mse\": nn.MSELoss(),  # Reconstruction loss\n","    \"vicreg\": cl_loss.VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1),  # VicReg loss\n","    \"ntxent\": cl_loss.NTXentLoss(temperature=config[\"temperature\"]),  # NT-Xent loss\n","    \"triplet\": cl_loss.TripletLoss(margin=config[\"margin\"]),  # Triplet loss\n","}\n","criterion = loss_functions[config[\"loss_type\"]]\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n","\n","# Define scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 4: Train the Model\n","# ------------------------------\n","\n","print(f\"Training {config['model_name']} with {config['loss_type']} loss...\")\n","encoder_training.train_autoencoder(\n","    model=model,\n","    data_loader=train_loader,\n","    loss_fn=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    epochs=config[\"epochs\"],\n","    device=config[\"device\"],\n","    noise_factor=config[\"noise_factor\"] if config[\"embedding_type\"] == \"denoising_autoencoders\" else 0.0,\n","    contrastive_loss_fn=criterion if config[\"loss_type\"] in [\"vicreg\", \"ntxent\", \"triplet\"] else None,\n","    augment_fn=cl_loss.augment if config[\"loss_type\"] in [\"vicreg\", \"ntxent\"] else None,\n","    triplet_data=(config[\"loss_type\"] == \"triplet\"),\n",")\n","\n","# ------------------------------\n","# Step 5: Generate and Save Embeddings\n","# ------------------------------\n","\n","# Generate embeddings\n","embeddings, labels = encoder_training.generate_embeddings(\n","    model=model,\n","    data_loader=test_loader,\n","    device=config[\"device\"],\n",")\n","\n","# Save embeddings\n","data_utils.save_embeddings(\n","    embeddings=embeddings,\n","    labels=labels,\n","    base_dir=\"./saved_embeddings\",\n","    model_name=config[\"model_name\"],\n","    loss_type=config[\"loss_type\"],\n","    embedding_type=config[\"embedding_type\"],\n","    save_format=\"pt\",  # Options: \"pt\" (PyTorch) or \"npy\" (NumPy)\n",")\n","\n","# Save the model\n","model_file = os.path.join(\"./saved_embeddings\", f\"{config['model_name']}_{config['loss_type']}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n","\n","# ------------------------------\n","# Step 6: Visualize Embeddings\n","# ------------------------------\n","\n","# Visualize embeddings\n","data_utils.visualize_embeddings(embeddings, labels, title=f\"{config['model_name']} Embeddings\")"],"metadata":{"id":"3tEV_wCe0DSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EnhancedAutoencoder(nn.Module):\n","    \"\"\"\n","    A deep autoencoder with advanced reconstruction capabilities.\n","\n","    Features:\n","    - Deeper architecture with additional convolutional and transposed convolutional layers.\n","    - Utilizes Batch Normalization and LeakyReLU activations.\n","    - Capable of learning highly expressive embeddings.\n","\n","    Designed for datasets requiring intricate reconstructions.\n","    \"\"\"\n","    def __init__(self, code_dim):\n","        super(EnhancedAutoencoder, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: (1, 28, 28)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","            nn.MaxPool2d(2, 2),  # Output: (32, 14, 14)\n","\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 14, 14)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d(2, 2),  # Output: (64, 7, 7)\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Output: (128, 7, 7)\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d(2, 2)  # Output: (128, 3, 3)\n","        )\n","\n","        self.fc_encoder = nn.Linear(128 * 3 * 3, code_dim)\n","\n","        # Decoder with learned upsampling\n","        self.fc_decoder = nn.Linear(code_dim, 128 * 3 * 3)\n","        self.decoder = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Unflatten(1, (128, 3, 3)),\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Learned upsampling\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(64),\n","\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.BatchNorm2d(32),\n","\n","            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: (1, 28, 28)\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Encoding\n","        encoded = self.encoder(x)\n","        encoded = encoded.view(batch_size, -1)\n","        encoded = self.fc_encoder(encoded)\n","\n","        # Decoding\n","        decoded = self.fc_decoder(encoded)\n","        decoded = self.decoder(decoded)\n","\n","        return encoded, decoded"],"metadata":{"id":"u56Uti9P5_ws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels"],"metadata":{"id":"_E5NCVi_6tOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","import numpy as np\n","from torch.utils.data import DataLoader\n","\n","# ------------------------------\n","# Step 1: Define Contrastive Loss Functions\n","# ------------------------------\n","\n","class NTXentLoss(nn.Module):\n","    def __init__(self, temperature=0.5):\n","        super(NTXentLoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, z_i, z_j):\n","        batch_size = z_i.size(0)\n","        z_i = F.normalize(z_i, dim=1)\n","        z_j = F.normalize(z_j, dim=1)\n","        z = torch.cat([z_i, z_j], dim=0)\n","        similarity_matrix = torch.matmul(z, z.T) / self.temperature\n","        mask = ~torch.eye(2 * batch_size, device=z.device).bool()\n","        positives = torch.cat([torch.diag(similarity_matrix, batch_size), torch.diag(similarity_matrix, -batch_size)])\n","        negatives = similarity_matrix.masked_select(mask).view(2 * batch_size, -1)\n","        numerator = torch.exp(positives)\n","        denominator = torch.sum(torch.exp(negatives), dim=-1)\n","        return -torch.mean(torch.log(numerator / denominator))\n","\n","# VicReg Loss\n","class VicRegLoss(nn.Module):\n","    def __init__(self, lambda_var=25, mu_mean=25, nu_cov=1):\n","        super(VicRegLoss, self).__init__()\n","        self.lambda_var = lambda_var\n","        self.mu_mean = mu_mean\n","        self.nu_cov = nu_cov\n","\n","    def forward(self, z1, z2):\n","        variance_loss = torch.mean(torch.relu(1 - torch.std(z1, dim=0))) + torch.mean(torch.relu(1 - torch.std(z2, dim=0)))\n","        mean_loss = torch.mean((torch.mean(z1, dim=0) - torch.mean(z2, dim=0))**2)\n","        z1_centered = z1 - z1.mean(dim=0)\n","        z2_centered = z2 - z2.mean(dim=0)\n","        covariance_matrix_z1 = torch.mm(z1_centered.T, z1_centered) / (z1.size(0) - 1)\n","        covariance_matrix_z2 = torch.mm(z2_centered.T, z2_centered) / (z2.size(0) - 1)\n","        covariance_loss = torch.sum(covariance_matrix_z1 ** 2) - torch.sum(torch.diag(covariance_matrix_z1) ** 2) + \\\n","                          torch.sum(covariance_matrix_z2 ** 2) - torch.sum(torch.diag(covariance_matrix_z2) ** 2)\n","        return self.lambda_var * variance_loss + self.mu_mean * mean_loss + self.nu_cov * covariance_loss\n","\n","# Triplet Loss\n","class TripletLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(TripletLoss, self).__init__()\n","        self.margin = margin\n","        self.criterion = nn.TripletMarginWithDistanceLoss(\n","            distance_function=lambda a, b: 1.0 - F.cosine_similarity(a, b),\n","            margin=self.margin\n","        )\n","\n","    def forward(self, anchor, positive, negative):\n","        return self.criterion(anchor, positive, negative)\n","\n","# ------------------------------\n","# Step 2: Define Augmentation Function\n","# ------------------------------\n","\n","def augment(images):\n","    \"\"\"\n","    Apply tensor-based augmentations to images.\n","\n","    Args:\n","        images (torch.Tensor): Batch of images of shape (batch_size, 1, 28, 28).\n","\n","    Returns:\n","        torch.Tensor: Augmented images of the same shape as input.\n","    \"\"\"\n","    # Resize and crop (ensure output size matches input size)\n","    images = resize(images, size=[28, 28])\n","\n","    # Random horizontal flip\n","    if torch.rand(1) > 0.5:\n","        images = hflip(images)\n","\n","    return images\n","\n","# ------------------------------\n","# Step 3: Define Training Function\n","# ------------------------------\n","\n","def train_autoencoder(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    scheduler=None,\n","    epochs=10,\n","    device=\"cpu\",\n","    noise_factor=0.0,\n","    augment_fn=None,\n","    contrastive_loss_fn=None,\n","    temperature=0.5,\n","    triplet_data=False,\n","):\n","    \"\"\"\n","    Train an autoencoder model with support for contrastive learning, noise injection, and augmentations.\n","\n","    Args:\n","        model (nn.Module): The autoencoder model.\n","        data_loader (DataLoader): DataLoader for training data.\n","        loss_fn (callable): Primary loss function (e.g., reconstruction loss).\n","        optimizer (torch.optim.Optimizer): Optimizer for the model.\n","        scheduler (torch.optim.lr_scheduler._LRScheduler, optional): Learning rate scheduler.\n","        epochs (int): Number of epochs to train.\n","        device (str): Device to train on ('cpu' or 'cuda').\n","        noise_factor (float): Factor for adding noise to input images (denoising autoencoder).\n","        augment_fn (callable, optional): Augmentation function for contrastive learning.\n","        contrastive_loss_fn (callable, optional): Contrastive loss function (e.g., NT-Xent, triplet loss).\n","        temperature (float): Temperature parameter for NT-Xent loss.\n","        triplet_data (bool): Whether the data_loader provides triplets (anchor, positive, negative).\n","\n","    Returns:\n","        None: Prints loss values for each epoch.\n","    \"\"\"\n","    model.to(device).train()\n","\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for batch in data_loader:\n","            # Prepare data based on whether it's triplet data or not\n","            if triplet_data:\n","                anchor, positive, negative = batch\n","                anchor, positive, negative = (\n","                    anchor.to(device).float(),\n","                    positive.to(device).float(),\n","                    negative.to(device).float(),\n","                )\n","                images = anchor  # Use anchor as the primary input for reconstruction\n","            else:\n","                images, _ = batch\n","                images = images.to(device).float()\n","\n","            # Add noise if specified\n","            if noise_factor > 0:\n","                noisy_images = images + noise_factor * torch.randn_like(images)\n","                noisy_images = torch.clamp(noisy_images, 0.0, 1.0)\n","                encoded, decoded = model(noisy_images)\n","            else:\n","                encoded, decoded = model(images)\n","\n","            # Compute reconstruction loss\n","            reconstruction_loss = loss_fn(decoded, images)\n","\n","            # Compute contrastive loss if specified\n","            contrastive_loss_value = 0\n","            if contrastive_loss_fn is not None:\n","                if triplet_data:\n","                    # Triplet loss\n","                    positive_encoded, _ = model(positive)\n","                    negative_encoded, _ = model(negative)\n","                    contrastive_loss_value = contrastive_loss_fn(encoded, positive_encoded, negative_encoded)\n","                else:\n","                    # NT-Xent or other contrastive loss\n","                    if augment_fn:\n","                        augmented_1 = augment_fn(images)\n","                        augmented_2 = augment_fn(images)\n","                        z1, _ = model(augmented_1)\n","                        z2, _ = model(augmented_2)\n","                    else:\n","                        z1, z2 = encoded, encoded  # Use the same embeddings if no augmentation\n","                    contrastive_loss_value = contrastive_loss_fn(z1, z2, temperature)\n","\n","            # Total loss\n","            total_loss_value = reconstruction_loss + contrastive_loss_value\n","\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            total_loss_value.backward()\n","            optimizer.step()\n","\n","            total_loss += total_loss_value.item()\n","\n","        # Step the scheduler if provided\n","        if scheduler:\n","            scheduler.step()\n","\n","        # Print epoch loss\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(data_loader):.4f}\")\n","\n","# ------------------------------\n","# Step 4: Configure Training Pipeline\n","# ------------------------------\n","\n","# ------------------------------\n","# Step 1: Model Selection and Parameters\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"IntermediateAutoencoder\"  # Choose from: \"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Choose from: \"autoencoders\", \"VAEs\", \"denoising_autoencoders\"\n","loss_type = \"mse\"  # Choose from: \"mse\", \"vicreg\", \"ntxent\", \"triplet\"\n","noise_factor = 0.0  # Noise factor for denoising autoencoders\n","temperature = 0.5  # Temperature parameter for NT-Xent loss\n","margin = 1.0  # Margin for Triplet Loss\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()  # Reconstruction loss\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss(lambda_var=25, mu_mean=25, nu_cov=1)  # VicReg loss\n","elif loss_type == \"ntxent\":\n","    criterion = NTXentLoss(temperature=temperature)  # NT-Xent loss\n","elif loss_type == \"triplet\":\n","    criterion = TripletLoss(margin=margin)  # Triplet loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 20 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","# ------------------------------\n","# Step 2: Training the Model\n","# ------------------------------\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_autoencoder(\n","    model=model,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    loss_fn=criterion,  # Primary loss function\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    epochs=100,\n","    device=device,\n","    noise_factor=noise_factor if embedding_type == \"denoising_autoencoders\" else 0.0,  # Add noise for denoising\n","    contrastive_loss_fn=criterion if loss_type in [\"vicreg\", \"ntxent\", \"triplet\"] else None,  # Optional: Contrastive loss\n","    augment_fn=augment if loss_type in [\"vicreg\", \"ntxent\"] else None,  # Optional: Augmentation function\n","    triplet_data=(loss_type == \"triplet\")  # Optional: Use triplet data\n",")\n","\n","# ------------------------------\n","# Step 3: Generate Embeddings\n","# ------------------------------\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    device=device\n",")\n","\n","# ------------------------------\n","# Step 4: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SDmzPRsGzDQp","executionInfo":{"status":"error","timestamp":1737562456314,"user_tz":-210,"elapsed":14409637,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"7135051e-61bf-4541-8aee-2f8252c3f03a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training IntermediateAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.0518\n","Epoch [2/100], Loss: 0.0172\n","Epoch [3/100], Loss: 0.0141\n","Epoch [4/100], Loss: 0.0125\n","Epoch [5/100], Loss: 0.0116\n","Epoch [6/100], Loss: 0.0108\n","Epoch [7/100], Loss: 0.0103\n","Epoch [8/100], Loss: 0.0100\n","Epoch [9/100], Loss: 0.0096\n","Epoch [10/100], Loss: 0.0093\n","Epoch [11/100], Loss: 0.0091\n","Epoch [12/100], Loss: 0.0089\n","Epoch [13/100], Loss: 0.0087\n","Epoch [14/100], Loss: 0.0086\n","Epoch [15/100], Loss: 0.0084\n","Epoch [16/100], Loss: 0.0083\n","Epoch [17/100], Loss: 0.0082\n","Epoch [18/100], Loss: 0.0081\n","Epoch [19/100], Loss: 0.0080\n","Epoch [20/100], Loss: 0.0080\n","Epoch [21/100], Loss: 0.0072\n","Epoch [22/100], Loss: 0.0071\n","Epoch [23/100], Loss: 0.0071\n","Epoch [24/100], Loss: 0.0071\n","Epoch [25/100], Loss: 0.0071\n","Epoch [26/100], Loss: 0.0071\n","Epoch [27/100], Loss: 0.0070\n","Epoch [28/100], Loss: 0.0070\n","Epoch [29/100], Loss: 0.0070\n","Epoch [30/100], Loss: 0.0070\n","Epoch [31/100], Loss: 0.0070\n","Epoch [32/100], Loss: 0.0070\n","Epoch [33/100], Loss: 0.0070\n","Epoch [34/100], Loss: 0.0070\n","Epoch [35/100], Loss: 0.0070\n","Epoch [36/100], Loss: 0.0069\n","Epoch [37/100], Loss: 0.0069\n","Epoch [38/100], Loss: 0.0069\n","Epoch [39/100], Loss: 0.0069\n","Epoch [40/100], Loss: 0.0069\n","Epoch [41/100], Loss: 0.0068\n","Epoch [42/100], Loss: 0.0068\n","Epoch [43/100], Loss: 0.0068\n","Epoch [44/100], Loss: 0.0068\n","Epoch [45/100], Loss: 0.0068\n","Epoch [46/100], Loss: 0.0068\n","Epoch [47/100], Loss: 0.0068\n","Epoch [48/100], Loss: 0.0068\n","Epoch [49/100], Loss: 0.0068\n","Epoch [50/100], Loss: 0.0068\n","Epoch [51/100], Loss: 0.0068\n","Epoch [52/100], Loss: 0.0068\n","Epoch [53/100], Loss: 0.0068\n","Epoch [54/100], Loss: 0.0068\n","Epoch [55/100], Loss: 0.0068\n","Epoch [56/100], Loss: 0.0068\n","Epoch [57/100], Loss: 0.0068\n","Epoch [58/100], Loss: 0.0068\n","Epoch [59/100], Loss: 0.0068\n","Epoch [60/100], Loss: 0.0068\n","Epoch [61/100], Loss: 0.0068\n","Epoch [62/100], Loss: 0.0068\n","Epoch [63/100], Loss: 0.0068\n","Epoch [64/100], Loss: 0.0068\n","Epoch [65/100], Loss: 0.0068\n","Epoch [66/100], Loss: 0.0068\n","Epoch [67/100], Loss: 0.0068\n","Epoch [68/100], Loss: 0.0068\n","Epoch [69/100], Loss: 0.0068\n","Epoch [70/100], Loss: 0.0068\n","Epoch [71/100], Loss: 0.0068\n","Epoch [72/100], Loss: 0.0068\n","Epoch [73/100], Loss: 0.0068\n","Epoch [74/100], Loss: 0.0068\n","Epoch [75/100], Loss: 0.0068\n","Epoch [76/100], Loss: 0.0068\n","Epoch [77/100], Loss: 0.0068\n","Epoch [78/100], Loss: 0.0068\n","Epoch [79/100], Loss: 0.0068\n","Epoch [80/100], Loss: 0.0068\n","Epoch [81/100], Loss: 0.0068\n","Epoch [82/100], Loss: 0.0068\n","Epoch [83/100], Loss: 0.0068\n","Epoch [84/100], Loss: 0.0068\n","Epoch [85/100], Loss: 0.0068\n","Epoch [86/100], Loss: 0.0068\n","Epoch [87/100], Loss: 0.0068\n","Epoch [88/100], Loss: 0.0068\n","Epoch [89/100], Loss: 0.0068\n","Epoch [90/100], Loss: 0.0068\n","Epoch [91/100], Loss: 0.0068\n","Epoch [92/100], Loss: 0.0068\n","Epoch [93/100], Loss: 0.0068\n","Epoch [94/100], Loss: 0.0068\n","Epoch [95/100], Loss: 0.0068\n","Epoch [96/100], Loss: 0.0068\n","Epoch [97/100], Loss: 0.0068\n","Epoch [98/100], Loss: 0.0068\n","Epoch [99/100], Loss: 0.0068\n","Epoch [100/100], Loss: 0.0068\n","Generating embeddings using IntermediateAutoencoder with mse loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"generate_embeddings() missing 1 required positional argument: 'embedding_type'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-540c87ac3aa1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;31m# Generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generating embeddings using {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m embeddings, labels = generate_embeddings(\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Your DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: generate_embeddings() missing 1 required positional argument: 'embedding_type'"]}]},{"cell_type":"code","source":["print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(\n","    model=model,\n","    embedding_type= embedding_type,\n","    data_loader=mnist_loader,  # Your DataLoader\n","    device=device\n",")\n","\n","# ------------------------------\n","# Step 4: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhthd1DRy3w8","executionInfo":{"status":"ok","timestamp":1737562615217,"user_tz":-210,"elapsed":59878,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"8e829cd2-551c-4ea4-a2b0-ce45a1b4cd7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating embeddings using IntermediateAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"AdvancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHkQTUPaOsBs","executionInfo":{"status":"ok","timestamp":1737386767714,"user_tz":-210,"elapsed":1502224,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"2f26c37a-85ac-47d2-daaa-d53499521289"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training AdvancedAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.6372\n","Epoch [2/100], Loss: 0.3665\n","Epoch [3/100], Loss: 0.3064\n","Epoch [4/100], Loss: 0.2097\n","Epoch [5/100], Loss: 0.0671\n","Epoch [6/100], Loss: 0.0357\n","Epoch [7/100], Loss: 0.0296\n","Epoch [8/100], Loss: 0.0267\n","Epoch [9/100], Loss: 0.0248\n","Epoch [10/100], Loss: 0.0227\n","Epoch [11/100], Loss: 0.0216\n","Epoch [12/100], Loss: 0.0205\n","Epoch [13/100], Loss: 0.0197\n","Epoch [14/100], Loss: 0.0188\n","Epoch [15/100], Loss: 0.0180\n","Epoch [16/100], Loss: 0.0174\n","Epoch [17/100], Loss: 0.0167\n","Epoch [18/100], Loss: 0.0162\n","Epoch [19/100], Loss: 0.0158\n","Epoch [20/100], Loss: 0.0154\n","Epoch [21/100], Loss: 0.0149\n","Epoch [22/100], Loss: 0.0147\n","Epoch [23/100], Loss: 0.0144\n","Epoch [24/100], Loss: 0.0142\n","Epoch [25/100], Loss: 0.0138\n","Epoch [26/100], Loss: 0.0136\n","Epoch [27/100], Loss: 0.0135\n","Epoch [28/100], Loss: 0.0131\n","Epoch [29/100], Loss: 0.0128\n","Epoch [30/100], Loss: 0.0128\n","Epoch [31/100], Loss: 0.0124\n","Epoch [32/100], Loss: 0.0124\n","Epoch [33/100], Loss: 0.0121\n","Epoch [34/100], Loss: 0.0118\n","Epoch [35/100], Loss: 0.0120\n","Epoch [36/100], Loss: 0.0117\n","Epoch [37/100], Loss: 0.0117\n","Epoch [38/100], Loss: 0.0117\n","Epoch [39/100], Loss: 0.0112\n","Epoch [40/100], Loss: 0.0115\n","Epoch [41/100], Loss: 0.0111\n","Epoch [42/100], Loss: 0.0112\n","Epoch [43/100], Loss: 0.0109\n","Epoch [44/100], Loss: 0.0106\n","Epoch [45/100], Loss: 0.0105\n","Epoch [46/100], Loss: 0.0103\n","Epoch [47/100], Loss: 0.0103\n","Epoch [48/100], Loss: 0.0103\n","Epoch [49/100], Loss: 0.0101\n","Epoch [50/100], Loss: 0.0099\n","Epoch [51/100], Loss: 0.0099\n","Epoch [52/100], Loss: 0.0098\n","Epoch [53/100], Loss: 0.0098\n","Epoch [54/100], Loss: 0.0098\n","Epoch [55/100], Loss: 0.0097\n","Epoch [56/100], Loss: 0.0097\n","Epoch [57/100], Loss: 0.0094\n","Epoch [58/100], Loss: 0.0094\n","Epoch [59/100], Loss: 0.0092\n","Epoch [60/100], Loss: 0.0092\n","Epoch [61/100], Loss: 0.0092\n","Epoch [62/100], Loss: 0.0090\n","Epoch [63/100], Loss: 0.0090\n","Epoch [64/100], Loss: 0.0090\n","Epoch [65/100], Loss: 0.0089\n","Epoch [66/100], Loss: 0.0090\n","Epoch [67/100], Loss: 0.0088\n","Epoch [68/100], Loss: 0.0088\n","Epoch [69/100], Loss: 0.0088\n","Epoch [70/100], Loss: 0.0086\n","Epoch [71/100], Loss: 0.0086\n","Epoch [72/100], Loss: 0.0085\n","Epoch [73/100], Loss: 0.0084\n","Epoch [74/100], Loss: 0.0084\n","Epoch [75/100], Loss: 0.0085\n","Epoch [76/100], Loss: 0.0084\n","Epoch [77/100], Loss: 0.0083\n","Epoch [78/100], Loss: 0.0082\n","Epoch [79/100], Loss: 0.0082\n","Epoch [80/100], Loss: 0.0082\n","Epoch [81/100], Loss: 0.0079\n","Epoch [82/100], Loss: 0.0080\n","Epoch [83/100], Loss: 0.0080\n","Epoch [84/100], Loss: 0.0081\n","Epoch [85/100], Loss: 0.0078\n","Epoch [86/100], Loss: 0.0078\n","Epoch [87/100], Loss: 0.0079\n","Epoch [88/100], Loss: 0.0078\n","Epoch [89/100], Loss: 0.0078\n","Epoch [90/100], Loss: 0.0077\n","Epoch [91/100], Loss: 0.0077\n","Epoch [92/100], Loss: 0.0076\n","Epoch [93/100], Loss: 0.0076\n","Epoch [94/100], Loss: 0.0076\n","Epoch [95/100], Loss: 0.0076\n","Epoch [96/100], Loss: 0.0075\n","Epoch [97/100], Loss: 0.0074\n","Epoch [98/100], Loss: 0.0075\n","Epoch [99/100], Loss: 0.0074\n","Epoch [100/100], Loss: 0.0074\n","Generating embeddings using AdvancedAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"IntermediateAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOYys9LMQ4Bc","executionInfo":{"status":"ok","timestamp":1737388309401,"user_tz":-210,"elapsed":1080757,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"ea235020-6123-42e9-b127-eadaca700bca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training IntermediateAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.3440\n","Epoch [2/100], Loss: 0.0693\n","Epoch [3/100], Loss: 0.0423\n","Epoch [4/100], Loss: 0.0327\n","Epoch [5/100], Loss: 0.0277\n","Epoch [6/100], Loss: 0.0246\n","Epoch [7/100], Loss: 0.0222\n","Epoch [8/100], Loss: 0.0205\n","Epoch [9/100], Loss: 0.0191\n","Epoch [10/100], Loss: 0.0181\n","Epoch [11/100], Loss: 0.0174\n","Epoch [12/100], Loss: 0.0165\n","Epoch [13/100], Loss: 0.0157\n","Epoch [14/100], Loss: 0.0149\n","Epoch [15/100], Loss: 0.0147\n","Epoch [16/100], Loss: 0.0139\n","Epoch [17/100], Loss: 0.0136\n","Epoch [18/100], Loss: 0.0136\n","Epoch [19/100], Loss: 0.0130\n","Epoch [20/100], Loss: 0.0128\n","Epoch [21/100], Loss: 0.0124\n","Epoch [22/100], Loss: 0.0122\n","Epoch [23/100], Loss: 0.0116\n","Epoch [24/100], Loss: 0.0116\n","Epoch [25/100], Loss: 0.0114\n","Epoch [26/100], Loss: 0.0111\n","Epoch [27/100], Loss: 0.0111\n","Epoch [28/100], Loss: 0.0109\n","Epoch [29/100], Loss: 0.0106\n","Epoch [30/100], Loss: 0.0103\n","Epoch [31/100], Loss: 0.0106\n","Epoch [32/100], Loss: 0.0103\n","Epoch [33/100], Loss: 0.0101\n","Epoch [34/100], Loss: 0.0098\n","Epoch [35/100], Loss: 0.0099\n","Epoch [36/100], Loss: 0.0095\n","Epoch [37/100], Loss: 0.0095\n","Epoch [38/100], Loss: 0.0095\n","Epoch [39/100], Loss: 0.0094\n","Epoch [40/100], Loss: 0.0093\n","Epoch [41/100], Loss: 0.0091\n","Epoch [42/100], Loss: 0.0091\n","Epoch [43/100], Loss: 0.0088\n","Epoch [44/100], Loss: 0.0088\n","Epoch [45/100], Loss: 0.0087\n","Epoch [46/100], Loss: 0.0086\n","Epoch [47/100], Loss: 0.0088\n","Epoch [48/100], Loss: 0.0086\n","Epoch [49/100], Loss: 0.0084\n","Epoch [50/100], Loss: 0.0085\n","Epoch [51/100], Loss: 0.0084\n","Epoch [52/100], Loss: 0.0082\n","Epoch [53/100], Loss: 0.0082\n","Epoch [54/100], Loss: 0.0082\n","Epoch [55/100], Loss: 0.0079\n","Epoch [56/100], Loss: 0.0079\n","Epoch [57/100], Loss: 0.0079\n","Epoch [58/100], Loss: 0.0079\n","Epoch [59/100], Loss: 0.0079\n","Epoch [60/100], Loss: 0.0076\n","Epoch [61/100], Loss: 0.0076\n","Epoch [62/100], Loss: 0.0076\n","Epoch [63/100], Loss: 0.0077\n","Epoch [64/100], Loss: 0.0075\n","Epoch [65/100], Loss: 0.0075\n","Epoch [66/100], Loss: 0.0075\n","Epoch [67/100], Loss: 0.0074\n","Epoch [68/100], Loss: 0.0073\n","Epoch [69/100], Loss: 0.0073\n","Epoch [70/100], Loss: 0.0072\n","Epoch [71/100], Loss: 0.0071\n","Epoch [72/100], Loss: 0.0072\n","Epoch [73/100], Loss: 0.0071\n","Epoch [74/100], Loss: 0.0071\n","Epoch [75/100], Loss: 0.0069\n","Epoch [76/100], Loss: 0.0069\n","Epoch [77/100], Loss: 0.0070\n","Epoch [78/100], Loss: 0.0070\n","Epoch [79/100], Loss: 0.0068\n","Epoch [80/100], Loss: 0.0069\n","Epoch [81/100], Loss: 0.0069\n","Epoch [82/100], Loss: 0.0067\n","Epoch [83/100], Loss: 0.0066\n","Epoch [84/100], Loss: 0.0066\n","Epoch [85/100], Loss: 0.0067\n","Epoch [86/100], Loss: 0.0066\n","Epoch [87/100], Loss: 0.0066\n","Epoch [88/100], Loss: 0.0066\n","Epoch [89/100], Loss: 0.0065\n","Epoch [90/100], Loss: 0.0065\n","Epoch [91/100], Loss: 0.0065\n","Epoch [92/100], Loss: 0.0065\n","Epoch [93/100], Loss: 0.0067\n","Epoch [94/100], Loss: 0.0066\n","Epoch [95/100], Loss: 0.0064\n","Epoch [96/100], Loss: 0.0063\n","Epoch [97/100], Loss: 0.0063\n","Epoch [98/100], Loss: 0.0063\n","Epoch [99/100], Loss: 0.0063\n","Epoch [100/100], Loss: 0.0063\n","Generating embeddings using IntermediateAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"BasicAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","elif loss_type == \"ssim\":\n","    criterion = vae_ssim_loss\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","if embedding_type == \"autoencoders\":\n","    train_autoencoder(model, mnist_loader, criterion, optimizer, epochs=100, device=device)\n","elif embedding_type == \"VAEs\":\n","    train_vae(model, mnist_loader, optimizer, loss_fn=vae_loss, epochs=10, device=device)\n","elif embedding_type == \"denoising_autoencoders\":\n","    train_denoising_autoencoder(model, mnist_loader, criterion, optimizer, noise_factor=0.1, epochs=10, device=device)\n","else:\n","    raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztH2i-oLRBUm","executionInfo":{"status":"ok","timestamp":1737388913536,"user_tz":-210,"elapsed":604139,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"2b5c963a-fc06-4b83-d253-fb74b3f2e0b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training BasicAutoencoder with mse loss...\n","Epoch [1/100], Loss: 0.3403\n","Epoch [2/100], Loss: 0.2061\n","Epoch [3/100], Loss: 0.1146\n","Epoch [4/100], Loss: 0.0868\n","Epoch [5/100], Loss: 0.0737\n","Epoch [6/100], Loss: 0.0653\n","Epoch [7/100], Loss: 0.0595\n","Epoch [8/100], Loss: 0.0546\n","Epoch [9/100], Loss: 0.0507\n","Epoch [10/100], Loss: 0.0478\n","Epoch [11/100], Loss: 0.0455\n","Epoch [12/100], Loss: 0.0433\n","Epoch [13/100], Loss: 0.0413\n","Epoch [14/100], Loss: 0.0398\n","Epoch [15/100], Loss: 0.0387\n","Epoch [16/100], Loss: 0.0373\n","Epoch [17/100], Loss: 0.0362\n","Epoch [18/100], Loss: 0.0352\n","Epoch [19/100], Loss: 0.0340\n","Epoch [20/100], Loss: 0.0332\n","Epoch [21/100], Loss: 0.0327\n","Epoch [22/100], Loss: 0.0319\n","Epoch [23/100], Loss: 0.0312\n","Epoch [24/100], Loss: 0.0305\n","Epoch [25/100], Loss: 0.0299\n","Epoch [26/100], Loss: 0.0295\n","Epoch [27/100], Loss: 0.0291\n","Epoch [28/100], Loss: 0.0286\n","Epoch [29/100], Loss: 0.0280\n","Epoch [30/100], Loss: 0.0278\n","Epoch [31/100], Loss: 0.0274\n","Epoch [32/100], Loss: 0.0269\n","Epoch [33/100], Loss: 0.0267\n","Epoch [34/100], Loss: 0.0263\n","Epoch [35/100], Loss: 0.0262\n","Epoch [36/100], Loss: 0.0257\n","Epoch [37/100], Loss: 0.0254\n","Epoch [38/100], Loss: 0.0251\n","Epoch [39/100], Loss: 0.0248\n","Epoch [40/100], Loss: 0.0246\n","Epoch [41/100], Loss: 0.0244\n","Epoch [42/100], Loss: 0.0241\n","Epoch [43/100], Loss: 0.0238\n","Epoch [44/100], Loss: 0.0235\n","Epoch [45/100], Loss: 0.0234\n","Epoch [46/100], Loss: 0.0234\n","Epoch [47/100], Loss: 0.0230\n","Epoch [48/100], Loss: 0.0228\n","Epoch [49/100], Loss: 0.0227\n","Epoch [50/100], Loss: 0.0225\n","Epoch [51/100], Loss: 0.0225\n","Epoch [52/100], Loss: 0.0223\n","Epoch [53/100], Loss: 0.0221\n","Epoch [54/100], Loss: 0.0219\n","Epoch [55/100], Loss: 0.0217\n","Epoch [56/100], Loss: 0.0216\n","Epoch [57/100], Loss: 0.0214\n","Epoch [58/100], Loss: 0.0212\n","Epoch [59/100], Loss: 0.0212\n","Epoch [60/100], Loss: 0.0211\n","Epoch [61/100], Loss: 0.0210\n","Epoch [62/100], Loss: 0.0207\n","Epoch [63/100], Loss: 0.0205\n","Epoch [64/100], Loss: 0.0206\n","Epoch [65/100], Loss: 0.0204\n","Epoch [66/100], Loss: 0.0204\n","Epoch [67/100], Loss: 0.0202\n","Epoch [68/100], Loss: 0.0202\n","Epoch [69/100], Loss: 0.0199\n","Epoch [70/100], Loss: 0.0199\n","Epoch [71/100], Loss: 0.0199\n","Epoch [72/100], Loss: 0.0197\n","Epoch [73/100], Loss: 0.0197\n","Epoch [74/100], Loss: 0.0195\n","Epoch [75/100], Loss: 0.0195\n","Epoch [76/100], Loss: 0.0194\n","Epoch [77/100], Loss: 0.0192\n","Epoch [78/100], Loss: 0.0193\n","Epoch [79/100], Loss: 0.0192\n","Epoch [80/100], Loss: 0.0190\n","Epoch [81/100], Loss: 0.0189\n","Epoch [82/100], Loss: 0.0187\n","Epoch [83/100], Loss: 0.0189\n","Epoch [84/100], Loss: 0.0188\n","Epoch [85/100], Loss: 0.0187\n","Epoch [86/100], Loss: 0.0187\n","Epoch [87/100], Loss: 0.0186\n","Epoch [88/100], Loss: 0.0185\n","Epoch [89/100], Loss: 0.0185\n","Epoch [90/100], Loss: 0.0184\n","Epoch [91/100], Loss: 0.0183\n","Epoch [92/100], Loss: 0.0182\n","Epoch [93/100], Loss: 0.0182\n","Epoch [94/100], Loss: 0.0180\n","Epoch [95/100], Loss: 0.0181\n","Epoch [96/100], Loss: 0.0180\n","Epoch [97/100], Loss: 0.0179\n","Epoch [98/100], Loss: 0.0179\n","Epoch [99/100], Loss: 0.0179\n","Epoch [100/100], Loss: 0.0178\n","Generating embeddings using BasicAutoencoder with mse loss...\n","Embeddings saved in PyTorch format: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse_embeddings.pt\n","Model saved: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse.pth\n"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"EnhancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"mse\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=20, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"MAaK-hZjah3-","executionInfo":{"status":"error","timestamp":1737472116552,"user_tz":-210,"elapsed":921,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"1b3b5306-3d86-44c6-cb6f-db17e3b68ccf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training EnhancedAutoencoder with mse loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"train_autoencoder() got an unexpected keyword argument 'scheduler'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-53762437c672>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: train_autoencoder() got an unexpected keyword argument 'scheduler'"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"FlexibleVAE\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"vae\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"Y_eXsL-sYuUl","executionInfo":{"status":"error","timestamp":1737455119844,"user_tz":-210,"elapsed":22324,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"08b18305-34ca-4681-8483-06f3db77f65d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training FlexibleVAE with vae loss...\n"]},{"output_type":"error","ename":"TypeError","evalue":"'function' object is not iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-615e66d254db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/embeddings/encoder_training.py\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(vae, train_loader, val_loader, optimizer, scheduler, loss_fn, epochs, device, save_best, save_path)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mtotal_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"AdvancedAutoencoder\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"ntxent\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"c4lXegQrZEhF","executionInfo":{"status":"error","timestamp":1737471735460,"user_tz":-210,"elapsed":16599432,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"c5a121af-1603-4ac2-c017-94c8870c0526"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training AdvancedAutoencoder with ntxent loss...\n","Epoch [1/200], Loss: 3.2167\n","Epoch [2/200], Loss: 3.1131\n","Epoch [3/200], Loss: 3.0889\n","Epoch [4/200], Loss: 3.0769\n","Epoch [5/200], Loss: 3.0689\n","Epoch [6/200], Loss: 3.0646\n","Epoch [7/200], Loss: 3.0587\n","Epoch [8/200], Loss: 3.0553\n","Epoch [9/200], Loss: 3.0534\n","Epoch [10/200], Loss: 3.0497\n","Epoch [11/200], Loss: 3.0399\n","Epoch [12/200], Loss: 3.0386\n","Epoch [13/200], Loss: 3.0380\n","Epoch [14/200], Loss: 3.0374\n","Epoch [15/200], Loss: 3.0367\n","Epoch [16/200], Loss: 3.0362\n","Epoch [17/200], Loss: 3.0360\n","Epoch [18/200], Loss: 3.0358\n","Epoch [19/200], Loss: 3.0352\n","Epoch [20/200], Loss: 3.0348\n","Epoch [21/200], Loss: 3.0337\n","Epoch [22/200], Loss: 3.0331\n","Epoch [23/200], Loss: 3.0336\n","Epoch [24/200], Loss: 3.0332\n","Epoch [25/200], Loss: 3.0330\n","Epoch [26/200], Loss: 3.0332\n","Epoch [27/200], Loss: 3.0334\n","Epoch [28/200], Loss: 3.0329\n","Epoch [29/200], Loss: 3.0338\n","Epoch [30/200], Loss: 3.0334\n","Epoch [31/200], Loss: 3.0331\n","Epoch [32/200], Loss: 3.0330\n","Epoch [33/200], Loss: 3.0332\n","Epoch [34/200], Loss: 3.0333\n","Epoch [35/200], Loss: 3.0332\n","Epoch [36/200], Loss: 3.0328\n","Epoch [37/200], Loss: 3.0333\n","Epoch [38/200], Loss: 3.0334\n","Epoch [39/200], Loss: 3.0329\n","Epoch [40/200], Loss: 3.0332\n","Epoch [41/200], Loss: 3.0327\n","Epoch [42/200], Loss: 3.0332\n","Epoch [43/200], Loss: 3.0328\n","Epoch [44/200], Loss: 3.0327\n","Epoch [45/200], Loss: 3.0331\n","Epoch [46/200], Loss: 3.0333\n","Epoch [47/200], Loss: 3.0329\n","Epoch [48/200], Loss: 3.0332\n","Epoch [49/200], Loss: 3.0330\n","Epoch [50/200], Loss: 3.0325\n","Epoch [51/200], Loss: 3.0330\n","Epoch [52/200], Loss: 3.0334\n","Epoch [53/200], Loss: 3.0337\n","Epoch [54/200], Loss: 3.0332\n","Epoch [55/200], Loss: 3.0332\n","Epoch [56/200], Loss: 3.0329\n","Epoch [57/200], Loss: 3.0328\n","Epoch [58/200], Loss: 3.0329\n","Epoch [59/200], Loss: 3.0331\n","Epoch [60/200], Loss: 3.0331\n","Epoch [61/200], Loss: 3.0329\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fd37e98c6c71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Train the model using the selected function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training {model_name} with {loss_type} loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/embeddings/encoder_training.py\u001b[0m in \u001b[0;36mtrain_with_ntxent_loss\u001b[0;34m(model, data_loader, ntxent_loss_fn, optimizer, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from skimage.metrics import structural_similarity as ssim\n","\n","\n","# ------------------------------\n","# Step 2: Train and Save Selected Model\n","# ------------------------------\n","\n","# Model selection and parameters\n","model_name = \"FlexibleVAE\"  # Change to desired model\n","code_dim = 50  # Dimensionality of the embedding\n","embedding_type = \"autoencoders\"  # Change based on the embedding type\n","loss_type = \"vae\"  # Choose from \"mse\", \"vicreg\", \"vae\", \"ssim\", etc.\n","\n","# Initialize the model\n","if model_name == \"BasicAutoencoder\":\n","    model = BasicAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"IntermediateAutoencoder\":\n","    model = IntermediateAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"AdvancedAutoencoder\":\n","    model = AdvancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"EnhancedAutoencoder\":\n","    model = EnhancedAutoencoder(code_dim=code_dim).to(device)\n","elif model_name == \"BasicVAE\":\n","    model = BasicVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"VAEWithFCDecoder\":\n","    model = VAEWithFCDecoder(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"ImprovedVAE\":\n","    model = ImprovedVAE(input_dim=1, code_dim=code_dim).to(device)\n","elif model_name == \"FlexibleVAE\":\n","    model = FlexibleVAE(input_shape=(1, 28, 28), code_dim=code_dim).to(device)\n","elif model_name == \"DenoisingAutoencoder\":\n","    model = DenoisingAutoencoder(code_dim=code_dim, strong_architecture=False).to(device)\n","else:\n","    raise ValueError(f\"Model '{model_name}' is not recognized.\")\n","\n","# Define the loss function and corresponding training function\n","if loss_type == \"mse\":\n","    criterion = torch.nn.MSELoss()\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_autoencoder\n","    elif model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder\n","\n","elif loss_type == \"vicreg\":\n","    criterion = VicRegLoss()  # VicReg loss requires a different training procedure\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_triplet_loss  # Assuming triplet loss works for VicReg\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VicReg loss.\")\n","\n","elif loss_type == \"vae\":\n","    criterion = vae_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae  # Use VAE training function for VAE loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE loss.\")\n","\n","elif loss_type == \"ssim\":\n","    criterion = ssim  # Assuming SSIM loss function exists\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim  # Train using SSIM-based loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with SSIM loss.\")\n","\n","elif loss_type == \"dae\":\n","    criterion = nn.MSELoss()\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_denoising_autoencoder  # Use DAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE loss.\")\n","\n","elif loss_type == \"dae_contrastive\":\n","    criterion = nn.MSELoss()  # DAE contrastive will use triplet loss or contrastive loss\n","    if model_name == \"DenoisingAutoencoder\":\n","        train_function = train_dae_ssim_contrastive  # Train DAE with contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with DAE contrastive loss.\")\n","\n","elif loss_type == \"ntxent\":\n","    criterion = nt_xent_loss\n","    if model_name in [\"BasicAutoencoder\", \"IntermediateAutoencoder\", \"AdvancedAutoencoder\", \"EnhancedAutoencoder\"]:\n","        train_function = train_with_ntxent_loss  # NT-Xent loss for contrastive learning\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with NT-Xent loss.\")\n","\n","elif loss_type == \"vae_improved\":\n","    criterion = vae_loss\n","    if model_name in [\"ImprovedVAE\"]:\n","        train_function = train_vae_improved  # Use improved VAE training function\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with improved VAE loss.\")\n","\n","elif loss_type == \"vae_ssim_contrastive\":\n","    criterion = vae_ssim_loss\n","    if model_name in [\"BasicVAE\", \"VAEWithFCDecoder\", \"ImprovedVAE\", \"FlexibleVAE\"]:\n","        train_function = train_vae_ssim_contrastive  # Train using SSIM + contrastive loss\n","    else:\n","        raise ValueError(f\"Model '{model_name}' is not compatible with VAE SSIM contrastive loss.\")\n","\n","else:\n","    raise ValueError(f\"Loss type '{loss_type}' is not recognized.\")\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Define scheduler (e.g., StepLR to reduce learning rate every 10 epochs by a factor of 0.1)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model using the selected function\n","print(f\"Training {model_name} with {loss_type} loss...\")\n","train_function(model, mnist_loader, criterion, optimizer, epochs=200, device=device, scheduler=scheduler)\n","\n","# ------------------------------\n","# Step 5: Save Embeddings and Model\n","# ------------------------------\n","\n","# Define the base storage directory for embeddings\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","# Ensure a dedicated directory for embeddings\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","# Create a unique subdirectory for this embedding type, model, and loss type\n","embedding_subdir = f\"{embedding_type}_{model_name}_{loss_type}\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","# Choose saving format: default is .pt, but .npy can be chosen\n","save_format = \"pt\"  # Change to \"npy\" for NumPy format\n","\n","def generate_embeddings(model, data_loader, embedding_type, device=\"cpu\"):\n","    model.eval()  # Set model to evaluation mode\n","    embeddings = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for images, label_batch in data_loader:\n","            images = images.to(device)\n","            if embedding_type == \"autoencoders\":\n","                encoded, _ = model(images)\n","            elif embedding_type == \"VAEs\":\n","                mu, _, _ = model(images)\n","                encoded = mu  # Use the mean of the latent space\n","            elif embedding_type == \"denoising_autoencoders\":\n","                _, _, encoded = model(images)\n","            else:\n","                raise ValueError(f\"Embedding type '{embedding_type}' is not recognized.\")\n","\n","            embeddings.append(encoded.cpu())\n","            labels.append(label_batch)\n","\n","    embeddings = torch.cat(embeddings, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","\n","    return embeddings, labels\n","\n","# Generate embeddings\n","print(f\"Generating embeddings using {model_name} with {loss_type} loss...\")\n","embeddings, labels = generate_embeddings(model, mnist_loader, embedding_type, device=device)\n","\n","# Save embeddings with differentiated names based on the model, loss type, and embedding type\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.pt\")\n","    torch.save({\"embeddings\": embeddings, \"labels\": labels}, embedding_file)\n","    print(f\"Embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": labels.numpy()})\n","    print(f\"Embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# Save the model\n","model_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}.pth\")\n","torch.save(model.state_dict(), model_file)\n","print(f\"Model saved: {model_file}\")\n"],"metadata":{"id":"d3HnUC18Y12q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------------------\n","# Step 1: Configuration\n","# ------------------------------\n","\n","save_format = \"pt\"  # Change to \"npy\" if needed\n","\n","# Ensure valid save format\n","if save_format not in ['pt', 'npy']:\n","    print(f\"Invalid save format: {save_format}. Defaulting to 'pt'.\")\n","    save_format = 'pt'\n","\n","# Define model and loss type for naming conventions (update as needed)\n","model_name = \"matrix_factorization\"  # Example model name\n","loss_type = \"default_loss\"  # Update this as necessary (e.g., \"mse\", \"contrastive\", etc.)\n","\n","# ------------------------------\n","# Step 2: Matrix Factorization\n","# ------------------------------\n","\n","# Extract flattened images and labels\n","sampled_x, sampled_y = mnist_loader.dataset.tensors[0].numpy(), mnist_loader.dataset.tensors[1].numpy()\n","\n","print(\"Processing matrix factorization models (PCA, SVD, NMF)...\")\n","base_dir = \"./saved_embeddings\"\n","os.makedirs(base_dir, exist_ok=True)\n","\n","embeddings_dir = os.path.join(base_dir, \"embeddings\")\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","factorized_embeddings, factorized_labels = process_matrix_factorization(\n","    sampled_x, sampled_y, n_components=50\n",")\n","for method, embeddings in factorized_embeddings.items():\n","    embedding_subdir = f\"matrix_factorization_{method}\"\n","    embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","    os.makedirs(embedding_dir, exist_ok=True)\n","\n","    if save_format == \"pt\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_{method}_embeddings.pt\")\n","        torch.save({\"embeddings\": embeddings, \"labels\": factorized_labels}, embedding_file)\n","        print(f\"{method} embeddings saved in PyTorch format: {embedding_file}\")\n","    elif save_format == \"npy\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_{method}_embeddings.npy\")\n","        np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": factorized_labels.numpy()})\n","        print(f\"{method} embeddings saved in NumPy format: {embedding_file}\")\n","    else:\n","        raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 3: SIFT Features\n","# ------------------------------\n","\n","print(\"Processing SIFT features...\")\n","sift_features = apply_sift(sampled_x, n_features=50)\n","sift_labels = torch.tensor(sampled_y, dtype=torch.long)\n","\n","embedding_subdir = \"sift_features\"\n","embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","os.makedirs(embedding_dir, exist_ok=True)\n","\n","if save_format == \"pt\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_sift_embeddings.pt\")\n","    torch.save({\"embeddings\": torch.tensor(sift_features), \"labels\": sift_labels}, embedding_file)\n","    print(f\"SIFT embeddings saved in PyTorch format: {embedding_file}\")\n","elif save_format == \"npy\":\n","    embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_sift_embeddings.npy\")\n","    np.save(embedding_file, {\"embeddings\": sift_features, \"labels\": sift_labels.numpy()})\n","    print(f\"SIFT embeddings saved in NumPy format: {embedding_file}\")\n","else:\n","    raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 4: Kernel PCA\n","# ------------------------------\n","\n","print(\"Processing Kernel PCA...\")\n","kernel_pca_features, kernel_pca_labels = process_feature_extraction(\n","    sampled_x, sampled_y, n_features=50, kernel=\"rbf\", n_components=50\n",")\n","for method, embeddings in kernel_pca_features.items():\n","    embedding_subdir = f\"kernel_pca_{method}\"\n","    embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","    os.makedirs(embedding_dir, exist_ok=True)\n","\n","    if save_format == \"pt\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_kernel_pca_{method}_embeddings.pt\")\n","        torch.save({\"embeddings\": embeddings, \"labels\": kernel_pca_labels}, embedding_file)\n","        print(f\"{method} Kernel PCA embeddings saved in PyTorch format: {embedding_file}\")\n","    elif save_format == \"npy\":\n","        embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_kernel_pca_{method}_embeddings.npy\")\n","        np.save(embedding_file, {\"embeddings\": embeddings.numpy(), \"labels\": kernel_pca_labels.numpy()})\n","        print(f\"{method} Kernel PCA embeddings saved in NumPy format: {embedding_file}\")\n","    else:\n","        raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","# ------------------------------\n","# Step 5: Normalizing Flow\n","# ------------------------------\n","\n","print(\"Processing Normalizing Flow...\")\n","for method, embeddings in factorized_embeddings.items():\n","    # Initialize Normalizing Flow model\n","    input_dim = embeddings.size(1)\n","    nf_model = NormalizingFlowModel(input_dim=input_dim, num_flows=4)\n","    nf_model.to(device)\n","\n","    # Train Normalizing Flow model\n","    trained_nf_model = train_nf_model(\n","        nf_model, embeddings, num_epochs=200, lr=1e-3, batch_size=128\n","    )\n","\n","    # Refine embeddings\n","    with torch.no_grad():\n","        refined_embeddings, _ = trained_nf_model(embeddings)\n","\n","        embedding_subdir = f\"normalizing_flow_{method}\"\n","        embedding_dir = os.path.join(embeddings_dir, embedding_subdir)\n","        os.makedirs(embedding_dir, exist_ok=True)\n","\n","        if save_format == \"pt\":\n","            embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_normalizing_flow_{method}_refined_embeddings.pt\")\n","            torch.save({\"embeddings\": refined_embeddings, \"labels\": factorized_labels}, embedding_file)\n","            print(f\"{method} refined embeddings (Normalizing Flow) saved in PyTorch format: {embedding_file}\")\n","        elif save_format == \"npy\":\n","            embedding_file = os.path.join(embedding_dir, f\"{model_name}_{loss_type}_normalizing_flow_{method}_refined_embeddings.npy\")\n","            np.save(embedding_file, {\"embeddings\": refined_embeddings.numpy(), \"labels\": factorized_labels.numpy()})\n","            print(f\"{method} refined embeddings (Normalizing Flow) saved in NumPy format: {embedding_file}\")\n","        else:\n","            raise ValueError(f\"Unsupported save format: {save_format}\")\n","\n","print(\"Feature extraction and normalizing flow processing complete!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BavfjvO2VAjZ","executionInfo":{"status":"ok","timestamp":1737454621474,"user_tz":-210,"elapsed":6311,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"e0a053ea-f4c2-44cb-ab01-5b8c07c45d6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing matrix factorization models (PCA, SVD, NMF)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1741: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_PCA/matrix_factorization_default_loss_PCA_embeddings.pt\n","SVD embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_SVD/matrix_factorization_default_loss_SVD_embeddings.pt\n","NMF embeddings saved in PyTorch format: ./saved_embeddings/embeddings/matrix_factorization_NMF/matrix_factorization_default_loss_NMF_embeddings.pt\n","Processing SIFT features...\n","SIFT embeddings saved in PyTorch format: ./saved_embeddings/embeddings/sift_features/matrix_factorization_default_loss_sift_embeddings.pt\n","Processing Kernel PCA...\n","SIFT Kernel PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/kernel_pca_SIFT/matrix_factorization_default_loss_kernel_pca_SIFT_embeddings.pt\n","Kernel PCA Kernel PCA embeddings saved in PyTorch format: ./saved_embeddings/embeddings/kernel_pca_Kernel PCA/matrix_factorization_default_loss_kernel_pca_Kernel PCA_embeddings.pt\n","Processing Normalizing Flow...\n","Epoch 1/200, Loss: 2105532.0000\n","Epoch 2/200, Loss: 1371861.2500\n","Epoch 3/200, Loss: 1126910.5000\n","Epoch 4/200, Loss: 959351.8750\n","Epoch 5/200, Loss: 864164.3125\n","Epoch 6/200, Loss: 786244.5000\n","Epoch 7/200, Loss: 721899.8125\n","Epoch 8/200, Loss: 664512.6250\n","Epoch 9/200, Loss: 609381.1875\n","Epoch 10/200, Loss: 559293.9375\n","Epoch 11/200, Loss: 510937.0938\n","Epoch 12/200, Loss: 470874.3438\n","Epoch 13/200, Loss: 436454.6250\n","Epoch 14/200, Loss: 404345.8750\n","Epoch 15/200, Loss: 379761.9062\n","Epoch 16/200, Loss: 357777.1250\n","Epoch 17/200, Loss: 335547.7500\n","Epoch 18/200, Loss: 314221.4062\n","Epoch 19/200, Loss: 294646.0000\n","Epoch 20/200, Loss: 277729.7500\n","Epoch 21/200, Loss: 262339.3750\n","Epoch 22/200, Loss: 248801.9062\n","Epoch 23/200, Loss: 237040.6875\n","Epoch 24/200, Loss: 225382.1250\n","Epoch 25/200, Loss: 213554.0469\n","Epoch 26/200, Loss: 203046.3906\n","Epoch 27/200, Loss: 192870.7812\n","Epoch 28/200, Loss: 184185.5625\n","Epoch 29/200, Loss: 175942.1875\n","Epoch 30/200, Loss: 167710.6562\n","Epoch 31/200, Loss: 159133.3750\n","Epoch 32/200, Loss: 151090.0625\n","Epoch 33/200, Loss: 143847.4219\n","Epoch 34/200, Loss: 136919.7656\n","Epoch 35/200, Loss: 130742.0703\n","Epoch 36/200, Loss: 125214.0391\n","Epoch 37/200, Loss: 120406.5625\n","Epoch 38/200, Loss: 116063.0234\n","Epoch 39/200, Loss: 111900.5469\n","Epoch 40/200, Loss: 107834.9375\n","Epoch 41/200, Loss: 103941.9062\n","Epoch 42/200, Loss: 100229.7344\n","Epoch 43/200, Loss: 96637.6250\n","Epoch 44/200, Loss: 93144.7109\n","Epoch 45/200, Loss: 89765.5312\n","Epoch 46/200, Loss: 86543.3594\n","Epoch 47/200, Loss: 83587.8281\n","Epoch 48/200, Loss: 80868.2891\n","Epoch 49/200, Loss: 78263.9922\n","Epoch 50/200, Loss: 75727.1953\n","Epoch 51/200, Loss: 73238.8438\n","Epoch 52/200, Loss: 70822.8906\n","Epoch 53/200, Loss: 68554.4844\n","Epoch 54/200, Loss: 66264.5625\n","Epoch 55/200, Loss: 64225.9844\n","Epoch 56/200, Loss: 62319.0078\n","Epoch 57/200, Loss: 60492.9102\n","Epoch 58/200, Loss: 58792.5703\n","Epoch 59/200, Loss: 57237.7578\n","Epoch 60/200, Loss: 55801.0859\n","Epoch 61/200, Loss: 54346.9727\n","Epoch 62/200, Loss: 52881.1914\n","Epoch 63/200, Loss: 51527.9141\n","Epoch 64/200, Loss: 50278.5625\n","Epoch 65/200, Loss: 49091.9766\n","Epoch 66/200, Loss: 47941.2852\n","Epoch 67/200, Loss: 46817.4375\n","Epoch 68/200, Loss: 45729.6016\n","Epoch 69/200, Loss: 44685.0000\n","Epoch 70/200, Loss: 43661.2500\n","Epoch 71/200, Loss: 42656.9375\n","Epoch 72/200, Loss: 41696.7031\n","Epoch 73/200, Loss: 40776.6484\n","Epoch 74/200, Loss: 39884.7383\n","Epoch 75/200, Loss: 39019.6680\n","Epoch 76/200, Loss: 38188.8594\n","Epoch 77/200, Loss: 37394.0898\n","Epoch 78/200, Loss: 36625.8203\n","Epoch 79/200, Loss: 35882.1875\n","Epoch 80/200, Loss: 35169.9102\n","Epoch 81/200, Loss: 34486.4688\n","Epoch 82/200, Loss: 33823.5742\n","Epoch 83/200, Loss: 33177.8164\n","Epoch 84/200, Loss: 32551.8242\n","Epoch 85/200, Loss: 31946.8691\n","Epoch 86/200, Loss: 31359.3652\n","Epoch 87/200, Loss: 30787.3340\n","Epoch 88/200, Loss: 30233.3398\n","Epoch 89/200, Loss: 29697.4609\n","Epoch 90/200, Loss: 29175.7383\n","Epoch 91/200, Loss: 28666.6055\n","Epoch 92/200, Loss: 28172.3789\n","Epoch 93/200, Loss: 27692.1426\n","Epoch 94/200, Loss: 27222.4727\n","Epoch 95/200, Loss: 26765.9258\n","Epoch 96/200, Loss: 26324.6719\n","Epoch 97/200, Loss: 25896.7305\n","Epoch 98/200, Loss: 25481.9746\n","Epoch 99/200, Loss: 25080.5391\n","Epoch 100/200, Loss: 24689.1719\n","Epoch 101/200, Loss: 24305.9316\n","Epoch 102/200, Loss: 23931.9062\n","Epoch 103/200, Loss: 23566.7090\n","Epoch 104/200, Loss: 23209.2207\n","Epoch 105/200, Loss: 22859.9180\n","Epoch 106/200, Loss: 22518.7441\n","Epoch 107/200, Loss: 22184.3594\n","Epoch 108/200, Loss: 21856.7676\n","Epoch 109/200, Loss: 21537.1270\n","Epoch 110/200, Loss: 21225.8438\n","Epoch 111/200, Loss: 20922.8809\n","Epoch 112/200, Loss: 20628.1504\n","Epoch 113/200, Loss: 20341.0020\n","Epoch 114/200, Loss: 20060.4609\n","Epoch 115/200, Loss: 19785.9141\n","Epoch 116/200, Loss: 19516.7344\n","Epoch 117/200, Loss: 19252.1523\n","Epoch 118/200, Loss: 18992.1680\n","Epoch 119/200, Loss: 18737.8105\n","Epoch 120/200, Loss: 18490.2461\n","Epoch 121/200, Loss: 18250.1562\n","Epoch 122/200, Loss: 18017.6680\n","Epoch 123/200, Loss: 17792.3457\n","Epoch 124/200, Loss: 17573.5000\n","Epoch 125/200, Loss: 17360.6152\n","Epoch 126/200, Loss: 17153.1270\n","Epoch 127/200, Loss: 16950.0996\n","Epoch 128/200, Loss: 16750.5918\n","Epoch 129/200, Loss: 16554.3926\n","Epoch 130/200, Loss: 16361.8965\n","Epoch 131/200, Loss: 16173.4922\n","Epoch 132/200, Loss: 15989.2266\n","Epoch 133/200, Loss: 15808.9131\n","Epoch 134/200, Loss: 15632.2549\n","Epoch 135/200, Loss: 15458.9541\n","Epoch 136/200, Loss: 15288.8477\n","Epoch 137/200, Loss: 15121.9639\n","Epoch 138/200, Loss: 14958.4229\n","Epoch 139/200, Loss: 14798.3320\n","Epoch 140/200, Loss: 14641.7598\n","Epoch 141/200, Loss: 14488.6709\n","Epoch 142/200, Loss: 14338.8359\n","Epoch 143/200, Loss: 14191.9453\n","Epoch 144/200, Loss: 14047.8545\n","Epoch 145/200, Loss: 13906.6152\n","Epoch 146/200, Loss: 13768.2539\n","Epoch 147/200, Loss: 13632.6572\n","Epoch 148/200, Loss: 13499.6143\n","Epoch 149/200, Loss: 13368.9805\n","Epoch 150/200, Loss: 13240.6631\n","Epoch 151/200, Loss: 13114.6094\n","Epoch 152/200, Loss: 12990.8145\n","Epoch 153/200, Loss: 12869.2852\n","Epoch 154/200, Loss: 12750.0078\n","Epoch 155/200, Loss: 12632.9160\n","Epoch 156/200, Loss: 12517.9258\n","Epoch 157/200, Loss: 12404.9639\n","Epoch 158/200, Loss: 12294.0127\n","Epoch 159/200, Loss: 12185.0859\n","Epoch 160/200, Loss: 12078.1631\n","Epoch 161/200, Loss: 11973.1680\n","Epoch 162/200, Loss: 11870.0000\n","Epoch 163/200, Loss: 11768.5840\n","Epoch 164/200, Loss: 11668.8877\n","Epoch 165/200, Loss: 11570.8867\n","Epoch 166/200, Loss: 11474.5254\n","Epoch 167/200, Loss: 11379.7246\n","Epoch 168/200, Loss: 11286.4268\n","Epoch 169/200, Loss: 11194.6035\n","Epoch 170/200, Loss: 11104.2188\n","Epoch 171/200, Loss: 11015.2275\n","Epoch 172/200, Loss: 10927.5889\n","Epoch 173/200, Loss: 10841.2588\n","Epoch 174/200, Loss: 10756.1904\n","Epoch 175/200, Loss: 10672.3398\n","Epoch 176/200, Loss: 10589.6748\n","Epoch 177/200, Loss: 10508.1709\n","Epoch 178/200, Loss: 10427.7969\n","Epoch 179/200, Loss: 10348.5186\n","Epoch 180/200, Loss: 10270.3154\n","Epoch 181/200, Loss: 10193.1895\n","Epoch 182/200, Loss: 10117.1660\n","Epoch 183/200, Loss: 10042.2715\n","Epoch 184/200, Loss: 9968.5381\n","Epoch 185/200, Loss: 9896.0156\n","Epoch 186/200, Loss: 9824.7305\n","Epoch 187/200, Loss: 9754.6768\n","Epoch 188/200, Loss: 9685.8164\n","Epoch 189/200, Loss: 9618.0967\n","Epoch 190/200, Loss: 9551.4570\n","Epoch 191/200, Loss: 9485.8428\n","Epoch 192/200, Loss: 9421.2070\n","Epoch 193/200, Loss: 9357.5127\n","Epoch 194/200, Loss: 9294.7295\n","Epoch 195/200, Loss: 9232.8203\n","Epoch 196/200, Loss: 9171.7578\n","Epoch 197/200, Loss: 9111.5137\n","Epoch 198/200, Loss: 9052.0703\n","Epoch 199/200, Loss: 8993.4043\n","Epoch 200/200, Loss: 8935.5127\n","PCA refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_PCA/matrix_factorization_default_loss_normalizing_flow_PCA_refined_embeddings.pt\n","Epoch 1/200, Loss: 1729079.7500\n","Epoch 2/200, Loss: 1269361.7500\n","Epoch 3/200, Loss: 1074794.2500\n","Epoch 4/200, Loss: 965479.5000\n","Epoch 5/200, Loss: 857789.2500\n","Epoch 6/200, Loss: 751496.5000\n","Epoch 7/200, Loss: 628635.3750\n","Epoch 8/200, Loss: 575189.8125\n","Epoch 9/200, Loss: 540162.9375\n","Epoch 10/200, Loss: 488456.0938\n","Epoch 11/200, Loss: 459694.4688\n","Epoch 12/200, Loss: 449375.0000\n","Epoch 13/200, Loss: 415592.5000\n","Epoch 14/200, Loss: 390766.3125\n","Epoch 15/200, Loss: 368766.2812\n","Epoch 16/200, Loss: 349098.9688\n","Epoch 17/200, Loss: 332017.5938\n","Epoch 18/200, Loss: 316723.5938\n","Epoch 19/200, Loss: 302317.8125\n","Epoch 20/200, Loss: 288276.3750\n","Epoch 21/200, Loss: 274500.5312\n","Epoch 22/200, Loss: 261038.2969\n","Epoch 23/200, Loss: 247624.7188\n","Epoch 24/200, Loss: 233180.6250\n","Epoch 25/200, Loss: 219732.7031\n","Epoch 26/200, Loss: 206750.2500\n","Epoch 27/200, Loss: 197098.7812\n","Epoch 28/200, Loss: 188452.6250\n","Epoch 29/200, Loss: 179932.4844\n","Epoch 30/200, Loss: 171514.6406\n","Epoch 31/200, Loss: 164366.2969\n","Epoch 32/200, Loss: 157405.9219\n","Epoch 33/200, Loss: 150652.1406\n","Epoch 34/200, Loss: 144091.1875\n","Epoch 35/200, Loss: 137732.9062\n","Epoch 36/200, Loss: 131878.7031\n","Epoch 37/200, Loss: 126356.6875\n","Epoch 38/200, Loss: 121095.7188\n","Epoch 39/200, Loss: 116108.7031\n","Epoch 40/200, Loss: 111570.0000\n","Epoch 41/200, Loss: 107226.4844\n","Epoch 42/200, Loss: 103056.3359\n","Epoch 43/200, Loss: 99171.6719\n","Epoch 44/200, Loss: 95610.7344\n","Epoch 45/200, Loss: 92332.8594\n","Epoch 46/200, Loss: 89287.8906\n","Epoch 47/200, Loss: 86436.2188\n","Epoch 48/200, Loss: 83686.1250\n","Epoch 49/200, Loss: 80969.9219\n","Epoch 50/200, Loss: 78286.3906\n","Epoch 51/200, Loss: 75665.6250\n","Epoch 52/200, Loss: 73122.6562\n","Epoch 53/200, Loss: 70650.6875\n","Epoch 54/200, Loss: 68278.5469\n","Epoch 55/200, Loss: 66032.1719\n","Epoch 56/200, Loss: 63903.3477\n","Epoch 57/200, Loss: 61912.3945\n","Epoch 58/200, Loss: 60011.9414\n","Epoch 59/200, Loss: 58107.3555\n","Epoch 60/200, Loss: 56300.2188\n","Epoch 61/200, Loss: 54520.9023\n","Epoch 62/200, Loss: 52711.2383\n","Epoch 63/200, Loss: 50970.0469\n","Epoch 64/200, Loss: 49512.9688\n","Epoch 65/200, Loss: 48227.9961\n","Epoch 66/200, Loss: 47020.4219\n","Epoch 67/200, Loss: 45882.2031\n","Epoch 68/200, Loss: 44753.2812\n","Epoch 69/200, Loss: 43619.6289\n","Epoch 70/200, Loss: 42507.7773\n","Epoch 71/200, Loss: 41419.2734\n","Epoch 72/200, Loss: 40364.6250\n","Epoch 73/200, Loss: 39372.5312\n","Epoch 74/200, Loss: 38441.9258\n","Epoch 75/200, Loss: 37558.5469\n","Epoch 76/200, Loss: 36725.7539\n","Epoch 77/200, Loss: 35931.5156\n","Epoch 78/200, Loss: 35162.0234\n","Epoch 79/200, Loss: 34424.8867\n","Epoch 80/200, Loss: 33711.4453\n","Epoch 81/200, Loss: 33019.6836\n","Epoch 82/200, Loss: 32354.8418\n","Epoch 83/200, Loss: 31709.5078\n","Epoch 84/200, Loss: 31081.6172\n","Epoch 85/200, Loss: 30475.1504\n","Epoch 86/200, Loss: 29885.5156\n","Epoch 87/200, Loss: 29311.1289\n","Epoch 88/200, Loss: 28754.8945\n","Epoch 89/200, Loss: 28213.1367\n","Epoch 90/200, Loss: 27683.5391\n","Epoch 91/200, Loss: 27168.1055\n","Epoch 92/200, Loss: 26666.0156\n","Epoch 93/200, Loss: 26176.6797\n","Epoch 94/200, Loss: 25698.3984\n","Epoch 95/200, Loss: 25222.3105\n","Epoch 96/200, Loss: 24744.2773\n","Epoch 97/200, Loss: 24283.6523\n","Epoch 98/200, Loss: 23864.8809\n","Epoch 99/200, Loss: 23471.5195\n","Epoch 100/200, Loss: 23086.2910\n","Epoch 101/200, Loss: 22707.4648\n","Epoch 102/200, Loss: 22335.4727\n","Epoch 103/200, Loss: 21970.8633\n","Epoch 104/200, Loss: 21613.3086\n","Epoch 105/200, Loss: 21262.4492\n","Epoch 106/200, Loss: 20919.8320\n","Epoch 107/200, Loss: 20588.4941\n","Epoch 108/200, Loss: 20272.1270\n","Epoch 109/200, Loss: 19967.5312\n","Epoch 110/200, Loss: 19666.4375\n","Epoch 111/200, Loss: 19371.8691\n","Epoch 112/200, Loss: 19087.2324\n","Epoch 113/200, Loss: 18811.8652\n","Epoch 114/200, Loss: 18546.0430\n","Epoch 115/200, Loss: 18292.3867\n","Epoch 116/200, Loss: 18051.0723\n","Epoch 117/200, Loss: 17816.7227\n","Epoch 118/200, Loss: 17583.8086\n","Epoch 119/200, Loss: 17351.0234\n","Epoch 120/200, Loss: 17120.6172\n","Epoch 121/200, Loss: 16896.7012\n","Epoch 122/200, Loss: 16682.3027\n","Epoch 123/200, Loss: 16476.0645\n","Epoch 124/200, Loss: 16273.7061\n","Epoch 125/200, Loss: 16073.8555\n","Epoch 126/200, Loss: 15877.9326\n","Epoch 127/200, Loss: 15686.4023\n","Epoch 128/200, Loss: 15498.8008\n","Epoch 129/200, Loss: 15314.8877\n","Epoch 130/200, Loss: 15134.8193\n","Epoch 131/200, Loss: 14958.9785\n","Epoch 132/200, Loss: 14787.5879\n","Epoch 133/200, Loss: 14620.3809\n","Epoch 134/200, Loss: 14456.5010\n","Epoch 135/200, Loss: 14294.8496\n","Epoch 136/200, Loss: 14135.3115\n","Epoch 137/200, Loss: 13979.0918\n","Epoch 138/200, Loss: 13827.0576\n","Epoch 139/200, Loss: 13678.8662\n","Epoch 140/200, Loss: 13533.6719\n","Epoch 141/200, Loss: 13390.9814\n","Epoch 142/200, Loss: 13250.8604\n","Epoch 143/200, Loss: 13113.5928\n","Epoch 144/200, Loss: 12979.1807\n","Epoch 145/200, Loss: 12847.3379\n","Epoch 146/200, Loss: 12718.1055\n","Epoch 147/200, Loss: 12591.7930\n","Epoch 148/200, Loss: 12468.2969\n","Epoch 149/200, Loss: 12347.1289\n","Epoch 150/200, Loss: 12227.9580\n","Epoch 151/200, Loss: 12110.8643\n","Epoch 152/200, Loss: 11996.0781\n","Epoch 153/200, Loss: 11883.5645\n","Epoch 154/200, Loss: 11773.1113\n","Epoch 155/200, Loss: 11664.7139\n","Epoch 156/200, Loss: 11558.4287\n","Epoch 157/200, Loss: 11454.0859\n","Epoch 158/200, Loss: 11351.4814\n","Epoch 159/200, Loss: 11250.6113\n","Epoch 160/200, Loss: 11151.6025\n","Epoch 161/200, Loss: 11054.4502\n","Epoch 162/200, Loss: 10959.0186\n","Epoch 163/200, Loss: 10865.2744\n","Epoch 164/200, Loss: 10773.2354\n","Epoch 165/200, Loss: 10682.7891\n","Epoch 166/200, Loss: 10593.8076\n","Epoch 167/200, Loss: 10506.3105\n","Epoch 168/200, Loss: 10420.3359\n","Epoch 169/200, Loss: 10335.8193\n","Epoch 170/200, Loss: 10252.6895\n","Epoch 171/200, Loss: 10170.9512\n","Epoch 172/200, Loss: 10090.5752\n","Epoch 173/200, Loss: 10011.4775\n","Epoch 174/200, Loss: 9933.6318\n","Epoch 175/200, Loss: 9857.0508\n","Epoch 176/200, Loss: 9781.7051\n","Epoch 177/200, Loss: 9707.5400\n","Epoch 178/200, Loss: 9634.5430\n","Epoch 179/200, Loss: 9562.6924\n","Epoch 180/200, Loss: 9491.9375\n","Epoch 181/200, Loss: 9422.2441\n","Epoch 182/200, Loss: 9353.6221\n","Epoch 183/200, Loss: 9286.0420\n","Epoch 184/200, Loss: 9219.4717\n","Epoch 185/200, Loss: 9153.8994\n","Epoch 186/200, Loss: 9089.3047\n","Epoch 187/200, Loss: 9025.6475\n","Epoch 188/200, Loss: 8962.9111\n","Epoch 189/200, Loss: 8901.0820\n","Epoch 190/200, Loss: 8840.1367\n","Epoch 191/200, Loss: 8780.0547\n","Epoch 192/200, Loss: 8720.8291\n","Epoch 193/200, Loss: 8662.4316\n","Epoch 194/200, Loss: 8604.8438\n","Epoch 195/200, Loss: 8548.0508\n","Epoch 196/200, Loss: 8492.0332\n","Epoch 197/200, Loss: 8436.7705\n","Epoch 198/200, Loss: 8382.2559\n","Epoch 199/200, Loss: 8328.4707\n","Epoch 200/200, Loss: 8275.3965\n","SVD refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_SVD/matrix_factorization_default_loss_normalizing_flow_SVD_refined_embeddings.pt\n","Epoch 1/200, Loss: 3338.7485\n","Epoch 2/200, Loss: 3224.9641\n","Epoch 3/200, Loss: 3159.9268\n","Epoch 4/200, Loss: 3114.6672\n","Epoch 5/200, Loss: 3082.0623\n","Epoch 6/200, Loss: 3058.4419\n","Epoch 7/200, Loss: 3041.1448\n","Epoch 8/200, Loss: 3028.2073\n","Epoch 9/200, Loss: 3018.2388\n","Epoch 10/200, Loss: 3010.2837\n","Epoch 11/200, Loss: 3003.7068\n","Epoch 12/200, Loss: 2998.1006\n","Epoch 13/200, Loss: 2993.2102\n","Epoch 14/200, Loss: 2988.8782\n","Epoch 15/200, Loss: 2985.0061\n","Epoch 16/200, Loss: 2981.5303\n","Epoch 17/200, Loss: 2978.4077\n","Epoch 18/200, Loss: 2975.6042\n","Epoch 19/200, Loss: 2973.0889\n","Epoch 20/200, Loss: 2970.8335\n","Epoch 21/200, Loss: 2968.8108\n","Epoch 22/200, Loss: 2966.9912\n","Epoch 23/200, Loss: 2965.3506\n","Epoch 24/200, Loss: 2963.8628\n","Epoch 25/200, Loss: 2962.5071\n","Epoch 26/200, Loss: 2961.2622\n","Epoch 27/200, Loss: 2960.1118\n","Epoch 28/200, Loss: 2959.0422\n","Epoch 29/200, Loss: 2958.0398\n","Epoch 30/200, Loss: 2957.0974\n","Epoch 31/200, Loss: 2956.2058\n","Epoch 32/200, Loss: 2955.3608\n","Epoch 33/200, Loss: 2954.5566\n","Epoch 34/200, Loss: 2953.7896\n","Epoch 35/200, Loss: 2953.0562\n","Epoch 36/200, Loss: 2952.3540\n","Epoch 37/200, Loss: 2951.6809\n","Epoch 38/200, Loss: 2951.0344\n","Epoch 39/200, Loss: 2950.4111\n","Epoch 40/200, Loss: 2949.8086\n","Epoch 41/200, Loss: 2949.2263\n","Epoch 42/200, Loss: 2948.6609\n","Epoch 43/200, Loss: 2948.1123\n","Epoch 44/200, Loss: 2947.5759\n","Epoch 45/200, Loss: 2947.0527\n","Epoch 46/200, Loss: 2946.5405\n","Epoch 47/200, Loss: 2946.0378\n","Epoch 48/200, Loss: 2945.5442\n","Epoch 49/200, Loss: 2945.0588\n","Epoch 50/200, Loss: 2944.5796\n","Epoch 51/200, Loss: 2944.1072\n","Epoch 52/200, Loss: 2943.6406\n","Epoch 53/200, Loss: 2943.1775\n","Epoch 54/200, Loss: 2942.7190\n","Epoch 55/200, Loss: 2942.2634\n","Epoch 56/200, Loss: 2941.8105\n","Epoch 57/200, Loss: 2941.3596\n","Epoch 58/200, Loss: 2940.9097\n","Epoch 59/200, Loss: 2940.4614\n","Epoch 60/200, Loss: 2940.0139\n","Epoch 61/200, Loss: 2939.5664\n","Epoch 62/200, Loss: 2939.1187\n","Epoch 63/200, Loss: 2938.6714\n","Epoch 64/200, Loss: 2938.2236\n","Epoch 65/200, Loss: 2937.7749\n","Epoch 66/200, Loss: 2937.3264\n","Epoch 67/200, Loss: 2936.8760\n","Epoch 68/200, Loss: 2936.4248\n","Epoch 69/200, Loss: 2935.9724\n","Epoch 70/200, Loss: 2935.5181\n","Epoch 71/200, Loss: 2935.0623\n","Epoch 72/200, Loss: 2934.6050\n","Epoch 73/200, Loss: 2934.1448\n","Epoch 74/200, Loss: 2933.6826\n","Epoch 75/200, Loss: 2933.2178\n","Epoch 76/200, Loss: 2932.7505\n","Epoch 77/200, Loss: 2932.2800\n","Epoch 78/200, Loss: 2931.8066\n","Epoch 79/200, Loss: 2931.3306\n","Epoch 80/200, Loss: 2930.8503\n","Epoch 81/200, Loss: 2930.3669\n","Epoch 82/200, Loss: 2929.8794\n","Epoch 83/200, Loss: 2929.3879\n","Epoch 84/200, Loss: 2928.8936\n","Epoch 85/200, Loss: 2928.3940\n","Epoch 86/200, Loss: 2927.8906\n","Epoch 87/200, Loss: 2927.3818\n","Epoch 88/200, Loss: 2926.8691\n","Epoch 89/200, Loss: 2926.3506\n","Epoch 90/200, Loss: 2925.8281\n","Epoch 91/200, Loss: 2925.2993\n","Epoch 92/200, Loss: 2924.7659\n","Epoch 93/200, Loss: 2924.2266\n","Epoch 94/200, Loss: 2923.6812\n","Epoch 95/200, Loss: 2923.1299\n","Epoch 96/200, Loss: 2922.5732\n","Epoch 97/200, Loss: 2922.0093\n","Epoch 98/200, Loss: 2921.4397\n","Epoch 99/200, Loss: 2920.8638\n","Epoch 100/200, Loss: 2920.2810\n","Epoch 101/200, Loss: 2919.6919\n","Epoch 102/200, Loss: 2919.0950\n","Epoch 103/200, Loss: 2918.4922\n","Epoch 104/200, Loss: 2917.8816\n","Epoch 105/200, Loss: 2917.2639\n","Epoch 106/200, Loss: 2916.6399\n","Epoch 107/200, Loss: 2916.0076\n","Epoch 108/200, Loss: 2915.3687\n","Epoch 109/200, Loss: 2914.7219\n","Epoch 110/200, Loss: 2914.0686\n","Epoch 111/200, Loss: 2913.4082\n","Epoch 112/200, Loss: 2912.7397\n","Epoch 113/200, Loss: 2912.0645\n","Epoch 114/200, Loss: 2911.3826\n","Epoch 115/200, Loss: 2910.6938\n","Epoch 116/200, Loss: 2909.9980\n","Epoch 117/200, Loss: 2909.2957\n","Epoch 118/200, Loss: 2908.5869\n","Epoch 119/200, Loss: 2907.8721\n","Epoch 120/200, Loss: 2907.1504\n","Epoch 121/200, Loss: 2906.4238\n","Epoch 122/200, Loss: 2905.6909\n","Epoch 123/200, Loss: 2904.9536\n","Epoch 124/200, Loss: 2904.2100\n","Epoch 125/200, Loss: 2903.4624\n","Epoch 126/200, Loss: 2902.7095\n","Epoch 127/200, Loss: 2901.9531\n","Epoch 128/200, Loss: 2901.1919\n","Epoch 129/200, Loss: 2900.4272\n","Epoch 130/200, Loss: 2899.6587\n","Epoch 131/200, Loss: 2898.8879\n","Epoch 132/200, Loss: 2898.1133\n","Epoch 133/200, Loss: 2897.3367\n","Epoch 134/200, Loss: 2896.5574\n","Epoch 135/200, Loss: 2895.7764\n","Epoch 136/200, Loss: 2894.9939\n","Epoch 137/200, Loss: 2894.2095\n","Epoch 138/200, Loss: 2893.4243\n","Epoch 139/200, Loss: 2892.6387\n","Epoch 140/200, Loss: 2891.8516\n","Epoch 141/200, Loss: 2891.0645\n","Epoch 142/200, Loss: 2890.2778\n","Epoch 143/200, Loss: 2889.4912\n","Epoch 144/200, Loss: 2888.7056\n","Epoch 145/200, Loss: 2887.9209\n","Epoch 146/200, Loss: 2887.1377\n","Epoch 147/200, Loss: 2886.3555\n","Epoch 148/200, Loss: 2885.5757\n","Epoch 149/200, Loss: 2884.7979\n","Epoch 150/200, Loss: 2884.0222\n","Epoch 151/200, Loss: 2883.2500\n","Epoch 152/200, Loss: 2882.4807\n","Epoch 153/200, Loss: 2881.7148\n","Epoch 154/200, Loss: 2880.9521\n","Epoch 155/200, Loss: 2880.1934\n","Epoch 156/200, Loss: 2879.4382\n","Epoch 157/200, Loss: 2878.6880\n","Epoch 158/200, Loss: 2877.9419\n","Epoch 159/200, Loss: 2877.2012\n","Epoch 160/200, Loss: 2876.4644\n","Epoch 161/200, Loss: 2875.7327\n","Epoch 162/200, Loss: 2875.0054\n","Epoch 163/200, Loss: 2874.2842\n","Epoch 164/200, Loss: 2873.5676\n","Epoch 165/200, Loss: 2872.8564\n","Epoch 166/200, Loss: 2872.1504\n","Epoch 167/200, Loss: 2871.4492\n","Epoch 168/200, Loss: 2870.7532\n","Epoch 169/200, Loss: 2870.0635\n","Epoch 170/200, Loss: 2869.3779\n","Epoch 171/200, Loss: 2868.6980\n","Epoch 172/200, Loss: 2868.0234\n","Epoch 173/200, Loss: 2867.3540\n","Epoch 174/200, Loss: 2866.6899\n","Epoch 175/200, Loss: 2866.0300\n","Epoch 176/200, Loss: 2865.3774\n","Epoch 177/200, Loss: 2864.7358\n","Epoch 178/200, Loss: 2864.1416\n","Epoch 179/200, Loss: 2863.5303\n","Epoch 180/200, Loss: 2862.8442\n","Epoch 181/200, Loss: 2862.2310\n","Epoch 182/200, Loss: 2861.6394\n","Epoch 183/200, Loss: 2861.0059\n","Epoch 184/200, Loss: 2860.3923\n","Epoch 185/200, Loss: 2859.8110\n","Epoch 186/200, Loss: 2859.2126\n","Epoch 187/200, Loss: 2858.6064\n","Epoch 188/200, Loss: 2858.0244\n","Epoch 189/200, Loss: 2857.4514\n","Epoch 190/200, Loss: 2856.8811\n","Epoch 191/200, Loss: 2856.3003\n","Epoch 192/200, Loss: 2855.7144\n","Epoch 193/200, Loss: 2855.1489\n","Epoch 194/200, Loss: 2854.6125\n","Epoch 195/200, Loss: 2854.0796\n","Epoch 196/200, Loss: 2853.5332\n","Epoch 197/200, Loss: 2852.9800\n","Epoch 198/200, Loss: 2852.4272\n","Epoch 199/200, Loss: 2851.8811\n","Epoch 200/200, Loss: 2851.3469\n","NMF refined embeddings (Normalizing Flow) saved in PyTorch format: ./saved_embeddings/embeddings/normalizing_flow_NMF/matrix_factorization_default_loss_normalizing_flow_NMF_refined_embeddings.pt\n","Feature extraction and normalizing flow processing complete!\n"]}]}]}