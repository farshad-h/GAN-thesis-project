# -*- coding: utf-8 -*-
"""embedding_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19GQzCKFbe7MFGcFnEPbvWYKZvE5WK80c
"""

# Embedding Generation and Evaluation Notebook

# Import necessary libraries
import torch
from src.data_utils import load_mnist_data
from src.embedding_models import BasicAutoencoder, IntermediateAutoencoder, AdvancedAutoencoder, EnhancedAutoencoder
from src.evaluation import evaluate_clustering_metrics, evaluate_embedding_quality, visualize_embeddings, overlay_clusters

# Load the dataset
data_loader = load_mnist_data(fraction=0.5, batch_size=64, shuffle=True)

# Define model parameters
code_dim = 50
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize and train Basic Autoencoder
basic_autoencoder = BasicAutoencoder(code_dim=code_dim).to(device)

# Define training parameters
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(basic_autoencoder.parameters(), lr=1e-3)

# Training loop for Basic Autoencoder
num_epochs = 10
basic_autoencoder.train()
for epoch in range(num_epochs):
    total_loss = 0
    for images, _ in data_loader:
        images = images.to(device).float()

        # Forward pass
        encoded, decoded = basic_autoencoder(images)
        loss = criterion(decoded, images)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(data_loader):.4f}")

# Save the embeddings
basic_autoencoder.eval()
with torch.no_grad():
    all_embeddings = []
    all_labels = []
    for images, labels in data_loader:
        images = images.to(device).float()
        encoded, _ = basic_autoencoder(images)
        all_embeddings.append(encoded.cpu())
        all_labels.append(labels.cpu())

all_embeddings = torch.cat(all_embeddings, dim=0)
all_labels = torch.cat(all_labels, dim=0)

# Save embeddings to file
embedding_path = "data/embeddings/basic_autoencoder_embeddings.pt"
torch.save({"embeddings": all_embeddings, "labels": all_labels}, embedding_path)
print(f"Embeddings saved to {embedding_path}")

# Evaluate the generated embeddings
print("\nEvaluating Embeddings...")
all_embeddings_np = all_embeddings.numpy()
all_labels_np = all_labels.numpy()

# Clustering Metrics
evaluation_results = evaluate_clustering_metrics(all_embeddings_np, all_labels_np, n_clusters=10)
print("Clustering Metrics:")
for metric, value in evaluation_results.items():
    print(f"{metric}: {value:.4f}")

# k-NN Accuracy
knn_accuracy = evaluate_embedding_quality(all_embeddings_np, all_labels_np, k=5)
print(f"k-NN Accuracy: {knn_accuracy:.4f}")

# Visualization
print("\nVisualizing Embeddings...")
visualize_embeddings(all_embeddings_np, all_labels_np, method="tsne")

# Overlay Clusters
print("\nOverlaying Clusters...")
overlay_clusters(all_embeddings_np, all_labels_np, n_clusters=10, method="tsne")