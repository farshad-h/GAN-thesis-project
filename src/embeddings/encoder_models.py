# -*- coding: utf-8 -*-
"""encoder.models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VTayE4pKmLCdgr62el0Omhq8uX51x5mR
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.decomposition import PCA, TruncatedSVD, NMF, KernelPCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler

def init_weights(m):
    """
    Initializes the weights of the given layer.

    Method:
    - Uses Xavier/Glorot initialization for Conv2d, ConvTranspose2d, and Linear layers.
    - Sets bias to zero if it exists.

    Args:
        m (nn.Module): Layer to initialize.
    """
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):
        # Xavier/Glorot weight initialization for better convergence
        nn.init.xavier_uniform_(m.weight)
        # Initialize bias to zero if it exists
        if m.bias is not None:
            nn.init.zeros_(m.bias)

class BasicAutoencoder(nn.Module):
    """
    A simple autoencoder for learning compact embeddings.

    Architecture:
    - Encoder: Two convolutional layers followed by max-pooling.
    - Decoder: Two transposed convolutional layers to reconstruct the input.

    Suitable for grayscale datasets like MNIST.
    """
    def init(self, code_dim):
        super(BasicAutoencoder, self).init()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Input: (1, 28, 28)
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # Output: (16, 14, 14)
            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),  # Output: (8, 14, 14)
            nn.ReLU(),
            nn.MaxPool2d(2, 2)  # Output: (8, 7, 7)
        )
        self.fc_encoder = nn.Linear(8 * 7 * 7, code_dim)  # Flatten to embedding dimension

        # Decoder
        self.fc_decoder = nn.Linear(code_dim, 8 * 7 * 7)  # Unflatten to feature map
        self.decoder = nn.Sequential(
            nn.ReLU(),
            nn.Unflatten(1, (8, 7, 7)),
            nn.ConvTranspose2d(8, 8, kernel_size=3, stride=1, padding=1),  # Learned upsampling
            nn.ReLU(),
            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=1, padding=1),  # Learned upsampling
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=1),  # Output: (1, 28, 28)
            nn.Tanh()  # Outputs pixel values in range [-1, 1]
        )

    def forward(self, x):
        batch_size = x.size(0)
        encoded = self.encoder(x)
        flattened_dim = encoded.size(1) * encoded.size(2) * encoded.size(3)
        encoded = encoded.view(batch_size, flattened_dim)
        encoded = self.fc_encoder(encoded)

        decoded = self.fc_decoder(encoded)
        decoded = self.decoder(decoded)
        return encoded, decoded

class IntermediateAutoencoder(nn.Module):
    """
    An autoencoder with intermediate complexity.

    Features:
    - Uses Batch Normalization to improve stability during training.
    - Deeper architecture compared to BasicAutoencoder, with more feature maps.

    Designed for more expressive embeddings while retaining simplicity.
    """
    def __init__(self, code_dim):
        super(IntermediateAutoencoder, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2, 2),
        )
        self.fc_encoder = nn.Linear(64 * 7 * 7, code_dim)

        # Decoder
        self.fc_decoder = nn.Linear(code_dim, 64 * 7 * 7)
        self.decoder = nn.Sequential(
            nn.ReLU(),
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )

    def forward(self, x):
        batch_size = x.size(0)
        encoded = self.encoder(x)
        encoded = encoded.view(batch_size, -1)
        encoded = self.fc_encoder(encoded)
        decoded = self.fc_decoder(encoded)
        decoded = self.decoder(decoded)
        return encoded, decoded

class AdvancedAutoencoder(nn.Module):
    """
    A more advanced autoencoder with skip connections.

    Features:
    - Skip connections between encoder and decoder layers for better gradient flow.
    - LeakyReLU activations and Batch Normalization for improved performance.

    Suitable for complex embedding tasks requiring detailed reconstruction.
    """

    def __init__(self, code_dim):
        super(AdvancedAutoencoder, self).__init__()

        # Encoder with Skip Connections
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2, 2),
        )
        self.fc_encoder = nn.Linear(64 * 7 * 7, code_dim)

        # Decoder (with transposed convolutions and skip connections)
        self.fc_decoder = nn.Linear(code_dim, 64 * 7 * 7)
        self.decoder = nn.Sequential(
            nn.ReLU(),
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(32),
            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Tanh()
        )

    def forward(self, x):
        batch_size = x.size(0)
        encoded = self.encoder(x)
        encoded = encoded.view(batch_size, -1)
        encoded = self.fc_encoder(encoded)
        decoded = self.fc_decoder(encoded)
        decoded = self.decoder(decoded)
        return encoded, decoded

class AdvancedAutoencoder(nn.Module):
    """
    A more advanced autoencoder with skip connections.

    Features:
    - Skip connections between encoder and decoder layers for better gradient flow.
    - LeakyReLU activations and Batch Normalization for improved performance.

    Suitable for complex embedding tasks requiring detailed reconstruction.
    """
    def __init__(self, code_dim):
        super(AdvancedAutoencoder, self).__init__()

        # Encoder with Skip Connections
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Input: (1, 28, 28)
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2, 2),  # Output: (32, 14, 14)
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 14, 14)
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2, 2),  # Output: (64, 7, 7)
        )
        self.fc_encoder = nn.Linear(64 * 7 * 7, code_dim)

        # Decoder with Skip Connections (actual skip connections)
        self.fc_decoder = nn.Linear(code_dim, 64 * 7 * 7)  # Unflatten to encoder's feature size
        self.decoder_conv1 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)  # Learned upsampling
        self.decoder_conv2 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)  # Output: (1, 28, 28)

        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.2)
        self.batch_norm = nn.BatchNorm2d(32)

    def forward(self, x):
        # Encoder
        batch_size = x.size(0)
        encoded = self.encoder(x)
        encoded = encoded.view(batch_size, -1)
        encoded = self.fc_encoder(encoded)

        # Decoder with Skip Connections
        decoded_fc = self.fc_decoder(encoded)  # Get fully connected output
        decoded_fc = decoded_fc.view(batch_size, 64, 7, 7)  # Unflatten back to feature map size
        
        # Skip connection from encoder (we'll add the encoder output directly)
        decoded = self.decoder_conv1(decoded_fc)  # First layer of decoder
        
        # Add skip connection from encoder
        # Here, we're directly adding the output of the encoder feature map to the decoder's output
        skip_connection = encoded.view(batch_size, 64, 7, 7)  # Recreate feature map from encoded code
        decoded = decoded + skip_connection  # Skip connection added

        decoded = self.leaky_relu(decoded)
        decoded = self.batch_norm(decoded)
        decoded = self.decoder_conv2(decoded)  # Output: (1, 28, 28)
        decoded = torch.tanh(decoded)

        return encoded, decoded

class EnhancedAutoencoder(nn.Module):
    """
    A deep autoencoder with advanced reconstruction capabilities.

    Features:
    - Deeper architecture with additional convolutional and transposed convolutional layers.
    - Utilizes Batch Normalization and LeakyReLU activations.
    - Capable of learning highly expressive embeddings.

    Designed for datasets requiring intricate reconstructions.
    """
    def __init__(self, code_dim):
        super(EnhancedAutoencoder, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(128),
            nn.MaxPool2d(2, 2)
        )

        # Code layer (fully connected)
        self.fc_encoder = nn.Linear(128 * 3 * 3, code_dim)

        # Decoder (with skip connections)
        self.fc_decoder = nn.Linear(code_dim, 128 * 3 * 3)
        self.decoder = nn.Sequential(
            nn.ReLU(),
            nn.Unflatten(1, (128, 3, 3)),

            # First transposed convolution (upsample)
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(64),

            # Second transposed convolution (upsample)
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.LeakyReLU(0.2),
            nn.BatchNorm2d(32),

            # Final transposed convolution (output layer)
            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Tanh()  # For MNIST, Tanh is typically used to normalize outputs between [-1, 1]
        )

    def forward(self, x):
        batch_size = x.size(0)

        # Encoding
        encoded = self.encoder(x)
        encoded = encoded.view(batch_size, -1)  # Flatten before the fully connected layer
        encoded = self.fc_encoder(encoded)

        # Decoding
        decoded = self.fc_decoder(encoded)
        decoded = self.decoder(decoded)

        return encoded, decoded

# Basic VAE for MNIST (Convolutional Encoder/Decoder)
class BasicVAE(nn.Module):
    """
    A simple Variational Autoencoder (VAE) for grayscale datasets.

    Features:
    - Encoder: Two convolutional layers followed by a fully connected layer to parameterize the latent space.
    - Decoder: Fully connected layers followed by transposed convolution layers to reconstruct input images.

    Methods:
        reparameterize(mu, logvar): Applies the reparameterization trick to sample latent variables.
        forward(x): Encodes the input, samples latent variables, and reconstructs the input.

    Args:
        input_dim (int): Number of input channels (e.g., 1 for grayscale images).
        code_dim (int): Dimensionality of the latent space.
    """

    def __init__(self, input_dim, code_dim):
        super(BasicVAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(16),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(8),
            nn.MaxPool2d(2, 2),
            nn.Flatten()
        )

        self.fc_mu = nn.Linear(8 * 7 * 7, code_dim)
        self.fc_logvar = nn.Linear(8 * 7 * 7, code_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(code_dim, 8 * 7 * 7),
            nn.ReLU(),
            nn.Unflatten(1, (8, 7, 7)),
            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),
            nn.Tanh()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        encoded = self.encoder(x)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        z = self.reparameterize(mu, logvar)
        decoded = self.decoder(z)
        return mu, logvar, decoded


# VAE with a fully connected decoder
class VAEWithFCDecoder(nn.Module):
    def __init__(self, input_dim, code_dim):
        super(VAEWithFCDecoder, self).__init__()

        self.encoder = nn.Sequential(
            nn.Conv2d(input_dim, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(16),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(8),
            nn.MaxPool2d(2, 2),
            nn.Flatten()
        )

        self.fc_mu = nn.Linear(8 * 7 * 7, code_dim)
        self.fc_logvar = nn.Linear(8 * 7 * 7, code_dim)

        # Decoder (with separate fully connected layers)
        self.fc_decoder = nn.Linear(code_dim, 8 * 7 * 7)
        self.decoder = nn.Sequential(
            nn.ReLU(),
            nn.Unflatten(1, (8, 7, 7)),
            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),
            nn.Tanh()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        encoded = self.encoder(x)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        z = self.reparameterize(mu, logvar)
        decoded = self.fc_decoder(z)
        decoded = self.decoder(decoded)
        return mu, logvar, decoded


# Improved VAE with Bottleneck Layer
class ImprovedVAE(nn.Module):
    """
    An improved VAE with a bottleneck layer and batch normalization.

    Features:
    - Bottleneck layer for enhanced feature extraction.
    - Uses transposed convolutions in the decoder for smooth reconstructions.
    - KL divergence loss for regularizing the latent space.

    Methods:
        reparameterize(mu, logvar): Applies the reparameterization trick.
        forward(x): Encodes input, samples latent variables, and decodes the latent space.

    Args:
        input_dim (int): Number of input channels (e.g., 1 for grayscale images).
        code_dim (int): Dimensionality of the latent space.
    """
    def __init__(self, input_dim, code_dim):
        super(ImprovedVAE, self).__init__()

        self.encoder = nn.Sequential(
            nn.Conv2d(input_dim, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2, 2),
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 256),  # Bottleneck layer
            nn.ReLU()
        )

        self.fc_mu = nn.Linear(256, code_dim)
        self.fc_logvar = nn.Linear(256, code_dim)

        # Decoder with Batch Normalization
        self.decoder = nn.Sequential(
            nn.Linear(code_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 64 * 7 * 7),
            nn.ReLU(),
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Upsampling with Transposed Convolution
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),  # Upsampling with Transposed Convolution
            nn.Tanh()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        encoded = self.encoder(x)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        z = self.reparameterize(mu, logvar)
        decoded = self.decoder(z)
        return mu, logvar, decoded


# Flexible VAE for Dynamic Projection and Inputs
class FlexibleVAE(nn.Module):
    """
    A flexible VAE with support for dynamic input shapes and optional projection heads.

    Features:
    - Encoder: Fully connected layers with adjustable input dimensions.
    - Decoder: Fully connected layers for reconstructing the input.
    - Projection Head (optional): Adds a projection layer for contrastive learning.

    Methods:
        reparameterize(mu, logvar): Applies the reparameterization trick.
        forward(x): Encodes, samples latent variables, and reconstructs the input.

    Args:
        input_shape (tuple): Shape of the input data.
        code_dim (int): Dimensionality of the latent space.
        projection_dim (int, optional): Dimensionality of the projection head.
    """

    def __init__(self, input_shape, code_dim, projection_dim=None):
        super(FlexibleVAE, self).__init__()
        self.input_shape = input_shape
        self.code_dim = code_dim
        self.flat_size = np.prod(input_shape)

        self.encoder = nn.Sequential(
            nn.Linear(self.flat_size, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU()
        )

        self.fc_mu = nn.Linear(256, code_dim)
        self.fc_logvar = nn.Linear(256, code_dim)
        if projection_dim:
            self.projection_head = nn.Sequential(
                nn.Linear(code_dim, projection_dim),
                nn.ReLU()
            )

        self.decoder = nn.Sequential(
            nn.Linear(code_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, self.flat_size),
            nn.Tanh()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = x.view(-1, self.flat_size)  # Flatten input dynamically
        encoded = self.encoder(x)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        z = self.reparameterize(mu, logvar)
        if hasattr(self, 'projection_head'):
            z = self.projection_head(z)
        decoded = self.decoder(z)
        decoded = decoded.view(-1, *self.input_shape)  # Reshape output
        return mu, logvar, decoded

class ImprovedFlexibleVAE(nn.Module):
    """
    A flexible and improved VAE with convolutional encoder layers.

    Features:
    - Encoder: Combines convolutional layers with fully connected layers for rich feature extraction.
    - Decoder: Uses transposed convolutions for reconstruction.
    - Projection Head (optional): Adds a projection layer for self-supervised tasks.

    Methods:
        reparameterize(mu, logvar): Samples latent variables using the reparameterization trick.
        forward(x): Encodes input, samples latent variables, and decodes to reconstruct input.

    Args:
        input_shape (tuple): Shape of the input data.
        code_dim (int): Dimensionality of the latent space.
        projection_dim (int, optional): Dimensionality of the projection head.
    """

    def __init__(self, input_shape, code_dim, projection_dim=None):
        super(ImprovedFlexibleVAE, self).__init__()
        self.input_shape = input_shape
        self.code_dim = code_dim
        self.flat_size = np.prod(input_shape)

        # Encoder: Adding Convolutional Layers for better feature extraction
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2, 2),
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 256),  # Bottleneck layer
            nn.ReLU()
        )

        self.fc_mu = nn.Linear(256, code_dim)
        self.fc_logvar = nn.Linear(256, code_dim)

        if projection_dim:
            self.projection_head = ContrastiveHead(code_dim, projection_dim)

        # Decoder: Adding convolutional layers for better image reconstruction
        self.decoder = nn.Sequential(
            nn.Linear(code_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 64 * 7 * 7),
            nn.ReLU(),
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # Upsampling with ConvTranspose2d
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),  # Upsampling with ConvTranspose2d
            nn.Tanh()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = x.view(-1, self.flat_size)  # Flatten input dynamically
        encoded = self.encoder(x)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)
        z = self.reparameterize(mu, logvar)
        if hasattr(self, 'projection_head'):
            z = self.projection_head(z)
        decoded = self.decoder(z)
        decoded = decoded.view(-1, *self.input_shape)  # Reshape output to match input shape
        return mu, logvar, decoded

# Define the Combined Denoising Autoencoder Class
class DenoisingAutoencoder(nn.Module):
    """
    A denoising autoencoder that learns embeddings while denoising corrupted inputs.

    Features:
    - Encoder: Convolutional layers with max-pooling for feature extraction.
    - Decoder: Transposed convolutional layers for reconstructing clean inputs.
    - Optional Projection Head: Supports embedding projection for contrastive learning.

    Args:
        code_dim (int): Dimensionality of the latent space.
        projection_dim (int, optional): Dimensionality of the projection head.
        strong_architecture (bool): Whether to use a deeper encoder/decoder structure.

    Methods:
        forward(x): Encodes the noisy input and reconstructs the clean output.
    """

    def __init__(self, code_dim, projection_dim=None, strong_architecture=False):
        super(DenoisingAutoencoder, self).__init__()

        # Set whether to use the strong architecture or basic architecture
        self.strong_architecture = strong_architecture

        # Encoder Configuration
        if self.strong_architecture:
            self.encoder = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 1 -> 32 channels
                nn.ReLU(),
                nn.BatchNorm2d(32),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 32 -> 64 channels
                nn.ReLU(),
                nn.BatchNorm2d(64),
                nn.MaxPool2d(2, 2),
                nn.Flatten(),
                nn.Linear(64 * 7 * 7, 256),  # Bottleneck layer (flattening the output)
                nn.ReLU()
            )
        else:
            self.encoder = nn.Sequential(
                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 1 -> 16 channels
                nn.ReLU(),
                nn.BatchNorm2d(16),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),  # 16 -> 8 channels
                nn.ReLU(),
                nn.BatchNorm2d(8),
                nn.MaxPool2d(2, 2),
            )

        self.fc_encoder = nn.Linear(256 if self.strong_architecture else 8 * 7 * 7, code_dim)

        # Optional Projection Head for Contrastive Learning
        self.projection_head = ContrastiveHead(code_dim, projection_dim) if projection_dim else None

        # Decoder Configuration (Same for both architectures)
        self.fc_decoder = nn.Linear(code_dim, 256 if self.strong_architecture else 8 * 7 * 7)
        self.decoder = nn.Sequential(
            nn.ReLU(),
            nn.Linear(256 if self.strong_architecture else 8 * 7 * 7, 64 * 7 * 7),  # Linear layer to match flattened size
            nn.ReLU(),
            nn.Unflatten(1, (64, 7, 7)),  # Unflatten to the correct shape for Conv2D
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Upsample using Transposed Convolution
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),  # Upsample back to the original shape
            nn.Tanh()  # Use Tanh to ensure pixel values are between -1 and 1 (standard for MNIST)
        )

    def forward(self, x):
        batch_size = x.size(0)

        # Encoding (forward pass through encoder)
        encoded = self.encoder(x)
        encoded = encoded.view(batch_size, -1)  # Flatten the encoded features
        encoded = self.fc_encoder(encoded)  # Map to the latent space (code)

        # Optionally, apply projection head (for contrastive learning, etc.)
        projected_encoded = self.projection_head(encoded) if self.projection_head else encoded

        # Decoding (forward pass through decoder)
        decoded = self.fc_decoder(encoded)  # Decode from latent space back to feature space
        decoded = self.decoder(decoded)  # Final image reconstruction

        return projected_encoded, decoded, encoded

def apply_dimensionality_reduction(method, data, n_components, scaler=None, **kwargs):
    """
    Apply dimensionality reduction to input data.

    Args:
        method (class): Dimensionality reduction class (e.g., PCA, TruncatedSVD, NMF).
        data (np.ndarray): Input data to reduce dimensions.
        n_components (int): Number of components to reduce to.
        scaler (object, optional): Scaler to preprocess data (e.g., StandardScaler, MinMaxScaler).
        **kwargs: Additional arguments for the dimensionality reduction method.

    Returns:
        np.ndarray: Reduced dimensional data.
    """
    if scaler:
        data = scaler.fit_transform(data)
    model = method(n_components=n_components, **kwargs)
    return model.fit_transform(data)

def create_dataloader(embeddings_tensor, labels_tensor, batch_size=128):
    """
    Create a DataLoader for embeddings and their corresponding labels.

    Args:
        embeddings_tensor (torch.Tensor): Tensor of embeddings.
        labels_tensor (torch.Tensor): Tensor of labels.
        batch_size (int): Batch size for the DataLoader.

    Returns:
        DataLoader: DataLoader for the provided tensors.
    """
    dataset = TensorDataset(embeddings_tensor, labels_tensor)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

def process_matrix_factorization(sampled_x, sampled_y, n_components=50):
    """
    Apply matrix factorization methods (PCA, SVD, NMF) to input data.

    Args:
        sampled_x (np.ndarray): Input data.
        sampled_y (np.ndarray): Corresponding labels.
        n_components (int): Number of components for factorization.

    Returns:
        dict: Dictionary containing embeddings for each method.
        torch.Tensor: Tensor of labels.
    """
    results = {}
    sampled_x_flat = sampled_x.reshape(sampled_x.shape[0], -1)

    # Standardize and Min-Max scale
    scaler_standard = StandardScaler()
    scaler_minmax = MinMaxScaler()

    # PCA
    pca_embeddings = apply_dimensionality_reduction(PCA, sampled_x_flat, n_components=n_components, scaler=scaler_standard)
    results["PCA"] = torch.tensor(pca_embeddings, dtype=torch.float32)

    # SVD
    svd_embeddings = apply_dimensionality_reduction(TruncatedSVD, sampled_x_flat, n_components=n_components, scaler=scaler_standard)
    results["SVD"] = torch.tensor(svd_embeddings, dtype=torch.float32)

    # NMF
    nmf_embeddings = apply_dimensionality_reduction(NMF, sampled_x_flat, n_components=n_components, scaler=scaler_minmax)
    results["NMF"] = torch.tensor(nmf_embeddings, dtype=torch.float32)

    return results, torch.tensor(sampled_y, dtype=torch.long)

def create_embedding_loaders(embeddings_dict, labels_tensor, batch_size=128):
    """
    Creates DataLoaders for each embedding method in the dictionary.

    Args:
        embeddings_dict (dict): Dictionary where keys are method names and values are embedding tensors.
        labels_tensor (torch.Tensor): Tensor of labels corresponding to embeddings.
        batch_size (int): Batch size for DataLoaders.

    Returns:
        dict: Dictionary containing DataLoaders for each embedding method.
    """
    loaders = {}
    for method, embeddings in embeddings_dict.items():
        loaders[method] = create_dataloader(embeddings, labels_tensor, batch_size=batch_size)
    return loaders

def apply_sift(data, n_features=50):
    """
    Applies SIFT (Scale-Invariant Feature Transform) to extract features from images.

    Args:
        data (np.ndarray): Array of grayscale images with shape (N, H, W).
        n_features (int): Maximum number of features to extract per image.

    Returns:
        np.ndarray: Extracted SIFT features with shape (N, n_features, 128).
    """
    sift = cv2.SIFT_create(nfeatures=n_features)
    descriptors = []

    for image in data:
        image = (image * 255).astype(np.uint8)  # Convert image to uint8 format
        keypoints, des = sift.detectAndCompute(image, None)
        if des is not None:
            descriptors.append(des)
        else:
            descriptors.append(np.zeros((0, 128)))  # Handle images with no keypoints

    # Padding descriptors to have consistent feature length
    padded_descriptors = np.array([
        np.pad(d, ((0, max(0, n_features - len(d))), (0, 0)), mode='constant')[:n_features] for d in descriptors
    ])
    return padded_descriptors

def process_feature_extraction(sampled_x, sampled_y, n_features=50, kernel="rbf", n_components=50):
    """
    Extracts features using SIFT and Kernel PCA from the input data.

    Args:
        sampled_x (np.ndarray): Input data with shape (N, H, W) for images.
        sampled_y (np.ndarray): Corresponding labels for input data.
        n_features (int): Number of features to extract using SIFT.
        kernel (str): Kernel type for Kernel PCA (e.g., "rbf", "linear").
        n_components (int): Number of components for Kernel PCA.

    Returns:
        dict: Dictionary containing extracted features (e.g., "SIFT", "Kernel PCA").
        torch.Tensor: Tensor of labels corresponding to the input data.
    """
    results = {}

    # SIFT Feature Extraction
    sift_features = apply_sift(sampled_x, n_features=n_features)
    results["SIFT"] = torch.tensor(sift_features.reshape(len(sift_features), -1), dtype=torch.float32)

    # Kernel PCA Feature Extraction
    sampled_x_flat = sampled_x.reshape(sampled_x.shape[0], -1)
    scaler_standard = StandardScaler()
    sampled_x_scaled = scaler_standard.fit_transform(sampled_x_flat)
    kpca_embeddings = apply_dimensionality_reduction(
        KernelPCA, sampled_x_scaled, n_components=n_components, kernel=kernel
    )
    results["Kernel PCA"] = torch.tensor(kpca_embeddings, dtype=torch.float32)

    return results, torch.tensor(sampled_y, dtype=torch.long)

class FlowLayer(nn.Module):
    def __init__(self, input_dim):
        """
        Initializes a single Normalizing Flow layer.

        Args:
            input_dim (int): Dimensionality of the input.
        """
        super(FlowLayer, self).__init__()
        self.scale = nn.Linear(input_dim, input_dim)
        self.shift = nn.Linear(input_dim, input_dim)

    def forward(self, z):
        """
        Forward pass for the flow layer.

        Args:
            z (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Transformed tensor.
            torch.Tensor: Log determinant of the Jacobian.
        """
        scale = torch.tanh(self.scale(z))
        shift = self.shift(z)
        transformed_z = z * torch.exp(scale) + shift
        log_det_jacobian = torch.sum(scale, dim=-1)
        return transformed_z, log_det_jacobian

class NormalizingFlowModel(nn.Module):
    def __init__(self, input_dim, num_flows):
        """
        Initializes a Normalizing Flow model with multiple layers.

        Args:
            input_dim (int): Dimensionality of the input.
            num_flows (int): Number of flow layers.
        """
        super(NormalizingFlowModel, self).__init__()
        self.flows = nn.ModuleList([FlowLayer(input_dim) for _ in range(num_flows)])

    def forward(self, z):
        """
        Forward pass through all flow layers.

        Args:
            z (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Transformed tensor.
            torch.Tensor: Total log determinant of the Jacobian.
        """
        log_det_jacobian_total = torch.zeros(z.size(0), device=z.device)
        for flow in self.flows:
            z, log_det_jacobian = flow(z)
            log_det_jacobian_total += log_det_jacobian
        return z, log_det_jacobian_total

def log_prob(z, model, base_distribution, detailed=False):
    """
    Computes the log probability of a tensor under the model and base distribution.

    Args:
        z (torch.Tensor): Input tensor.
        model (NormalizingFlowModel): Normalizing Flow model.
        base_distribution (torch.distributions.Distribution): Base distribution.
        detailed (bool, optional): If True, returns detailed outputs including transformed
                                   tensor, base log probability, and log determinant of Jacobian.

    Returns:
        torch.Tensor: Log probability.
        If detailed=True:
            torch.Tensor: Transformed tensor.
            torch.Tensor: Log probability from the base distribution.
            torch.Tensor: Log determinant of the Jacobian.
    """
    z_transformed, log_det_jacobian = model(z)
    log_base_prob = base_distribution.log_prob(z_transformed).sum(dim=-1)
    if detailed:
        return z_transformed, log_base_prob, log_det_jacobian
    return log_base_prob + log_det_jacobian

def train_nf_model(model, embeddings, num_epochs=10, lr=1e-3, batch_size=128, save_path=None):
    """
    Trains a Normalizing Flow model.

    Args:
        model (NormalizingFlowModel): Normalizing Flow model to train.
        embeddings (torch.Tensor): Input embeddings for training.
        num_epochs (int): Number of training epochs.
        lr (float): Learning rate.
        batch_size (int): Batch size for training.
        save_path (str, optional): Path to save the model checkpoints. Defaults to None.

    Returns:
        NormalizingFlowModel: Trained Normalizing Flow model.
    """
    optimizer = optim.Adam(model.parameters(), lr=lr)
    dataset = TensorDataset(embeddings)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    base_distribution = torch.distributions.MultivariateNormal(
        torch.zeros(embeddings.size(1), device=embeddings.device),
        torch.eye(embeddings.size(1), device=embeddings.device)
    )

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch in dataloader:
            batch_z = batch[0].to(embeddings.device)
            loss = -torch.mean(log_prob(batch_z, model, base_distribution))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader):.4f}")

        if save_path:
            torch.save(model.state_dict(), save_path)

    return model


def refine_embeddings_NF(embedding_type, embedding_method, embeddings, num_flows=4, num_epochs=20):
    """
    Refines embeddings using a Normalizing Flow model.

    Args:
        embedding_type (str): Type of embedding (e.g., 'Matrix Factorization').
        embedding_method (str): Specific embedding method (e.g., 'PCA').
        embeddings (dict): Dictionary containing all embeddings.
        num_flows (int): Number of flow layers in the model.
        num_epochs (int): Number of training epochs.

    Returns:
        dict: Updated embeddings dictionary with refined embeddings.
    """
    embeddings_tensor = torch.tensor(embeddings[embedding_type][embedding_method]).float()
    latent_dim = embeddings_tensor.shape[1]
    nf_model = NormalizingFlowModel(input_dim=latent_dim, num_flows=num_flows)
    trained_nf_model = train_nf_model(nf_model, embeddings_tensor, num_epochs=num_epochs)

    with torch.no_grad():
        refined_embeddings, _ = trained_nf_model(embeddings_tensor)
        refined_key = f"Refined {embedding_method} Embeddings"
        embeddings["Generative Models"][refined_key] = refined_embeddings.numpy()

    return embeddings

class ProjectionHead(nn.Module):
    def __init__(self, input_dim, projection_dim, use_batchnorm=True):
        """
        Initializes the projection head for SimCLR with optional batch normalization.

        Args:
            input_dim (int): Input dimensionality.
            projection_dim (int): Dimensionality of the projected space.
            use_batchnorm (bool, optional): Whether to use BatchNorm. Defaults to True.
        """
        super(ProjectionHead, self).__init__()
        self.use_batchnorm = use_batchnorm
        self.fc1 = nn.Linear(input_dim, projection_dim)
        self.bn1 = nn.BatchNorm1d(projection_dim) if use_batchnorm else nn.Identity()
        self.fc2 = nn.Linear(projection_dim, projection_dim)
        self.bn2 = nn.BatchNorm1d(projection_dim) if use_batchnorm else nn.Identity()

    def forward(self, x):
        """
        Forward pass for the projection head.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Projected tensor.
        """
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.bn2(self.fc2(x))
        return x

class SimCLR(nn.Module):
    def __init__(self, base_model, projection_dim=128):
        """
        Initializes the SimCLR model.

        Args:
            base_model (nn.Module): Base encoder model with an 'output_dim' attribute.
            projection_dim (int): Dimensionality of the projection head.
        """
        super(SimCLR, self).__init__()
        assert hasattr(base_model, 'output_dim'), "Base model must have 'output_dim' attribute."
        self.encoder = base_model
        self.projection_head = ProjectionHead(input_dim=base_model.output_dim, projection_dim=projection_dim)

    def forward(self, x):
        """
        Forward pass through the encoder and projection head.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Projections.
        """
        features = self.encoder(x)
        projections = self.projection_head(features)
        return projections

    @staticmethod
    def contrastive_loss(projections, temperature=0.1):
        """
        Computes contrastive loss for the SimCLR model.

        Args:
            projections (torch.Tensor): Projections from the SimCLR model.
            temperature (float): Temperature scaling for contrastive loss.

        Returns:
            torch.Tensor: Contrastive loss value.
        """
        projections = F.normalize(projections, dim=1)
        similarity_matrix = torch.matmul(projections, projections.T) / temperature

        # Mask out self-similarities
        mask = torch.eye(similarity_matrix.size(0), device=projections.device).bool()
        similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))

        labels = torch.arange(projections.size(0), device=projections.device)
        return F.cross_entropy(similarity_matrix, labels)