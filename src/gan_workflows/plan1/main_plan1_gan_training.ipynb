{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Plan 1 GAN Training Notebook\n","\n","import os\n","import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import logging\n","import json\n","import datetime\n","from datetime import datetime\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from torchvision.models import inception_v3\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.linalg import sqrtm\n","from scipy.stats import wasserstein_distance, entropy, spearmanr, gaussian_kde\n","\n","\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Repository path (adjust if needed)\n","repo_path = \"/content/drive/MyDrive/GAN-thesis-project\"\n","\n","# Add repository path to sys.path for module imports\n","if repo_path not in sys.path:\n","    sys.path.append(repo_path)\n","\n","# Change working directory to the repository\n","os.chdir(repo_path)\n","\n","# Verify the working directory\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","\n","# Set random seed and device\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IyJnrpx1UiH3","executionInfo":{"status":"ok","timestamp":1738345472561,"user_tz":-210,"elapsed":49072,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"36eb786f-3c07-4769-ab89-8c017fd7e303"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/GAN-thesis-project\n","Using device: cpu\n"]}]},{"cell_type":"code","source":["# just for you to see what are the implemented function and classes that have been coded up...\n","\n","import inspect\n","\n","# Import the entire modules\n","import src.data_utils as data_utils\n","import src.cl_loss_function as cl_loss\n","import src.losses as losses\n","import src.gan_workflows.plan1.plan1_gan_models as gan_models\n","import src.gan_workflows.plan1.plan1_gan_training as gan_training\n","\n","# Function to list functions and classes in a module\n","def list_functions_and_classes(module):\n","    members = inspect.getmembers(module)\n","    functions = [name for name, obj in members if inspect.isfunction(obj)]\n","    classes = [name for name, obj in members if inspect.isclass(obj)]\n","    return functions, classes\n","\n","# Function to print functions and classes in a readable format\n","def print_functions_and_classes(module_name, module):\n","    functions, classes = list_functions_and_classes(module)\n","    print(f\"Module: {module_name}\")\n","    print(\"  Functions:\")\n","    for func in functions:\n","        print(f\"    - {func}\")\n","    print(\"  Classes:\")\n","    for cls in classes:\n","        print(f\"    - {cls}\")\n","    print()  # Add a blank line for separation\n","\n","# Print functions and classes for each module\n","print_functions_and_classes(\"src.data_utils\", data_utils)\n","print_functions_and_classes(\"src.cl_loss_function\", cl_loss)\n","print_functions_and_classes(\"src.losses\", losses)\n","print_functions_and_classes(\"src.embeddings.encoder_models\", gan_models)\n","print_functions_and_classes(\"src.embeddings.encoder_training\", gan_training)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mj2_ITKkUoYF","executionInfo":{"status":"ok","timestamp":1738345476848,"user_tz":-210,"elapsed":4292,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"86488ec1-50de-494c-fc7b-d951d9ec9ccc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Module: src.data_utils\n","  Functions:\n","    - analyze_embeddings\n","    - analyze_embeddings_v2\n","    - create_dataloader\n","    - create_embedding_loaders\n","    - generate_embeddings\n","    - kurtosis\n","    - load_data\n","    - load_embeddings\n","    - load_mnist_data\n","    - pdist\n","    - preprocess_images\n","    - save_embeddings\n","    - skew\n","    - split_dataset\n","    - train_test_split\n","    - visualize_embeddings\n","  Classes:\n","    - DataLoader\n","    - LocalOutlierFactor\n","    - TensorDataset\n","\n","Module: src.cl_loss_function\n","  Functions:\n","    - augment\n","    - compute_nt_xent_loss_with_augmentation\n","    - compute_triplet_loss_with_augmentation\n","    - contrastive_loss\n","    - hflip\n","    - info_nce_loss\n","    - resize\n","  Classes:\n","    - BYOLLoss\n","    - BarlowTwinsLoss\n","    - ContrastiveHead\n","    - DataLoader\n","    - NTXentLoss\n","    - PCA\n","    - Predictor\n","    - TensorDataset\n","    - TripletLoss\n","    - VicRegLoss\n","\n","Module: src.losses\n","  Functions:\n","    - add_noise\n","    - cyclical_beta_schedule\n","    - linear_beta_schedule\n","    - loss_function_dae_ssim\n","    - vae_loss\n","    - vae_ssim_loss\n","  Classes:\n","\n","Module: src.embeddings.encoder_models\n","  Functions:\n","  Classes:\n","    - AC_Discriminator\n","    - AC_Generator\n","    - D_block\n","    - Dataset\n","    - Discriminator\n","    - EmbeddingAsInputGenerator\n","    - G_block\n","    - Info_Discriminator\n","    - Info_Generator\n","    - SimpleGenerator\n","\n","Module: src.embeddings.encoder_training\n","  Functions:\n","    - train_acgan\n","    - train_infogan\n","    - train_normal_gan\n","  Classes:\n","    - DataLoader\n","\n"]}]},{"cell_type":"code","source":["import os\n","\n","# Base directory where embeddings are stored\n","embedding_base_dir = \"./saved_embeddings/embeddings/\"\n","\n","def list_available_embeddings(base_dir, filter_by=None):\n","    \"\"\"\n","    List available embedding directories and files, optionally filtered by method.\n","\n","    Args:\n","        base_dir (str): The base directory containing embeddings.\n","        filter_by (str or list, optional): Method(s) to filter by (e.g., \"autoencoder\", \"vae\").\n","                                           If None, all embeddings are displayed.\n","    \"\"\"\n","    print(\"\\nðŸ“‚ Available Embeddings:\\n\")\n","\n","    if isinstance(filter_by, str):\n","        filter_by = [filter_by]  # Convert single filter to list\n","\n","    for method in sorted(os.listdir(base_dir)):\n","        method_path = os.path.join(base_dir, method)\n","\n","        # Check if it's a directory\n","        if os.path.isdir(method_path):\n","            if filter_by is None or any(f.lower() in method.lower() for f in filter_by):\n","                pt_files = [f for f in sorted(os.listdir(method_path)) if f.endswith(\".pt\")]\n","                if pt_files:\n","                    print(f\"\\nðŸ”¹ {method}\")  # Show only the category\n","                    for file in pt_files:\n","                        print(f\"   ðŸ“„ {method}/{file}\")  # Show category + filename\n","\n","# Default: Show everything\n","list_available_embeddings(embedding_base_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzAx9F_KdSkZ","executionInfo":{"status":"ok","timestamp":1738345479690,"user_tz":-210,"elapsed":2849,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"0da2fd30-82b4-4d10-aa05-0c17c46e7fb1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸ“‚ Available Embeddings:\n","\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_barlow_twins\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_barlow_twins/AdvancedAutoencoder_barlow_twins_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_contrastive\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_contrastive/AdvancedAutoencoder_contrastive_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_info_nce\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_info_nce/AdvancedAutoencoder_info_nce_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_mse\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_ntxent\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_ntxent/AdvancedAutoencoder_ntxent_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_vicreg\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_vicreg/AdvancedAutoencoder_vicreg_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_barlow_twins\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_barlow_twins/EnhancedAutoencoder_barlow_twins_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_contrastive\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_contrastive/EnhancedAutoencoder_contrastive_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_info_nce\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_info_nce/EnhancedAutoencoder_info_nce_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_mse\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_mse/EnhancedAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_ntxent\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_ntxent/EnhancedAutoencoder_ntxent_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_vicreg\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_vicreg/EnhancedAutoencoder_vicreg_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_barlow_twins\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_barlow_twins/IntermediateAutoencoder_barlow_twins_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_contrastive\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_contrastive/IntermediateAutoencoder_contrastive_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_info_nce\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_info_nce/IntermediateAutoencoder_info_nce_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_mse\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_ntxent\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_ntxent/IntermediateAutoencoder_ntxent_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_vicreg\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_vicreg/IntermediateAutoencoder_vicreg_embeddings.pt\n","\n","ðŸ”¹ autoencoders_AdvancedAutoencoder\n","   ðŸ“„ autoencoders_AdvancedAutoencoder/AdvancedAutoencoder_embeddings.pt\n","\n","ðŸ”¹ autoencoders_AdvancedAutoencoder_mse\n","   ðŸ“„ autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoders_BasicAutoencoder\n","   ðŸ“„ autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\n","\n","ðŸ”¹ autoencoders_BasicAutoencoder_mse\n","   ðŸ“„ autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoders_IntermediateAutoencoder_mse\n","   ðŸ“„ autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ kernel_pca_Kernel PCA\n","   ðŸ“„ kernel_pca_Kernel PCA/Kernel PCA_embeddings.pt\n","   ðŸ“„ kernel_pca_Kernel PCA/matrix_factorization_default_loss_kernel_pca_Kernel PCA_embeddings.pt\n","\n","ðŸ”¹ kernel_pca_SIFT\n","   ðŸ“„ kernel_pca_SIFT/SIFT_embeddings.pt\n","   ðŸ“„ kernel_pca_SIFT/matrix_factorization_default_loss_kernel_pca_SIFT_embeddings.pt\n","\n","ðŸ”¹ matrix_factorization_NMF\n","   ðŸ“„ matrix_factorization_NMF/NMF_embeddings.pt\n","   ðŸ“„ matrix_factorization_NMF/matrix_factorization_default_loss_NMF_embeddings.pt\n","\n","ðŸ”¹ matrix_factorization_PCA\n","   ðŸ“„ matrix_factorization_PCA/PCA_embeddings.pt\n","   ðŸ“„ matrix_factorization_PCA/matrix_factorization_default_loss_PCA_embeddings.pt\n","\n","ðŸ”¹ matrix_factorization_SVD\n","   ðŸ“„ matrix_factorization_SVD/SVD_embeddings.pt\n","   ðŸ“„ matrix_factorization_SVD/matrix_factorization_default_loss_SVD_embeddings.pt\n","\n","ðŸ”¹ normalizing_flow_NMF\n","   ðŸ“„ normalizing_flow_NMF/NMF_refined_embeddings.pt\n","   ðŸ“„ normalizing_flow_NMF/matrix_factorization_default_loss_normalizing_flow_NMF_refined_embeddings.pt\n","\n","ðŸ”¹ normalizing_flow_PCA\n","   ðŸ“„ normalizing_flow_PCA/PCA_refined_embeddings.pt\n","   ðŸ“„ normalizing_flow_PCA/matrix_factorization_default_loss_normalizing_flow_PCA_refined_embeddings.pt\n","\n","ðŸ”¹ normalizing_flow_SVD\n","   ðŸ“„ normalizing_flow_SVD/SVD_refined_embeddings.pt\n","   ðŸ“„ normalizing_flow_SVD/matrix_factorization_default_loss_normalizing_flow_SVD_refined_embeddings.pt\n","\n","ðŸ”¹ sift_features\n","   ðŸ“„ sift_features/matrix_factorization_default_loss_sift_embeddings.pt\n","   ðŸ“„ sift_features/sift_embeddings.pt\n","\n","ðŸ”¹ vae_BasicVAE_mse\n","   ðŸ“„ vae_BasicVAE_mse/BasicVAE_mse_embeddings.pt\n","\n","ðŸ”¹ vae_FlexibleVAE_mse\n","   ðŸ“„ vae_FlexibleVAE_mse/FlexibleVAE_mse_embeddings.pt\n","\n","ðŸ”¹ vae_ImprovedFlexibleVAE_mse\n","   ðŸ“„ vae_ImprovedFlexibleVAE_mse/ImprovedFlexibleVAE_mse_embeddings.pt\n","\n","ðŸ”¹ vae_ImprovedVAE_mse\n","   ðŸ“„ vae_ImprovedVAE_mse/ImprovedVAE_mse_embeddings.pt\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ==========================================\n","# Critic Model (Discriminator in WGAN-GP)\n","# ==========================================\n","class Critic(nn.Module):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 64, 4, 2, 1),  # 28x28 -> 14x14\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, 4, 2, 1, bias=False),  # 14x14 -> 7x7\n","            nn.InstanceNorm2d(128),  # Better for WGAN-GP than BatchNorm\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),  # 7x7 -> 3x3\n","            nn.InstanceNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(256, 1, 3, 1, 0)  # 3x3 -> 1x1\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x).view(-1)\n","\n","# ==========================================\n","# Generator Model (Fixed for 28x28 output)\n","# ==========================================\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim=100):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","\n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim, 256, 7, 1, 0, bias=False),  # 1x1 -> 7x7\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # 7x7 -> 14x14\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),  # 14x14 -> 28x28\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 1, 3, 1, 1, bias=False),  # Maintain 28x28\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        z = z.view(-1, self.latent_dim, 1, 1)\n","        return self.model(z)\n","\n","# ==========================================\n","# Gradient Penalty Function\n","# ==========================================\n","def gradient_penalty(critic, real_samples, fake_samples, device):\n","    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n","    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n","    critic_interpolates = critic(interpolates)\n","\n","    gradients = autograd.grad(\n","        outputs=critic_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=torch.ones_like(critic_interpolates),\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True\n","    )[0]\n","\n","    gradients = gradients.view(gradients.size(0), -1)\n","    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","    return gp\n","\n","# ==========================================\n","# Training Function\n","# ==========================================\n","def train_wgan_gp(generator, critic, dataloader, epochs, device, lambda_gp=10, n_critic=5):\n","    generator.to(device)\n","    critic.to(device)\n","\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.0, 0.9))\n","    optimizer_C = optim.Adam(critic.parameters(), lr=0.0001, betas=(0.0, 0.9))\n","\n","    for epoch in range(epochs):\n","        for i, (real_images, _) in enumerate(dataloader):\n","            real_images = real_images.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # Train Critic\n","            optimizer_C.zero_grad()\n","\n","            # Generate fake images\n","            z = torch.randn(batch_size, generator.latent_dim, device=device)\n","            fake_images = generator(z)\n","\n","            # Critic loss\n","            real_loss = -torch.mean(critic(real_images))\n","            fake_loss = torch.mean(critic(fake_images.detach()))\n","            gp = gradient_penalty(critic, real_images, fake_images.detach(), device)\n","            loss_C = real_loss + fake_loss + lambda_gp * gp\n","\n","            loss_C.backward()\n","            optimizer_C.step()\n","\n","            # Train Generator less frequently\n","            if i % n_critic == 0:\n","                optimizer_G.zero_grad()\n","                gen_loss = -torch.mean(critic(fake_images))\n","                gen_loss.backward()\n","                optimizer_G.step()\n","\n","        print(f\"Epoch {epoch+1}/{epochs} | Critic Loss: {loss_C.item():.4f} | Gen Loss: {gen_loss.item():.4f}\")\n","\n","# ==========================================\n","# Data Loading\n","# ==========================================\n","def get_data_loader(batch_size=64):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","    dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","\n","# ==========================================\n","# Main Execution\n","# ==========================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","generator = Generator(latent_dim=100)\n","critic = Critic()\n","dataloader = get_data_loader()\n","\n","train_wgan_gp(generator, critic, dataloader, epochs=50, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"U4xOpChhrtDV","executionInfo":{"status":"error","timestamp":1738351920620,"user_tz":-210,"elapsed":531228,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"ef270320-1d21-405a-d7a5-3fa410e52254"},"execution_count":27,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-bb5a29736665>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mtrain_wgan_gp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-bb5a29736665>\u001b[0m in \u001b[0;36mtrain_wgan_gp\u001b[0;34m(generator, critic, dataloader, epochs, device, lambda_gp, n_critic)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mloss_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_gp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mloss_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0moptimizer_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ==========================================\n","# Conditional Generator\n","# ==========================================\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim=100, num_classes=10, img_channels=1):\n","        super(Generator, self).__init__()\n","        self.label_embed = nn.Embedding(num_classes, num_classes)\n","\n","        self.init_size = 7  # Initial size before upsampling\n","        self.l1 = nn.Linear(latent_dim + num_classes, 256 * self.init_size**2)\n","\n","        self.model = nn.Sequential(\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # 7x7 -> 14x14\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),   # 14x14 -> 28x28\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, img_channels, 3, 1, 1, bias=False),   # Maintain 28x28\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, noise, labels):\n","        # Concatenate noise and label embedding\n","        gen_input = torch.cat((self.label_embed(labels), noise), -1)\n","        out = self.l1(gen_input)\n","        out = out.view(out.shape[0], 256, self.init_size, self.init_size)\n","        return self.model(out)\n","\n","# ==========================================\n","# Conditional Discriminator\n","# ==========================================\n","class Discriminator(nn.Module):\n","    def __init__(self, num_classes=10, img_channels=1):\n","        super(Discriminator, self).__init__()\n","        self.img_size = 28\n","        self.num_classes = num_classes\n","\n","        # Label embedding and processing\n","        self.label_embed = nn.Embedding(num_classes, num_classes)\n","\n","        # Adjusted input channels: img_channels + num_classes\n","        self.model = nn.Sequential(\n","            nn.Conv2d(img_channels + num_classes, 64, 4, 2, 1),  # Fixed input channels\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","        )\n","\n","        # Output layers\n","        self.adv_layer = nn.Sequential(nn.Linear(256*3*3, 1), nn.Sigmoid())\n","        self.aux_layer = nn.Sequential(nn.Linear(256*3*3, num_classes), nn.Softmax(dim=1))\n","\n","    def forward(self, img, labels):\n","        # Label processing with proper dimensions\n","        embedded_labels = self.label_embed(labels)\n","        embedded_labels = embedded_labels.view(\n","            embedded_labels.size(0),\n","            self.num_classes,  # Use num_classes instead of hard-coded 1\n","            1, 1\n","        )\n","        label_map = embedded_labels.repeat(1, 1, self.img_size, self.img_size)\n","\n","        # Concatenate image and label map\n","        combined = torch.cat((img, label_map), dim=1)\n","\n","        features = self.model(combined)\n","        features = features.view(features.shape[0], -1)\n","\n","        validity = self.adv_layer(features)\n","        label_pred = self.aux_layer(features)\n","        return validity, label_pred\n","\n","# ==========================================\n","# Training Function\n","# ==========================================\n","def train_cgan(generator, discriminator, dataloader, epochs, device,\n","              latent_dim=100, num_classes=10):\n","    generator.to(device)\n","    discriminator.to(device)\n","\n","    # Loss functions\n","    adversarial_loss = nn.BCELoss()\n","    auxiliary_loss = nn.CrossEntropyLoss()\n","\n","    # Optimizers\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for i, (imgs, labels) in enumerate(dataloader):\n","            batch_size = imgs.shape[0]\n","            real_imgs = imgs.to(device)\n","            real_labels = labels.to(device)\n","\n","            # Adversarial ground truths\n","            valid = torch.ones(batch_size, 1, device=device)\n","            fake = torch.zeros(batch_size, 1, device=device)\n","\n","            # ==========================\n","            #  Train Discriminator\n","            # ==========================\n","            optimizer_D.zero_grad()\n","\n","            # Real images\n","            real_pred, real_label_pred = discriminator(real_imgs, real_labels)\n","            d_real_loss = (adversarial_loss(real_pred, valid) +\n","                          auxiliary_loss(real_label_pred, real_labels)) / 2\n","\n","            # Fake images\n","            z = torch.randn(batch_size, latent_dim, device=device)\n","            gen_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n","            fake_imgs = generator(z, gen_labels)\n","            fake_pred, fake_label_pred = discriminator(fake_imgs.detach(), gen_labels)\n","            d_fake_loss = (adversarial_loss(fake_pred, fake) +\n","                          auxiliary_loss(fake_label_pred, gen_labels)) / 2\n","\n","            # Total discriminator loss\n","            d_loss = (d_real_loss + d_fake_loss) / 2\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","            # =========================\n","            #  Train Generator\n","            # =========================\n","            optimizer_G.zero_grad()\n","\n","            # Generate fake images\n","            gen_imgs = generator(z, gen_labels)\n","            validity, pred_label = discriminator(gen_imgs, gen_labels)\n","\n","            # Generator losses\n","            g_adv_loss = adversarial_loss(validity, valid)\n","            g_aux_loss = auxiliary_loss(pred_label, gen_labels)\n","            g_loss = (g_adv_loss + g_aux_loss) / 2\n","\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","        # Print progress\n","        print(f\"[Epoch {epoch+1}/{epochs}] \"\n","              f\"D Loss: {d_loss.item():.4f} G Loss: {g_loss.item():.4f}\")\n","\n","# ==========================================\n","# Data Loading\n","# ==========================================\n","def get_data_loader(batch_size=128):\n","    transform = transforms.Compose([\n","        transforms.Resize(28),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    dataset = torchvision.datasets.MNIST(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","# ==========================================\n","# Main Execution\n","# ==========================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","latent_dim = 100\n","num_classes = 10\n","epochs = 50\n","\n","# Initialize models\n","generator = Generator(latent_dim=latent_dim, num_classes=num_classes)\n","discriminator = Discriminator(num_classes=num_classes)\n","\n","# Get data loader\n","dataloader = get_data_loader()\n","\n","# Train CGAN\n","train_cgan(generator, discriminator, dataloader, epochs, device,\n","          latent_dim=latent_dim, num_classes=num_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"1_3ZiaSH2OKL","executionInfo":{"status":"error","timestamp":1738352607450,"user_tz":-210,"elapsed":300442,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"1592e981-83b3-458d-de4f-d95232b93ae2"},"execution_count":29,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-07a3f7993645>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;31m# Train CGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m train_cgan(generator, discriminator, dataloader, epochs, device, \n\u001b[0m\u001b[1;32m    199\u001b[0m           latent_dim=latent_dim, num_classes=num_classes)\n","\u001b[0;32m<ipython-input-29-07a3f7993645>\u001b[0m in \u001b[0;36mtrain_cgan\u001b[0;34m(generator, discriminator, dataloader, epochs, device, latent_dim, num_classes)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m# Real images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mreal_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             d_real_loss = (adversarial_loss(real_pred, valid) + \n\u001b[1;32m    126\u001b[0m                           auxiliary_loss(real_label_pred, real_labels)) / 2\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-07a3f7993645>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, labels)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ==========================================\n","# ACGAN Generator\n","# ==========================================\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim=100, num_classes=10, img_channels=1):\n","        super(Generator, self).__init__()\n","        self.label_embed = nn.Embedding(num_classes, latent_dim)\n","        self.img_size = 28\n","        self.latent_dim = latent_dim\n","\n","        self.init_size = 7  # Starting dimension before upsampling\n","        self.l1 = nn.Linear(latent_dim * 2, 256 * self.init_size**2)  # Combined noise + label\n","\n","        self.model = nn.Sequential(\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # 7x7 -> 14x14\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),   # 14x14 -> 28x28\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, img_channels, 3, 1, 1, bias=False),   # Maintain 28x28\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, noise, labels):\n","        # Combine noise and label embedding\n","        label_embed = self.label_embed(labels)\n","        gen_input = torch.cat((label_embed, noise), dim=1)\n","        out = self.l1(gen_input)\n","        out = out.view(out.size(0), 256, self.init_size, self.init_size)\n","        return self.model(out)\n","\n","# ==========================================\n","# ACGAN Discriminator with Auxiliary Classifier\n","# ==========================================\n","class Discriminator(nn.Module):\n","    def __init__(self, num_classes=10, img_channels=1):\n","        super(Discriminator, self).__init__()\n","        self.img_size = 28\n","\n","        # Shared feature extractor\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(img_channels, 64, 4, 2, 1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","        )\n","\n","        # Calculate feature dimension\n","        self.feature_dim = 256 * (self.img_size // 8) ** 2\n","\n","        # Adversarial head (real/fake)\n","        self.adv_head = nn.Sequential(\n","            nn.Linear(self.feature_dim, 1),\n","            nn.Sigmoid()\n","        )\n","\n","        # Auxiliary classification head\n","        self.aux_head = nn.Sequential(\n","            nn.Linear(self.feature_dim, num_classes),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, img):\n","        features = self.feature_extractor(img)\n","        features = features.view(features.size(0), -1)\n","\n","        validity = self.adv_head(features)\n","        label = self.aux_head(features)\n","        return validity, label\n","\n","# ==========================================\n","# ACGAN Training Function\n","# ==========================================\n","def train_acgan(generator, discriminator, dataloader, epochs, device,\n","               latent_dim=100, num_classes=10):\n","    generator.to(device)\n","    discriminator.to(device)\n","\n","    # Loss functions\n","    adversarial_loss = nn.BCELoss()\n","    auxiliary_loss = nn.CrossEntropyLoss()\n","\n","    # Optimizers\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for i, (real_imgs, real_labels) in enumerate(dataloader):\n","            batch_size = real_imgs.size(0)\n","            real_imgs = real_imgs.to(device)\n","            real_labels = real_labels.to(device)\n","\n","            # Adversarial ground truths\n","            valid = torch.ones(batch_size, 1, device=device)\n","            fake = torch.zeros(batch_size, 1, device=device)\n","\n","            # ==========================\n","            #  Train Discriminator\n","            # ==========================\n","            optimizer_D.zero_grad()\n","\n","            # Real images\n","            real_pred, real_aux = discriminator(real_imgs)\n","            d_real_loss = (adversarial_loss(real_pred, valid) +\n","                          auxiliary_loss(real_aux, real_labels)) / 2\n","\n","            # Fake images\n","            z = torch.randn(batch_size, latent_dim, device=device)\n","            gen_labels = torch.randint(0, num_classes, (batch_size,), device=device)\n","            fake_imgs = generator(z, gen_labels)\n","            fake_pred, fake_aux = discriminator(fake_imgs.detach())\n","            d_fake_loss = (adversarial_loss(fake_pred, fake) +\n","                          auxiliary_loss(fake_aux, gen_labels)) / 2\n","\n","            # Total discriminator loss\n","            d_loss = (d_real_loss + d_fake_loss) / 2\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","            # =========================\n","            #  Train Generator\n","            # =========================\n","            optimizer_G.zero_grad()\n","\n","            # Generate new fake images\n","            gen_imgs = generator(z, gen_labels)\n","            gen_pred, gen_aux = discriminator(gen_imgs)\n","\n","            # Generator loss combines adversarial and auxiliary losses\n","            g_loss = (adversarial_loss(gen_pred, valid) +\n","                     auxiliary_loss(gen_aux, gen_labels)) / 2\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","        # Print epoch progress\n","        print(f\"[Epoch {epoch+1}/{epochs}] \"\n","              f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f} \"\n","              f\"Real Acc: {(real_aux.argmax(1) == real_labels).float().mean().item():.2f} \"\n","              f\"Fake Acc: {(fake_aux.argmax(1) == gen_labels).float().mean().item():.2f}\")\n","\n","# ==========================================\n","# Data Loading (MNIST)\n","# ==========================================\n","def get_data_loader(batch_size=128):\n","    transform = transforms.Compose([\n","        transforms.Resize(28),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    dataset = torchvision.datasets.MNIST(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","# ==========================================\n","# Main Execution\n","# ==========================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","latent_dim = 100\n","num_classes = 10\n","epochs = 50\n","\n","# Initialize models\n","generator = Generator(latent_dim=latent_dim, num_classes=num_classes)\n","discriminator = Discriminator(num_classes=num_classes)\n","\n","# Get data loader\n","dataloader = get_data_loader()\n","\n","# Train ACGAN\n","train_acgan(generator, discriminator, dataloader, epochs, device,\n","            latent_dim=latent_dim, num_classes=num_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"Iqt8CmXY46NH","executionInfo":{"status":"error","timestamp":1738352746768,"user_tz":-210,"elapsed":7690,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"e84fce93-b316-42c2-fd3c-a110dc254855"},"execution_count":31,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-a0e876530724>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m# Train ACGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m train_acgan(generator, discriminator, dataloader, epochs, device, \n\u001b[0m\u001b[1;32m    197\u001b[0m             latent_dim=latent_dim, num_classes=num_classes)\n","\u001b[0;32m<ipython-input-31-a0e876530724>\u001b[0m in \u001b[0;36mtrain_acgan\u001b[0;34m(generator, discriminator, dataloader, epochs, device, latent_dim, num_classes)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mgen_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mfake_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mfake_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             d_fake_loss = (adversarial_loss(fake_pred, fake) + \n\u001b[1;32m    132\u001b[0m                           auxiliary_loss(fake_aux, gen_labels)) / 2\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-a0e876530724>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["def generate_samples(generator, num_samples, labels, device):\n","    generator.eval()\n","    with torch.no_grad():\n","        z = torch.randn(num_samples, 100, device=device)\n","        labels = torch.tensor(labels, device=device)\n","        samples = generator(z, labels).cpu()\n","    return samples\n","\n","# Generate digit '7' samples\n","samples = generate_samples(generator, 16, [7]*16, device)\n","\n","# Display generated images\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 10))\n","for i in range(16):\n","    plt.subplot(4, 4, i+1)\n","    plt.imshow(samples[i].squeeze(), cmap='gray')\n","    plt.axis('off')\n","plt.show()"],"metadata":{"id":"dTB1vCtV5BlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ==========================================\n","# InfoGAN Generator\n","# ==========================================\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim=74, num_classes=10, cont_dim=2):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim  # noise_z (62) + categorical (10) + continuous (2)\n","\n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),  # 1x1 -> 4x4\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),  # 4x4 -> 8x8\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),  # 8x8 -> 16x16\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),   # 16x16 -> 32x32\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 1, 3, 1, 1, bias=False),              # 32x32 -> 28x28\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, noise, categorical, continuous):\n","        latent = torch.cat([noise, categorical, continuous], dim=1)\n","        latent = latent.view(-1, self.latent_dim, 1, 1)\n","        return self.model(latent)[:, :, 2:30, 2:30]  # Center crop to 28x28\n","\n","# ==========================================\n","# Proper 28x28 Discriminator\n","# ==========================================\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.shared_conv = nn.Sequential(\n","            nn.Conv2d(1, 64, 4, 2, 1),           # 28x28 -> 14x14\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, 4, 2, 1),         # 14x14 -> 7x7\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, 3, 2, 1),        # 7x7 -> 4x4 (critical fix)\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","        )\n","\n","        # Correct feature dimension: 256 channels * 4x4 = 4096\n","        self.feature_dim = 256 * 4 * 4\n","\n","        # Adversarial head\n","        self.adv_head = nn.Sequential(\n","            nn.Linear(self.feature_dim, 1),\n","            nn.Sigmoid()\n","        )\n","\n","        # Q Network\n","        self.q_net = nn.Sequential(\n","            nn.Linear(self.feature_dim, 128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(128, 10 + 2)  # 10 classes + 2 continuous\n","        )\n","\n","    def forward(self, x):\n","        features = self.shared_conv(x)\n","        features = features.view(features.size(0), -1)\n","        validity = self.adv_head(features)\n","        latent_codes = self.q_net(features)\n","        return validity, latent_codes\n","\n","# ==========================================\n","# Training Function with Mutual Information\n","# ==========================================\n","def train_infogan(generator, discriminator, dataloader, epochs, device,\n","                 noise_dim=62, cat_dim=10, cont_dim=2, lambda_cat=1.0, lambda_cont=0.1):\n","    generator.to(device)\n","    discriminator.to(device)\n","\n","    # Optimizers\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n","\n","    # Loss functions\n","    adversarial_loss = nn.BCELoss()\n","    categorical_loss = nn.CrossEntropyLoss()\n","    continuous_loss = nn.MSELoss()\n","\n","    for epoch in range(epochs):\n","        for i, (real_imgs, _) in enumerate(dataloader):\n","            batch_size = real_imgs.size(0)\n","            real_imgs = real_imgs.to(device)\n","\n","            # Adversarial ground truths\n","            valid = torch.ones(batch_size, 1, device=device)\n","            fake = torch.zeros(batch_size, 1, device=device)\n","\n","            # Generate latent codes\n","            noise = torch.randn(batch_size, noise_dim, device=device)\n","            cat_codes = torch.randint(0, cat_dim, (batch_size,), device=device)\n","            cont_codes = torch.randn(batch_size, cont_dim, device=device)\n","\n","            # Convert categorical codes to one-hot\n","            cat_onehot = torch.zeros(batch_size, cat_dim, device=device)\n","            cat_onehot.scatter_(1, cat_codes.view(-1, 1), 1)\n","\n","            # ==========================\n","            #  Train Discriminator\n","            # ==========================\n","            optimizer_D.zero_grad()\n","\n","            # Real images\n","            real_pred, _ = discriminator(real_imgs)\n","            d_real_loss = adversarial_loss(real_pred, valid)\n","\n","            # Fake images\n","            fake_imgs = generator(noise, cat_onehot, cont_codes)\n","            fake_pred, _ = discriminator(fake_imgs.detach())\n","            d_fake_loss = adversarial_loss(fake_pred, fake)\n","\n","            # Total discriminator loss\n","            d_loss = (d_real_loss + d_fake_loss) / 2\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","            # =========================\n","            #  Train Generator\n","            # =========================\n","            optimizer_G.zero_grad()\n","\n","            # Generate fake images\n","            gen_imgs = generator(noise, cat_onehot, cont_codes)\n","            gen_pred, latent_pred = discriminator(gen_imgs)\n","\n","            # Adversarial loss\n","            g_adv_loss = adversarial_loss(gen_pred, valid)\n","\n","            # Mutual information loss\n","            pred_cat = latent_pred[:, :cat_dim]\n","            pred_cont = latent_pred[:, cat_dim:]\n","\n","            # Categorical loss\n","            g_cat_loss = categorical_loss(pred_cat, cat_codes)\n","\n","            # Continuous loss (assuming Gaussian distribution)\n","            g_cont_loss = continuous_loss(pred_cont, cont_codes)\n","\n","            # Total generator loss\n","            g_loss = g_adv_loss + lambda_cat*g_cat_loss + lambda_cont*g_cont_loss\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","        # Print progress\n","        print(f\"[Epoch {epoch+1}/{epochs}] \"\n","              f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f} \"\n","              f\"Cat Acc: {(pred_cat.argmax(1) == cat_codes).float().mean().item():.2f} \"\n","              f\"Cont Err: {g_cont_loss.item():.4f}\")\n","\n","# ==========================================\n","# Data Loading (MNIST)\n","# ==========================================\n","def get_data_loader(batch_size=128):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),  # Keep original 28x28\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    dataset = torchvision.datasets.MNIST(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","# ==========================================\n","# Main Execution\n","# ==========================================\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize models\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","# Get data loader\n","dataloader = get_data_loader()\n","\n","# Train InfoGAN\n","train_infogan(generator, discriminator, dataloader, epochs=100, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"m8auqVES5QDw","executionInfo":{"status":"error","timestamp":1738353919245,"user_tz":-210,"elapsed":125974,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"138f6e9c-826a-44c7-9e5f-ad4eecd0a21c"},"execution_count":36,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-ebaa76039c7a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;31m# Train InfoGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \u001b[0mtrain_infogan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-36-ebaa76039c7a>\u001b[0m in \u001b[0;36mtrain_infogan\u001b[0;34m(generator, discriminator, dataloader, epochs, device, noise_dim, cat_dim, cont_dim, lambda_cat, lambda_cont)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# Generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mgen_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-ebaa76039c7a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, noise, categorical, continuous)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Center crop to 28x28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["def generate_samples(generator, noise, categorical, continuous, device):\n","    generator.eval()\n","    with torch.no_grad():\n","        # Convert categorical to one-hot\n","        cat_onehot = torch.zeros(1, 10, device=device)\n","        cat_onehot[0, categorical] = 1\n","\n","        # Generate image\n","        noise = noise.view(1, -1).to(device)\n","        continuous = continuous.view(1, -1).to(device)\n","        sample = generator(noise, cat_onehot, continuous).cpu()\n","    return sample\n","\n","# Example: Generate digit 3 with specific style\n","noise = torch.randn(62)\n","sample = generate_samples(generator, noise, categorical=3,\n","                         continuous=torch.tensor([0.5, -0.7]), device=device)"],"metadata":{"id":"ATOVSRro9tud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","import numpy as np\n","\n","# ==========================================\n","# Contrastive Augmentation Module\n","# ==========================================\n","class MNISTContrastiveTransform:\n","    def __init__(self):\n","        self.aug = transforms.Compose([\n","            transforms.RandomRotation(15),\n","            transforms.RandomAffine(0, translate=(0.1, 0.1)),\n","            transforms.RandomApply([transforms.GaussianBlur(3)], p=0.5),\n","        ])\n","\n","    def __call__(self, x):\n","        return self.aug(x), self.aug(x)  # Two augmented views\n","\n","# ==========================================\n","# Contrastive Generator\n","# ==========================================\n","class ContrastiveGenerator(nn.Module):\n","    def __init__(self, latent_dim=128, projection_dim=64):\n","        super().__init__()\n","        self.latent_dim = latent_dim  # Add this line\n","        self.main = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.BatchNorm1d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(256, 512),\n","            nn.BatchNorm1d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(512, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Linear(1024, 784),\n","            nn.Tanh()\n","        )\n","\n","        self.projection = nn.Sequential(\n","            nn.Linear(784, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, projection_dim)\n","        )\n","\n","    def forward(self, z):\n","        img = self.main(z)\n","        img = img.view(-1, 1, 28, 28)\n","        features = self.projection(img.view(img.size(0), -1))\n","        return img, features\n","\n","# ==========================================\n","# Contrastive Discriminator\n","# ==========================================\n","class ContrastiveDiscriminator(nn.Module):\n","    def __init__(self, projection_dim=64):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, 3, 2, 1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, 3, 2, 1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, 3, 2, 1),\n","            nn.LeakyReLU(0.2),\n","            nn.AdaptiveAvgPool2d(1)\n","        )\n","\n","        self.projection = nn.Sequential(\n","            nn.Linear(256, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, projection_dim)\n","        )\n","\n","        self.discriminator_head = nn.Linear(256, 1)\n","\n","    def forward(self, x):\n","        features = self.encoder(x).squeeze()\n","        projection = self.projection(features)\n","        validity = self.discriminator_head(features)\n","        return validity, projection\n","\n","# ==========================================\n","# Contrastive Loss (NT-Xent)\n","# ==========================================\n","class ContrastiveLoss(nn.Module):\n","    def __init__(self, temperature=0.5):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def forward(self, projections):\n","        # Normalize projections\n","        projections = nn.functional.normalize(projections, p=2, dim=1)\n","\n","        # Compute similarity matrix\n","        sim_matrix = torch.mm(projections, projections.T) / self.temperature\n","\n","        # Create labels (positive pairs are adjacent in batch)\n","        batch_size = projections.size(0)\n","        labels = torch.arange(batch_size, device=projections.device)\n","        labels = (labels - labels % 2 * 2) + 1  # Pair each element with next\n","\n","        return self.criterion(sim_matrix, labels)\n","\n","# ==========================================\n","# Contrastive GAN Trainer\n","# ==========================================\n","class ContrastiveGAN:\n","    def __init__(self, device='cuda', latent_dim=128, projection_dim=64):\n","        self.device = device\n","        self.G = ContrastiveGenerator(latent_dim, projection_dim).to(device)\n","        self.D = ContrastiveDiscriminator(projection_dim).to(device)\n","\n","        self.optim_G = optim.Adam(self.G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n","        self.optim_D = optim.Adam(self.D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n","\n","        self.criterion_adv = nn.BCEWithLogitsLoss()\n","        self.criterion_cont = ContrastiveLoss(temperature=0.07)\n","\n","        self.augment = MNISTContrastiveTransform()\n","\n","    def _get_contrastive_batch(self, real_imgs):\n","        augmented = [self.augment(img) for img in real_imgs.cpu()]\n","        view1 = torch.stack([a[0] for a in augmented]).to(self.device)\n","        view2 = torch.stack([a[1] for a in augmented]).to(self.device)\n","        return torch.cat([view1, view2], dim=0)\n","\n","    def train_step(self, real_imgs):\n","        # Prepare data\n","        real_imgs = real_imgs.to(self.device)\n","        batch_size = real_imgs.size(0)\n","\n","        # Generate fake images\n","        z = torch.randn(batch_size, self.G.latent_dim, device=self.device)\n","        fake_imgs, fake_proj = self.G(z)\n","\n","        # ---------------------\n","        # Train Discriminator\n","        # ---------------------\n","        self.optim_D.zero_grad()\n","\n","        # Process real images with contrastive views\n","        cont_batch = self._get_contrastive_batch(real_imgs)\n","        real_validity, real_proj = self.D(cont_batch)\n","\n","        # Combine real and fake projections\n","        D_proj = torch.cat([real_proj, fake_proj.detach()], dim=0)\n","\n","        # Compute losses\n","        d_loss_adv = self.criterion_adv(\n","            real_validity[:batch_size*2],\n","            torch.ones(batch_size*2, 1, device=self.device)\n","        )\n","        d_loss_cont = self.criterion_cont(D_proj)\n","        d_loss = d_loss_adv + 0.5 * d_loss_cont\n","        d_loss.backward()\n","        self.optim_D.step()\n","\n","        # -----------------\n","        # Train Generator\n","        # -----------------\n","        self.optim_G.zero_grad()\n","\n","        _, fake_proj = self.D(fake_imgs)\n","        g_loss_adv = self.criterion_adv(\n","            self.D(fake_imgs)[0],\n","            torch.ones(batch_size, 1, device=self.device)\n","        )\n","        g_loss_cont = self.criterion_cont(fake_proj)\n","        g_loss = g_loss_adv + 0.5 * g_loss_cont\n","        g_loss.backward()\n","        self.optim_G.step()\n","\n","        return {\n","            'd_loss': d_loss.item(),\n","            'g_loss': g_loss.item(),\n","            'd_cont': d_loss_cont.item(),\n","            'g_cont': g_loss_cont.item()\n","        }\n","\n","# ==========================================\n","# Training Pipeline\n","# ==========================================\n","def train_contrastive_gan(epochs=100, batch_size=128):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    dataset = datasets.MNIST(\n","        root='./data',\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = ContrastiveGAN(device=device)\n","\n","    for epoch in range(epochs):\n","        for i, (imgs, _) in enumerate(loader):\n","            metrics = model.train_step(imgs)\n","\n","            if i % 50 == 0:\n","                print(f\"Epoch {epoch+1}/{epochs} | Batch {i} | \"\n","                      f\"D Loss: {metrics['d_loss']:.4f} | G Loss: {metrics['g_loss']:.4f} \"\n","                      f\"(D Cont: {metrics['d_cont']:.3f}, G Cont: {metrics['g_cont']:.3f})\")\n","\n","    print(\"Training complete!\")\n","    return model\n","\n","# ==========================================\n","# Sample Generation\n","# ==========================================\n","def generate_samples(model, num_samples=16):\n","    model.G.eval()\n","    with torch.no_grad():\n","        z = torch.randn(num_samples, 128).to(model.device)\n","        samples, _ = model.G(z)\n","        return samples.cpu().numpy()\n","\n","trained_model = train_contrastive_gan(epochs=50)\n","samples = generate_samples(trained_model)\n","\n","# Visualize samples\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 10))\n","for i in range(16):\n","    plt.subplot(4, 4, i+1)\n","    plt.imshow(samples[i].squeeze(), cmap='gray')\n","    plt.axis('off')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"FlMnOgBc99-o","executionInfo":{"status":"error","timestamp":1738354783728,"user_tz":-210,"elapsed":39784,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"800ee3a4-3424-4542-e26e-8529b700f2e7"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50 | Batch 0 | D Loss: 3.6839 | G Loss: 3.0856 (D Cont: 6.002, G Cont: 4.852)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-038d30e0109f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_contrastive_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-038d30e0109f>\u001b[0m in \u001b[0;36mtrain_contrastive_gan\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-038d30e0109f>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, real_imgs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Process real images with contrastive views\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mcont_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_contrastive_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mreal_validity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Combine real and fake projections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-038d30e0109f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mvalidity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","# ==========================\n","# GAN Building Blocks\n","# ==========================\n","\n","import torch.nn as nn\n","\n","class D_block(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=4, strides=2, padding=1, alpha=0.2, dropout=0.3):\n","        super(D_block, self).__init__()\n","        self.conv2d = nn.Conv2d(\n","            in_channels=in_channels, out_channels=out_channels,\n","            kernel_size=kernel_size, stride=strides, padding=padding, bias=False\n","        )\n","        self.activation = nn.LeakyReLU(alpha, inplace=True)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.dropout(self.activation(self.conv2d(x)))\n","\n","class G_block(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, strides=2, padding=2):\n","        super(G_block, self).__init__()\n","        self.conv2d_trans = nn.ConvTranspose2d(\n","            in_channels=in_channels, out_channels=out_channels,\n","            kernel_size=kernel_size, stride=strides, padding=padding, bias=False\n","        )\n","        self.batch_norm = nn.BatchNorm2d(out_channels)\n","        self.activation = nn.LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        return self.activation(self.batch_norm(self.conv2d_trans(x)))\n","\n","# ==========================\n","# GAN Models\n","# ==========================\n","\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim=100):\n","        super(Generator, self).__init__()\n","        self.latent_dim = latent_dim\n","\n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(self.latent_dim, 256, kernel_size=4, stride=1, padding=0, bias=False), # Initial upsampling\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # G_block(256, 256),  # Input channels now match\n","            G_block(256, 128),\n","            G_block(128, 64),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=0, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        z = z.view(-1, self.latent_dim, 1, 1)  # Reshape to (batch_size, latent_dim, 1, 1)\n","        return self.model(z)\n","\n","class Discriminator(nn.Module):  # No changes needed here, but included for completeness\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            D_block(1, 64, kernel_size=4, strides=2, padding=1),  # 28x28 -> 14x14\n","            D_block(64, 128, kernel_size=4, strides=2, padding=1),  # 14x14 -> 7x7\n","            D_block(128, 256, kernel_size=3, strides=2, padding=1),  # 7x7 -> 4x4\n","            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),  # 4x4 -> 1x1\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x).view(-1, 1)\n","\n","\n","# ==========================\n","# WGAN-GP Critic\n","# ==========================\n","\n","class Critic(nn.Module):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.model = nn.Sequential(\n","            D_block(1, 64, kernel_size=4, strides=2, padding=1),  # 28x28 -> 14x14\n","            D_block(64, 128, kernel_size=4, strides=2, padding=1),  # 14x14 -> 7x7\n","            D_block(128, 256, kernel_size=3, strides=2, padding=1),  # 7x7 -> 4x4\n","            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False)  # 4x4 -> 1x1  NO SIGMOID\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x).view(-1, 1)\n","\n","# ==========================\n","# InfoGAN\n","# ==========================\n","\n","class InfoGANGenerator(nn.Module):\n","    def __init__(self, latent_dim=100):\n","        super(InfoGANGenerator, self).__init__()\n","        self.model = nn.Sequential(\n","            G_block(latent_dim, 256),\n","            G_block(256, 128),\n","            G_block(128, 64),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        return self.model(z)\n","\n","class InfoGANDiscriminator(nn.Module):\n","    def __init__(self):\n","        super(InfoGANDiscriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            D_block(1, 64),\n","            D_block(64, 128),\n","            D_block(128, 256),\n","            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x).view(-1, 1)\n","# ==========================\n","# Conditional GAN (CGAN)\n","# ==========================\n","\n","class ConditionalGenerator(nn.Module):\n","    def __init__(self, latent_dim=100, num_classes=10):\n","        super(ConditionalGenerator, self).__init__()\n","        self.label_emb = nn.Embedding(num_classes, num_classes)\n","        self.model = nn.Sequential(\n","            G_block(latent_dim + num_classes, 256),\n","            G_block(256, 128),\n","            G_block(128, 64),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z, labels):\n","        z = torch.cat((z, self.label_emb(labels)), dim=1)\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        return self.model(z)\n","\n","class ConditionalDiscriminator(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ConditionalDiscriminator, self).__init__()\n","        self.label_emb = nn.Embedding(num_classes, num_classes)\n","        self.model = nn.Sequential(\n","            D_block(1 + num_classes, 64),\n","            D_block(64, 128),\n","            D_block(128, 256),\n","            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x, labels):\n","        labels = self.label_emb(labels).unsqueeze(2).unsqueeze(3).expand(-1, -1, x.size(2), x.size(3))\n","        x = torch.cat((x, labels), dim=1)\n","        return self.model(x).view(-1, 1)\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# ==========================\n","# Contrastive Learning GAN\n","# ==========================\n","class ContrastiveGANGenerator(nn.Module):\n","    def __init__(self, latent_dim=100):\n","        super(ContrastiveGANGenerator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        return self.model(z)\n","\n","class ContrastiveGANDiscriminator(nn.Module):\n","    def __init__(self):\n","        super(ContrastiveGANDiscriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x).view(-1, 1)\n","\n","# ==========================\n","# Auxiliary Classifier GAN (ACGAN)\n","# ==========================\n","class ACGANGenerator(nn.Module):\n","    def __init__(self, latent_dim=100, num_classes=10):\n","        super(ACGANGenerator, self).__init__()\n","        self.label_embedding = nn.Embedding(num_classes, num_classes)\n","        self.model = nn.Sequential(\n","            nn.ConvTranspose2d(latent_dim + num_classes, 256, kernel_size=4, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z, labels):\n","        z = torch.cat((z, self.label_embedding(labels)), dim=1)\n","        z = z.view(z.size(0), z.size(1), 1, 1)\n","        return self.model(z)\n","\n","class ACGANDiscriminator(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ACGANDiscriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        self.adv_layer = nn.Sequential(nn.Linear(256*4*4, 1), nn.Sigmoid())\n","        self.aux_layer = nn.Sequential(nn.Linear(256*4*4, num_classes), nn.Softmax(dim=1))\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        x = x.view(x.size(0), -1)\n","        validity = self.adv_layer(x)\n","        label = self.aux_layer(x)\n","        return validity, label\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd\n","\n","def train_gan(generator, discriminator, dataloader, epochs, device):\n","    criterion = nn.BCELoss()\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for real_images, _ in dataloader:\n","            real_images = real_images.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # Train discriminator\n","            optimizer_D.zero_grad()\n","            noise = torch.randn(batch_size, 100, 1, 1, device=device)\n","            fake_images = generator(noise)\n","            real_labels = torch.ones(batch_size, 1, device=device)\n","            fake_labels = torch.zeros(batch_size, 1, device=device)\n","            loss_D = criterion(discriminator(real_images), real_labels) + criterion(discriminator(fake_images.detach()), fake_labels)\n","            loss_D.backward()\n","            optimizer_D.step()\n","\n","            # Train generator\n","            optimizer_G.zero_grad()\n","            loss_G = criterion(discriminator(fake_images), real_labels)\n","            loss_G.backward()\n","            optimizer_G.step()\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss_D: {loss_D.item()}, Loss_G: {loss_G.item()}\")\n","\n","\n","def train_conditional_gan(generator, discriminator, dataloader, epochs, device, num_classes):\n","    criterion = nn.BCELoss()\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for real_images, labels in dataloader:\n","            real_images, labels = real_images.to(device), labels.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # One-hot encode labels\n","            labels_onehot = torch.zeros(batch_size, num_classes, device=device)\n","            labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n","\n","            # Train discriminator\n","            optimizer_D.zero_grad()\n","            noise = torch.randn(batch_size, 100, device=device)\n","            fake_images = generator(noise, labels_onehot)\n","            real_labels = torch.ones(batch_size, 1, device=device)\n","            fake_labels = torch.zeros(batch_size, 1, device=device)\n","            loss_D = criterion(discriminator(real_images, labels_onehot), real_labels) + criterion(discriminator(fake_images.detach(), labels_onehot), fake_labels)\n","            loss_D.backward()\n","            optimizer_D.step()\n","\n","            # Train generator\n","            optimizer_G.zero_grad()\n","            loss_G = criterion(discriminator(fake_images, labels_onehot), real_labels)\n","            loss_G.backward()\n","            optimizer_G.step()\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss_D: {loss_D.item()}, Loss_G: {loss_G.item()}\")\n","\n","\n","def train_infogan(generator, discriminator, dataloader, epochs, device):\n","    criterion = nn.BCELoss()\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for real_images, _ in dataloader:\n","            real_images = real_images.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # Train discriminator\n","            optimizer_D.zero_grad()\n","            noise = torch.randn(batch_size, 100, device=device)\n","            fake_images = generator(noise)\n","            real_labels = torch.ones(batch_size, 1, device=device)\n","            fake_labels = torch.zeros(batch_size, 1, device=device)\n","            loss_D = criterion(discriminator(real_images), real_labels) + criterion(discriminator(fake_images.detach()), fake_labels)\n","            loss_D.backward()\n","            optimizer_D.step()\n","\n","            # Train generator\n","            optimizer_G.zero_grad()\n","            loss_G = criterion(discriminator(fake_images), real_labels)\n","            loss_G.backward()\n","            optimizer_G.step()\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss_D: {loss_D.item()}, Loss_G: {loss_G.item()}\")\n","\n","def train_contrastive_gan(generator, discriminator, dataloader, epochs, device):\n","    criterion = nn.BCELoss()\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for real_images, _ in dataloader:\n","            real_images = real_images.to(device)\n","            batch_size = real_images.size(0)\n","\n","            optimizer_D.zero_grad()\n","            noise = torch.randn(batch_size, 100, device=device)\n","            fake_images = generator(noise)\n","            real_labels = torch.ones(batch_size, 1, device=device)\n","            fake_labels = torch.zeros(batch_size, 1, device=device)\n","            loss_D = criterion(discriminator(real_images), real_labels) + criterion(discriminator(fake_images.detach()), fake_labels)\n","            loss_D.backward()\n","            optimizer_D.step()\n","\n","            optimizer_G.zero_grad()\n","            loss_G = criterion(discriminator(fake_images), real_labels)\n","            loss_G.backward()\n","            optimizer_G.step()\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss_D: {loss_D.item()}, Loss_G: {loss_G.item()}\")\n","\n","# Define training functions for ACGAN and CycleGAN\n","\n","def train_acgan(generator, discriminator, dataloader, epochs, device):\n","    criterion_adv = nn.BCELoss()\n","    criterion_aux = nn.CrossEntropyLoss()\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for real_images, labels in dataloader:\n","            real_images, labels = real_images.to(device), labels.to(device)\n","            batch_size = real_images.size(0)\n","\n","            optimizer_D.zero_grad()\n","            noise = torch.randn(batch_size, 100, device=device)\n","            fake_images = generator(noise, labels)\n","            real_validity, real_aux = discriminator(real_images)\n","            fake_validity, fake_aux = discriminator(fake_images.detach())\n","\n","            loss_D = criterion_adv(real_validity, torch.ones_like(real_validity)) + \\\n","                     criterion_adv(fake_validity, torch.zeros_like(fake_validity)) + \\\n","                     criterion_aux(real_aux, labels)\n","            loss_D.backward()\n","            optimizer_D.step()\n","\n","            optimizer_G.zero_grad()\n","            validity, pred_label = discriminator(fake_images)\n","            loss_G = criterion_adv(validity, torch.ones_like(validity)) + criterion_aux(pred_label, labels)\n","            loss_G.backward()\n","            optimizer_G.step()\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss_D: {loss_D.item()}, Loss_G: {loss_G.item()}\")\n","\n","gan_configurations = {\n","    \"GAN\": {\n","        \"generator\": Generator,\n","        \"discriminator\": Discriminator,\n","        \"train_function\": train_gan\n","    },\n","    \"CGAN\": {\n","        \"generator\": ConditionalGenerator,\n","        \"discriminator\": ConditionalDiscriminator,\n","        \"train_function\": train_conditional_gan\n","    },\n","    \"WGAN-GP\": {\n","        \"generator\": Generator,\n","        \"critic\": Critic,\n","        \"train_function\": train_wgan_gp,\n","        \"train_kwargs\": {\"lambda_gp\": 10}\n","    },\n","    \"InfoGAN\": {\n","        \"generator\": InfoGANGenerator,\n","        \"discriminator\": InfoGANDiscriminator,\n","        \"train_function\": train_infogan\n","    },\n","    \"Contrastive-GAN\": {\n","        \"generator\": ContrastiveGANGenerator,\n","        \"discriminator\": ContrastiveGANDiscriminator,\n","        \"train_function\": train_contrastive_gan\n","    },\n","    \"ACGAN\": {\n","        \"generator\": ACGANGenerator,\n","        \"discriminator\": ACGANDiscriminator,\n","        \"train_function\": train_acgan\n","    }\n","}"],"metadata":{"id":"L2qnUiLfw751","executionInfo":{"status":"ok","timestamp":1738345479691,"user_tz":-210,"elapsed":8,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def check_generator_dimensions(generator_class, latent_dim=100, num_classes=None):\n","    \"\"\"Check generator output dimensions.\"\"\"\n","    generator = generator_class(latent_dim, num_classes) if num_classes is not None else generator_class(latent_dim)\n","    generator.eval()\n","\n","    with torch.no_grad():\n","        noise = torch.randn(1, latent_dim, 1, 1)\n","        if num_classes is not None:\n","            labels = torch.randint(0, num_classes, (1,)).to(torch.long)\n","            fake_image = generator(noise, labels)\n","        else:\n","            fake_image = generator(noise)\n","\n","    print(f\"{generator_class.__name__} output shape: {fake_image.shape}\")\n","\n","def check_discriminator_dimensions(discriminator_class, num_classes=None):\n","    \"\"\"Check discriminator input-output dimensions.\"\"\"\n","    discriminator = discriminator_class(num_classes) if num_classes is not None else discriminator_class()\n","    discriminator.eval()\n","\n","    with torch.no_grad():\n","        fake_image = torch.randn(1, 1, 28, 28)\n","        if num_classes is not None:\n","            labels = torch.randint(0, num_classes, (1,)).to(torch.long)\n","            score = discriminator(fake_image, labels)\n","        else:\n","            score = discriminator(fake_image)\n","\n","    print(f\"{discriminator_class.__name__} output shape: {score.shape}\")\n","\n","# Run checks\n","check_generator_dimensions(Generator)\n","check_generator_dimensions(InfoGANGenerator)\n","check_generator_dimensions(ConditionalGenerator, num_classes=10)\n","# check_generator_dimensions(ContrastiveGANGenerator)\n","check_generator_dimensions(ACGANGenerator, num_classes=10)\n","\n","check_discriminator_dimensions(Discriminator)\n","check_discriminator_dimensions(Critic)\n","check_discriminator_dimensions(InfoGANDiscriminator)\n","check_discriminator_dimensions(ConditionalDiscriminator, num_classes=10)\n","# check_discriminator_dimensions(ContrastiveGANDiscriminator)\n","check_discriminator_dimensions(ACGANDiscriminator, num_classes=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"mdF0cANUDQhO","executionInfo":{"status":"error","timestamp":1738347824866,"user_tz":-210,"elapsed":2212,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"dfbef2f7-d94b-45f4-b2db-76b60ef67b62"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Generator output shape: torch.Size([1, 1, 32, 32])\n"]},{"output_type":"error","ename":"TypeError","evalue":"InfoGANGenerator.forward() missing 2 required positional arguments: 'cat' and 'cont'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-b89f7cf4e020>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Run checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mcheck_generator_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcheck_generator_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInfoGANGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mcheck_generator_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConditionalGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# check_generator_dimensions(ContrastiveGANGenerator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-b89f7cf4e020>\u001b[0m in \u001b[0;36mcheck_generator_dimensions\u001b[0;34m(generator_class, latent_dim, num_classes)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mfake_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mfake_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{generator_class.__name__} output shape: {fake_image.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: InfoGANGenerator.forward() missing 2 required positional arguments: 'cat' and 'cont'"]}]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","\n","# ==========================\n","# FAKE INITIALIZATION FOR DEBUGGING\n","# ==========================\n","\n","def generate_fake_samples(gan_type=\"GAN\", latent_dim=100, device=\"cpu\", num_samples=16):\n","    \"\"\"Generate fake samples using a random generator initialization.\"\"\"\n","    generator = Generator(latent_dim).to(device)\n","    generator.eval()\n","\n","    with torch.no_grad():\n","        noise = torch.randn(num_samples, latent_dim, 1, 1, device=device)\n","        fake_images = generator(noise).cpu()\n","\n","    return fake_images\n","\n","# ==========================\n","# PLOT GENERATED SAMPLES\n","# ==========================\n","\n","def plot_fake_samples(fake_images, num_samples=16):\n","    \"\"\"Plot generated samples for visualization.\"\"\"\n","    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n","    fake_images = fake_images.squeeze().numpy()\n","\n","    for i, ax in enumerate(axes.flatten()):\n","        if i < num_samples:\n","            ax.imshow(fake_images[i], cmap=\"gray\")\n","            ax.axis(\"off\")\n","\n","    plt.show()\n","\n","# ==========================\n","# RUN DEBUGGING\n","# ==========================\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fake_images = generate_fake_samples(device=device)\n","print(f\"Shape of fake_images: {fake_images.shape}\")\n","plot_fake_samples(fake_images)\n","\n","# ==========================\n","# FAKE INITIALIZATION FOR DEBUGGING DISCRIMINATOR\n","# ==========================\n","\n","def test_discriminator(gan_type=\"GAN\", device=\"cpu\", num_samples=16):\n","    \"\"\"Generate random samples and pass them through the discriminator for debugging.\"\"\"\n","    discriminator = Discriminator().to(device)\n","    discriminator.eval()\n","\n","    with torch.no_grad():\n","        fake_images = torch.randn(num_samples, 1, 28, 28, device=device)  # MNIST-like shape\n","        scores = discriminator(fake_images).cpu()\n","\n","    return fake_images, scores\n","\n","# ==========================\n","# PLOT DISCRIMINATOR OUTPUT\n","# ==========================\n","\n","def plot_discriminator_scores(fake_images, scores, num_samples=16):\n","    \"\"\"Plot fake images with their corresponding discriminator scores.\"\"\"\n","    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n","    fake_images = fake_images.squeeze().numpy()\n","    scores = scores.squeeze().numpy()\n","\n","    for i, ax in enumerate(axes.flatten()):\n","        if i < num_samples:\n","            ax.imshow(fake_images[i], cmap=\"gray\")\n","            ax.set_title(f\"Score: {scores[i]:.2f}\")\n","            ax.axis(\"off\")\n","\n","    plt.show()\n","\n","# ==========================\n","# RUN DEBUGGING\n","# ==========================\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fake_images, scores = test_discriminator(device=device)\n","print(f\"Shape of fake_images: {fake_images.shape}\")\n","print(f\"Shape of scores: {scores.shape}\")\n","plot_discriminator_scores(fake_images, scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"id":"6SAsI_1z-xAN","executionInfo":{"status":"ok","timestamp":1738337388377,"user_tz":-210,"elapsed":1357,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"2b4f2f82-c41e-4bde-af78-9b050ce9b402"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of fake_images: torch.Size([16, 1, 28, 28])\n","Shape of scores: torch.Size([16, 1])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x600 with 16 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets, transforms\n","from scipy.linalg import sqrtm\n","from scipy.stats import entropy, spearmanr, gaussian_kde\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.neighbors import NearestNeighbors\n","from torch.optim import Adam\n","\n","# ==========================\n","# CONFIGURATION & DATA LOADING\n","# ==========================\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Configuration\n","config = {\n","    \"gan_type\": \"GAN\",  # Options: GAN, CGAN, WGAN-GP, InfoGAN\n","    \"latent_dim\": 100,\n","    \"num_classes\": 10,\n","    \"categorical_dim\": 10,\n","    \"epochs\": 1,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 2e-4,\n","    \"device\": device,\n","    \"beta1\": 0.5,\n","    \"beta2\": 0.999,\n","    \"save_path\": \"gan_model.pth\",\n","    \"eval_fraction\": 0.1,\n","    \"kl_method\": \"histogram\",\n","    \"show_model_architecture\": True,\n","}\n","\n","# Load MNIST dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n","num_samples = len(dataset)\n","eval_size = int(num_samples * config[\"eval_fraction\"])\n","train_size = num_samples - eval_size\n","\n","train_data, eval_data = random_split(dataset, [train_size, eval_size])\n","train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n","eval_loader = DataLoader(eval_data, batch_size=config[\"batch_size\"], shuffle=False)\n","\n","config.update({\n","    \"data_loader\": train_loader,\n","    \"eval_loader\": eval_loader,\n","})\n","\n","# ==========================\n","# MODEL INITIALIZATION\n","# ==========================\n","\n","def initialize_gan_components(config):\n","    \"\"\"Initialize GAN components based on type.\"\"\"\n","    components = {}\n","    gan_type = config[\"gan_type\"]\n","    print(f\"Initializing {gan_type} components...\")\n","\n","    if gan_type == \"GAN\":\n","        components[\"generator\"] = Generator(config[\"latent_dim\"]).to(config[\"device\"])\n","        components[\"discriminator\"] = Discriminator().to(config[\"device\"])\n","    elif gan_type == \"CGAN\":\n","        components[\"generator\"] = ConditionalGenerator(config[\"latent_dim\"], config[\"num_classes\"]).to(config[\"device\"])\n","        components[\"discriminator\"] = ConditionalDiscriminator(config[\"num_classes\"]).to(config[\"device\"])\n","    elif gan_type == \"WGAN-GP\":\n","        components[\"generator\"] = Generator(config[\"latent_dim\"]).to(config[\"device\"])\n","        components[\"critic\"] = Critic().to(config[\"device\"])\n","    elif gan_type == \"InfoGAN\":\n","        components[\"generator\"] = InfoGANGenerator(config[\"latent_dim\"]).to(config[\"device\"])\n","        components[\"discriminator\"] = InfoGANDiscriminator().to(config[\"device\"])\n","    else:\n","        raise ValueError(f\"Unsupported GAN type: {gan_type}\")\n","\n","    print(f\"Initialized {gan_type} components: {components}\")\n","    if config[\"show_model_architecture\"]:\n","        for name, model in components.items():\n","            print(f\"{name} architecture:\\n{model}\")\n","    return components\n","\n","# ==========================\n","# EVALUATION METRICS\n","# ==========================\n","\n","def calculate_fid(real_samples, generated_samples, eps=1e-6):\n","    \"\"\"Compute FrÃ©chet Inception Distance (FID).\"\"\"\n","    if real_samples.ndim > 2:\n","        real_samples = real_samples.reshape(real_samples.shape[0], -1)\n","    if generated_samples.ndim > 2:\n","        generated_samples = generated_samples.reshape(generated_samples.shape[0], -1)\n","\n","    mu1, sigma1 = np.mean(real_samples, axis=0), np.cov(real_samples, rowvar=False)\n","    mu2, sigma2 = np.mean(generated_samples, axis=0), np.cov(generated_samples, rowvar=False)\n","\n","    diff = mu1 - mu2\n","    covmean = sqrtm(sigma1 @ sigma2 + np.eye(sigma1.shape[0]) * eps)\n","\n","    if np.iscomplexobj(covmean):\n","        covmean = covmean.real\n","\n","    return np.sum(diff**2) + np.trace(sigma1 + sigma2 - 2 * covmean)\n","\n","def calculate_kl_divergence(real_samples, generated_samples, method=\"histogram\"):\n","    \"\"\"Compute KL divergence between real and generated samples.\"\"\"\n","    if method == \"histogram\":\n","        real_prob = np.histogram(real_samples, bins=50, density=True)[0]\n","        gen_prob = np.histogram(generated_samples, bins=50, density=True)[0]\n","        real_prob += 1e-10\n","        gen_prob += 1e-10\n","    elif method == \"kde\":\n","        real_kde = gaussian_kde(real_samples.T)\n","        gen_kde = gaussian_kde(generated_samples.T)\n","        sample_points = np.linspace(\n","            min(real_samples.min(), generated_samples.min()),\n","            max(real_samples.max(), generated_samples.max()),\n","            100\n","        )\n","        real_prob = real_kde(sample_points) + 1e-10\n","        gen_prob = gen_kde(sample_points) + 1e-10\n","    else:\n","        raise ValueError(\"Invalid method. Choose 'histogram' or 'kde'.\")\n","    return entropy(real_prob, gen_prob)\n","\n","def calculate_cosine_similarity(real_samples, generated_samples):\n","    \"\"\"Compute cosine similarity between real and generated samples.\"\"\"\n","    return np.mean(cosine_similarity(real_samples, generated_samples))\n","\n","def rank_similarity(real_samples, generated_samples):\n","    \"\"\"Compute Spearman Rank Correlation.\"\"\"\n","    min_size = min(real_samples.shape[0], generated_samples.shape[0])\n","    real_samples, generated_samples = real_samples[:min_size], generated_samples[:min_size]\n","    real_rank = np.argsort(real_samples, axis=0)\n","    gen_rank = np.argsort(generated_samples, axis=0)\n","    return spearmanr(real_rank.flatten(), gen_rank.flatten()).correlation\n","\n","def unique_embedding_ratio(generated_samples):\n","    \"\"\"Compute the ratio of unique samples in the generated set.\"\"\"\n","    unique_samples = np.unique(generated_samples, axis=0)\n","    return len(unique_samples) / len(generated_samples)\n","\n","def calculate_wasserstein_distance(real_samples, generated_samples):\n","    \"\"\"Compute Wasserstein Distance between real and generated samples.\"\"\"\n","    real_samples = (real_samples - np.mean(real_samples)) / (np.std(real_samples) + 1e-8)\n","    generated_samples = (generated_samples - np.mean(generated_samples)) / (np.std(generated_samples) + 1e-8)\n","    return wasserstein_distance(real_samples.flatten(), generated_samples.flatten())\n","\n","def calculate_coverage_score(real_samples, generated_samples, n_neighbors=5):\n","    \"\"\"Compute Coverage Score.\"\"\"\n","    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n","    neigh.fit(generated_samples)\n","    distances, _ = neigh.kneighbors(real_samples)\n","    return np.mean(distances[:, 0] < 0.1)\n","\n","def calculate_memorization_score(real_samples, generated_samples, n_neighbors=1, tolerance=1e-3):\n","    \"\"\"Compute Memorization Score.\"\"\"\n","    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n","    neigh.fit(real_samples)\n","    distances, _ = neigh.kneighbors(generated_samples)\n","    return np.mean(distances[:, 0] < tolerance)\n","\n","def aggregate_quality_score(fid, kl, cosine, rank_corr):\n","    \"\"\"Compute a weighted quality score based on multiple metrics.\"\"\"\n","    fid_norm = 1 / (1 + fid)\n","    kl_norm = 1 / (1 + kl)\n","    cosine_norm = cosine\n","    rank_norm = (rank_corr + 1) / 2\n","    return (0.4 * fid_norm) + (0.2 * kl_norm) + (0.2 * cosine_norm) + (0.2 * rank_norm)\n","\n","def generate_synthetic_samples(generator, num_samples=1000, latent_dim=100, device=\"cuda\"):\n","    \"\"\"Generate synthetic samples using a trained GAN generator.\"\"\"\n","    generator.eval()\n","    with torch.no_grad():\n","        latent_vectors = torch.randn(num_samples, latent_dim, 1, 1, device=device)\n","        generated_samples = generator(latent_vectors)\n","    return generated_samples.cpu()\n","\n","def evaluate_gan(gan_components, config):\n","    \"\"\"Evaluate GAN with multiple metrics.\"\"\"\n","    device = config[\"device\"]\n","    generator = gan_components[\"generator\"]\n","\n","    # Generate synthetic samples\n","    generated_samples = generate_synthetic_samples(generator, num_samples=1000, latent_dim=config[\"latent_dim\"], device=device)\n","    generated_dataloader = DataLoader(generated_samples, batch_size=config[\"batch_size\"], shuffle=False)\n","\n","    # Extract real and generated samples\n","    real_samples = torch.cat([batch for batch in config[\"data_loader\"]], dim=0).cpu().numpy()\n","    gen_samples = torch.cat([batch for batch in generated_dataloader], dim=0).cpu().numpy()\n","\n","    # Flatten the samples if they have more than 2 dimensions\n","    if real_samples.ndim > 2:\n","        real_samples = real_samples.reshape(real_samples.shape[0], -1)\n","    if gen_samples.ndim > 2:\n","        gen_samples = gen_samples.reshape(gen_samples.shape[0], -1)\n","\n","    # Calculate metrics\n","    metrics = {\n","        \"FID\": calculate_fid(real_samples, gen_samples),\n","        \"KL Divergence\": calculate_kl_divergence(real_samples, gen_samples, method=config[\"kl_method\"]),\n","        \"Cosine Similarity\": calculate_cosine_similarity(real_samples, gen_samples),\n","        \"Spearman Rank Correlation\": rank_similarity(real_samples, gen_samples),\n","        \"Wasserstein Distance\": calculate_wasserstein_distance(real_samples, gen_samples),\n","        \"Coverage Score\": calculate_coverage_score(real_samples, gen_samples),\n","        \"Memorization Score\": calculate_memorization_score(real_samples, gen_samples),\n","        \"Unique Embedding Ratio\": unique_embedding_ratio(gen_samples)\n","    }\n","\n","    # Compute aggregate quality score\n","    metrics[\"Aggregate Quality Score\"] = aggregate_quality_score(\n","        metrics[\"FID\"],\n","        metrics[\"KL Divergence\"],\n","        metrics[\"Cosine Similarity\"],\n","        metrics[\"Spearman Rank Correlation\"]\n","    )\n","\n","    # Print results\n","    print(json.dumps(metrics, indent=4))\n","\n","    # Save results\n","    save_report(metrics, config[\"gan_type\"])\n","\n","def save_report(metrics, gan_type, embedding_identifier):\n","    \"\"\"Saves evaluation metrics as a JSON file.\"\"\"\n","    report_filename = f\"evaluation_{gan_type}.json\"\n","    report_path = os.path.join(\"./reports\", report_filename)\n","\n","    with open(report_path, \"w\") as f:\n","        json.dump(metrics, f, indent=4)\n","\n","    print(f\"âœ… Report saved: {report_path}\")\n","\n","# ==========================\n","# EXECUTION PIPELINE\n","# ==========================\n","\n","# Initialize GAN components\n","gan_components = initialize_gan_components(config)\n","\n","# Train the GAN\n","gan_train_functions = {\n","    \"GAN\": train_gan,\n","    \"CGAN\": train_conditional_gan,\n","    \"WGAN-GP\": train_wgan_gp,\n","    \"InfoGAN\": train_infogan,\n","}\n","\n","gan_train_functions[config[\"gan_type\"]](gan_components[\"generator\"], gan_components.get(\"discriminator\", gan_components.get(\"critic\")), config[\"data_loader\"], config[\"epochs\"], config[\"device\"])\n","\n","# Evaluate the GAN\n","evaluate_gan(gan_components, config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"uaCeHxFLgTOh","executionInfo":{"status":"error","timestamp":1738347299984,"user_tz":-210,"elapsed":160563,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"69ec51f4-e546-4fea-f215-11e0ad02b377"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing GAN components...\n","Initialized GAN components: {'generator': Generator(\n","  (model): Sequential(\n","    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (3): G_block(\n","      (conv2d_trans): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n","      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","    (4): G_block(\n","      (conv2d_trans): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n","      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","    (5): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n","    (6): Tanh()\n","  )\n","), 'discriminator': Discriminator(\n","  (model): Sequential(\n","    (0): D_block(\n","      (conv2d): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (1): D_block(\n","      (conv2d): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (2): D_block(\n","      (conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (3): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (4): Sigmoid()\n","  )\n",")}\n","generator architecture:\n","Generator(\n","  (model): Sequential(\n","    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (3): G_block(\n","      (conv2d_trans): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n","      (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","    (4): G_block(\n","      (conv2d_trans): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n","      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","    )\n","    (5): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n","    (6): Tanh()\n","  )\n",")\n","discriminator architecture:\n","Discriminator(\n","  (model): Sequential(\n","    (0): D_block(\n","      (conv2d): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (1): D_block(\n","      (conv2d): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (2): D_block(\n","      (conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (activation): LeakyReLU(negative_slope=0.2, inplace=True)\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (3): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","    (4): Sigmoid()\n","  )\n",")\n","Epoch 1/1 - Loss_D: 1.135545015335083, Loss_G: 1.1011542081832886\n"]},{"output_type":"error","ename":"TypeError","evalue":"expected Tensor as element 0 in argument 0, but got list","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a5b467dc864b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;31m# Evaluate the GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m \u001b[0mevaluate_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-a5b467dc864b>\u001b[0m in \u001b[0;36mevaluate_gan\u001b[0;34m(gan_components, config)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# Extract real and generated samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mreal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_loader\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mgen_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_dataloader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","# Generator Block\n","class G_block(nn.Module):\n","    \"\"\"\n","    A generator block consisting of a transposed convolution, batch normalization, and LeakyReLU activation.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=5, strides=2, padding=2):\n","        super(G_block, self).__init__()\n","        self.conv2d_trans = nn.ConvTranspose2d(\n","            in_channels=in_channels, out_channels=out_channels,\n","            kernel_size=kernel_size, stride=strides, padding=padding, bias=False\n","        )\n","        self.batch_norm = nn.BatchNorm2d(out_channels)\n","        self.activation = nn.LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        return self.activation(self.batch_norm(self.conv2d_trans(x)))\n","\n","\n","# Simple Generator\n","class SimpleGenerator(nn.Module):\n","    \"\"\"\n","    A simple generator that maps a latent vector to an image.\n","    \"\"\"\n","    def __init__(self, input_dim):\n","        super(SimpleGenerator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 7 * 7 * 256),\n","            nn.BatchNorm1d(7 * 7 * 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Unflatten(1, (256, 7, 7)),\n","            G_block(256, 128, 5, 1, 2),\n","            G_block(128, 64, 4, 2, 1),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","# Embedding Generator\n","class EmbeddingAsInputGenerator(nn.Module):\n","    \"\"\"\n","    A generator that uses embeddings as part of its input.\n","    \"\"\"\n","    def __init__(self, latent_dim, embedding_dim):\n","        super(EmbeddingAsInputGenerator, self).__init__()\n","        self.embedding_transform = nn.Linear(embedding_dim, 7 * 7 * 256, bias=False)\n","        self.model = nn.Sequential(\n","            nn.BatchNorm1d(7 * 7 * 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Unflatten(1, (256, 7, 7)),\n","            G_block(256, 128, 5, 1, 2),\n","            G_block(128, 64, 5, 2, 2),\n","            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, z, embedding):\n","        \"\"\"\n","        Forward pass for the generator.\n","\n","        Args:\n","            z (torch.Tensor): Latent noise vector of shape (batch_size, latent_dim).\n","            embedding (torch.Tensor): Embedding vector of shape (batch_size, embedding_dim).\n","\n","        Returns:\n","            torch.Tensor: Generated image of shape (batch_size, 1, 28, 28).\n","        \"\"\"\n","        # Ensure embedding has the correct shape\n","        if embedding.dim() != 2:\n","            raise ValueError(f\"Expected embedding to have shape [batch_size, embedding_dim], got {embedding.shape}\")\n","\n","        # Transform embedding\n","        transformed_embedding = self.embedding_transform(embedding)  # Shape: [batch_size, 7 * 7 * 256]\n","\n","        # Reshape to [batch_size, 256, 7, 7]\n","        transformed_embedding = transformed_embedding.view(-1, 256, 7, 7)\n","\n","        # Reshape z to match the spatial dimensions of transformed_embedding\n","        z = z.view(z.size(0), z.size(1), 1, 1)  # Shape: [batch_size, latent_dim, 1, 1]\n","        z = z.expand(-1, -1, 7, 7)  # Shape: [batch_size, latent_dim, 7, 7]\n","\n","        # Combine with latent noise\n","        combined_input = z + transformed_embedding\n","\n","        # Generate image\n","        return self.model(combined_input)\n","\n","# Discriminator Block\n","class D_block(nn.Module):\n","    \"\"\"\n","    A discriminator block consisting of a convolution, LeakyReLU activation, and dropout.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=4, strides=2, padding=1, alpha=0.2, dropout=0.3):\n","        super(D_block, self).__init__()\n","        self.conv2d = nn.Conv2d(\n","            in_channels=in_channels, out_channels=out_channels,\n","            kernel_size=kernel_size, stride=strides, padding=padding, bias=False\n","        )\n","        self.activation = nn.LeakyReLU(alpha, inplace=True)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.dropout(self.activation(self.conv2d(x)))\n","\n","\n","# Discriminator\n","class Discriminator(nn.Module):\n","    \"\"\"\n","    A discriminator that classifies images as real or fake.\n","    \"\"\"\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            D_block(1, 64, 5, 2, 2),\n","            D_block(64, 128, 5, 2, 2),\n","            nn.Flatten(),\n","            nn.Linear(128 * 7 * 7, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","# Training Function for Normal GAN\n","def train_normal_gan(generator, discriminator, embedding_loader, data_loader, latent_dim, epochs, device):\n","    \"\"\"\n","    Trains the Normal GAN.\n","\n","    Args:\n","        generator (nn.Module): Generator model.\n","        discriminator (nn.Module): Discriminator model.\n","        embedding_loader (DataLoader): Embedding DataLoader.\n","        data_loader (DataLoader): Real data DataLoader.\n","        latent_dim (int): Dimension of latent noise.\n","        epochs (int): Number of training epochs.\n","        device (torch.device): Device for training.\n","    \"\"\"\n","    criterion = nn.BCELoss()\n","    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(epochs):\n","        for embedd_batch, (real_images, _) in zip(embedding_loader, data_loader):\n","            real_images = real_images.to(device)\n","            embedd_batch = embedd_batch.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # Labels\n","            real_labels = torch.ones(batch_size, 1, device=device)\n","            fake_labels = torch.zeros(batch_size, 1, device=device)\n","\n","            # Train Discriminator\n","            optimizer_d.zero_grad()\n","            real_output = discriminator(real_images)\n","            loss_real = criterion(real_output, real_labels)\n","\n","            # Generate fake images\n","            noise = torch.randn(batch_size, latent_dim, device=device)\n","            fake_images = generator(noise, embedd_batch)\n","            fake_output = discriminator(fake_images.detach())\n","            loss_fake = criterion(fake_output, fake_labels)\n","\n","            loss_d = loss_real + loss_fake\n","            loss_d.backward()\n","            optimizer_d.step()\n","\n","            # Train Generator\n","            optimizer_g.zero_grad()\n","            fake_output = discriminator(fake_images)\n","            loss_g = criterion(fake_output, real_labels)\n","            loss_g.backward()\n","            optimizer_g.step()\n","\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}\")"],"metadata":{"id":"uSfHDCFfgLsp","executionInfo":{"status":"ok","timestamp":1738330767003,"user_tz":-210,"elapsed":348,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Set up device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize models\n","latent_dim = 100\n","embedding_dim = 50\n","generator = EmbeddingAsInputGenerator(latent_dim, embedding_dim).to(device)\n","discriminator = Discriminator().to(device)\n","\n","# Load data and embeddings\n","data_loader = load_mnist_data(fraction=1.0, batch_size=64, shuffle=True)\n","embedding_loader = load_embeddings(\"./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\", device, batch_size=64)\n","\n","# Train the GAN\n","train_normal_gan(generator, discriminator, embedding_loader, data_loader, latent_dim, epochs=50, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":721},"id":"-BMs2XUSlo9d","executionInfo":{"status":"error","timestamp":1738330794351,"user_tz":-210,"elapsed":2360,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"934792cc-1cbe-4b18-9811-38804ac97381"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (70000, 1, 28, 28) (70000,)\n"]},{"output_type":"stream","name":"stderr","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (100) must match the size of tensor b (256) at non-singleton dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-48b7498ec643>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_normal_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-03c45c8e538b>\u001b[0m in \u001b[0;36mtrain_normal_gan\u001b[0;34m(generator, discriminator, embedding_loader, data_loader, latent_dim, epochs, device)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# Generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedd_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mloss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-03c45c8e538b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, embedding)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Combine with latent noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mcombined_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtransformed_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Generate image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (256) at non-singleton dimension 1"]}]},{"cell_type":"code","source":["# Set up device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize models\n","latent_dim = 100\n","embedding_dim = 50\n","generator = EmbeddingAsInputGenerator(latent_dim, embedding_dim).to(device)\n","discriminator = Discriminator().to(device)\n","\n","# Load data and embeddings\n","data_loader = load_mnist_data(fraction=1.0, batch_size=64, shuffle=True)\n","embedding_loader = load_embeddings(\"./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\", device, batch_size=64)\n","\n","# Train the GAN\n","train_normal_gan(generator, discriminator, embedding_loader, data_loader, latent_dim, epochs=50, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"P3Ef9pAVgSTt","executionInfo":{"status":"error","timestamp":1738329731912,"user_tz":-210,"elapsed":2362,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"63219749-b559-426c-97c6-7c230f438e20"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (70000, 1, 28, 28) (70000,)\n"]},{"output_type":"stream","name":"stderr","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\n"]},{"output_type":"error","ename":"TypeError","evalue":"EmbeddingAsInputGenerator.forward() missing 1 required positional argument: 'embedding'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-48b7498ec643>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_normal_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/gan_workflows/plan1/plan1_gan_training.py\u001b[0m in \u001b[0;36mtrain_normal_gan\u001b[0;34m(generator, discriminator, embedding_loader, data_loader, latent_dim, epochs, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mloss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedd_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: EmbeddingAsInputGenerator.forward() missing 1 required positional argument: 'embedding'"]}]},{"cell_type":"code","source":["# Import models and training functions\n","from src.gan_workflows.plan1.plan1_gan_models import (\n","    SimpleGenerator,\n","    EmbeddingAsInputGenerator,\n","    Discriminator,\n","    AC_Generator,\n","    AC_Discriminator,\n","    Info_Generator,\n","    Info_Discriminator,\n",")\n","from src.gan_workflows.plan1.plan1_gan_training import train_normal_gan, train_acgan, train_infogan"],"metadata":{"id":"V4iFzWgvhpjX","executionInfo":{"status":"ok","timestamp":1738329727138,"user_tz":-210,"elapsed":358,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Configuration Cell for GAN Training with Embeddings\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","\n","# Import models and training functions\n","from src.gan_workflows.plan1.plan1_gan_models import (\n","    SimpleGenerator,\n","    EmbeddingAsInputGenerator,\n","    Discriminator,\n","    AC_Generator,\n","    AC_Discriminator,\n","    Info_Generator,\n","    Info_Discriminator,\n",")\n","from src.gan_workflows.plan1.plan1_gan_training import train_normal_gan, train_acgan, train_infogan\n","from src.data_utils import load_mnist_data, load_embeddings, create_dataloader\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Configuration Options\n","GAN_TYPE = \"NormalGAN\"  # Options: \"NormalGAN\", \"ACGAN\", \"InfoGAN\"\n","LATENT_DIM = 100        # Dimension of latent noise\n","EMBEDDING_DIM = 128     # Dimension of embeddings (for EmbeddingAsInputGenerator)\n","NUM_CLASSES = 10        # Number of classes (for ACGAN and InfoGAN)\n","CONTINUOUS_DIM = 2      # Dimension of continuous latent code (for InfoGAN)\n","DISCRETE_DIM = 10       # Dimension of discrete latent code (for InfoGAN)\n","EPOCHS = 50             # Number of training epochs\n","BATCH_SIZE = 64         # Batch size for training\n","EMBEDDING_FILE = \"./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\"  # Path to your embeddings file\n","\n","# Load MNIST Data\n","data_loader = load_mnist_data(fraction=1.0, batch_size=BATCH_SIZE, shuffle=True)\n","\n","# Load Embeddings\n","embeddings, labels, embedding_loader = load_embeddings(EMBEDDING_FILE, device, batch_size=BATCH_SIZE)\n","\n","# Initialize Models\n","if GAN_TYPE == \"NormalGAN\":\n","    generator = EmbeddingAsInputGenerator(LATENT_DIM, EMBEDDING_DIM).to(device)\n","    discriminator = Discriminator().to(device)\n","elif GAN_TYPE == \"ACGAN\":\n","    generator = AC_Generator(LATENT_DIM, NUM_CLASSES).to(device)\n","    discriminator = AC_Discriminator(NUM_CLASSES).to(device)\n","elif GAN_TYPE == \"InfoGAN\":\n","    generator = Info_Generator(EMBEDDING_DIM, CONTINUOUS_DIM, DISCRETE_DIM).to(device)\n","    discriminator = Info_Discriminator(CONTINUOUS_DIM, DISCRETE_DIM).to(device)\n","else:\n","    raise ValueError(f\"Unknown GAN type: {GAN_TYPE}\")\n","\n","# Train the Model\n","if GAN_TYPE == \"NormalGAN\":\n","    train_normal_gan(generator, discriminator, embedding_loader, data_loader, LATENT_DIM, EPOCHS, device)\n","elif GAN_TYPE == \"ACGAN\":\n","    train_acgan(generator, discriminator, embedding_loader, data_loader, LATENT_DIM, NUM_CLASSES, EPOCHS, device)\n","elif GAN_TYPE == \"InfoGAN\":\n","    train_infogan(generator, discriminator, embedding_loader, data_loader, LATENT_DIM, CONTINUOUS_DIM, DISCRETE_DIM, EPOCHS, device)\n","\n","# Generate and Visualize Sample Images\n","with torch.no_grad():\n","    if GAN_TYPE == \"NormalGAN\":\n","        noise = torch.randn(16, LATENT_DIM).to(device)\n","        fake_images = generator(noise, embeddings[:16]).cpu()  # Use embeddings as input\n","    elif GAN_TYPE == \"ACGAN\":\n","        noise = torch.randn(16, LATENT_DIM).to(device)\n","        labels = torch.randint(0, NUM_CLASSES, (16,)).to(device)\n","        fake_images = generator(noise, labels).cpu()\n","    elif GAN_TYPE == \"InfoGAN\":\n","        noise = torch.randn(16, EMBEDDING_DIM).to(device)\n","        continuous_code = torch.randn(16, CONTINUOUS_DIM).to(device)\n","        discrete_code = torch.randint(0, DISCRETE_DIM, (16,)).to(device)\n","        discrete_code_one_hot = F.one_hot(discrete_code, num_classes=DISCRETE_DIM).float().to(device)\n","        fake_images = generator(noise, continuous_code, discrete_code_one_hot).cpu()\n","\n","    # Denormalize images if necessary\n","    fake_images = fake_images * 0.5 + 0.5\n","\n","    # Plot generated images\n","    plt.figure(figsize=(8, 8))\n","    for i in range(16):\n","        plt.subplot(4, 4, i + 1)\n","        plt.imshow(fake_images[i].squeeze(), cmap=\"gray\")\n","        plt.axis(\"off\")\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"Rv3N0HCCbxv7","executionInfo":{"status":"error","timestamp":1738328268517,"user_tz":-210,"elapsed":3633,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"18733680-8fd7-4e66-e204-7f236c40daf0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Dataset: (70000, 1, 28, 28) (70000,)\n"]},{"output_type":"stream","name":"stderr","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\n"]},{"output_type":"error","ename":"TypeError","evalue":"EmbeddingAsInputGenerator.forward() missing 1 required positional argument: 'embedding'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-98d066138cd6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mGAN_TYPE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NormalGAN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtrain_normal_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLATENT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mGAN_TYPE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ACGAN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtrain_acgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLATENT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/gan_workflows/plan1/plan1_gan_training.py\u001b[0m in \u001b[0;36mtrain_normal_gan\u001b[0;34m(generator, discriminator, embedding_loader, data_loader, latent_dim, epochs, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mloss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedd_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: EmbeddingAsInputGenerator.forward() missing 1 required positional argument: 'embedding'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGck4GUG2rDh"},"outputs":[],"source":["# Plan 1 GAN Training Notebook\n","\n","# Import necessary libraries\n","import torch\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","\n","# Import GAN models and training functions\n","from plan1_gan_models import (\n","    SimpleGenerator,\n","    EmbeddingAsInputGenerator,\n","    Discriminator,\n","    AC_Generator,\n","    AC_Discriminator,\n","    Info_Generator,\n","    Info_Discriminator,\n",")\n","from plan1_gan_training import (\n","    train_normal_gan,\n","    train_acgan,\n","    train_infogan,\n",")\n","\n","# Load embeddings (replace with actual embedding loading code)\n","# Example: Loading a precomputed embedding\n","embeddings = torch.randn(1000, 50)  # Replace with your actual embeddings\n","\n","# Prepare the embedding DataLoader\n","from plan1_gan_training import prepare_embedding_loader\n","embedding_loader = prepare_embedding_loader(embeddings, add_noise=True, noise_dim=50, batch_size=64)\n","\n","# Load real dataset (replace with actual data loading)\n","# Example: Using a DataLoader for the MNIST dataset\n","from torchvision import datasets, transforms\n","\n","mnist_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n","])\n","mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n","real_data_loader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Train Normal GAN\n","print(\"\\n--- Training Normal GAN ---\\n\")\n","generator = SimpleGenerator(input_dim=100).to(device)\n","discriminator = Discriminator().to(device)\n","train_normal_gan(\n","    generator,\n","    discriminator,\n","    embedding_loader,\n","    real_data_loader,\n","    latent_dim=100,\n","    epochs=20,\n","    device=device\n",")\n","\n","# Train ACGAN\n","print(\"\\n--- Training ACGAN ---\\n\")\n","generator = AC_Generator(latent_dim=100, num_classes=10).to(device)\n","discriminator = AC_Discriminator(num_classes=10).to(device)\n","train_acgan(\n","    generator,\n","    discriminator,\n","    embedding_loader,\n","    real_data_loader,\n","    latent_dim=100,\n","    num_classes=10,\n","    epochs=20,\n","    device=device\n",")\n","\n","# Train InfoGAN\n","print(\"\\n--- Training InfoGAN ---\\n\")\n","generator = Info_Generator(embedding_dim=50, continuous_dim=2, discrete_dim=10).to(device)\n","discriminator = Info_Discriminator(continuous_dim=2, discrete_dim=10).to(device)\n","train_infogan(\n","    generator,\n","    discriminator,\n","    embedding_loader,\n","    real_data_loader,\n","    latent_dim=100,\n","    continuous_dim=2,\n","    discrete_dim=10,\n","    epochs=20,\n","    device=device\n",")\n"]}]}