# -*- coding: utf-8 -*-
"""plan1_gan_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3fPtoBE-gAW85i8DQrHmH3DXfu28MlZ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from plan1_gan_models import (
    SimpleGenerator,
    EmbeddingAsInputGenerator,
    Discriminator,
    AC_Generator,
    AC_Discriminator,
    Info_Generator,
    Info_Discriminator,
)

# Training Normal GAN

def train_normal_gan(generator, discriminator, embedding_loader, data_loader, latent_dim, epochs, device):
    """
    Trains the Normal GAN.

    Args:
        generator (nn.Module): Generator model.
        discriminator (nn.Module): Discriminator model.
        embedding_loader (DataLoader): Embedding DataLoader.
        data_loader (DataLoader): Real data DataLoader.
        latent_dim (int): Dimension of latent noise.
        epochs (int): Number of training epochs.
        device (torch.device): Device for training.
    """
    criterion = nn.BCELoss()
    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(epochs):
        for embedd_batch, (real_images, _) in zip(embedding_loader, data_loader):
            real_images = real_images.to(device)
            embedd_batch = embedd_batch.to(device)
            batch_size = real_images.size(0)

            # Labels
            real_labels = torch.ones(batch_size, 1, device=device)
            fake_labels = torch.zeros(batch_size, 1, device=device)

            # Train Discriminator
            optimizer_d.zero_grad()
            real_output = discriminator(real_images)
            loss_real = criterion(real_output, real_labels)

            fake_images = generator(embedd_batch)
            fake_output = discriminator(fake_images.detach())
            loss_fake = criterion(fake_output, fake_labels)

            loss_d = loss_real + loss_fake
            loss_d.backward()
            optimizer_d.step()

            # Train Generator
            optimizer_g.zero_grad()
            fake_output = discriminator(fake_images)
            loss_g = criterion(fake_output, real_labels)
            loss_g.backward()
            optimizer_g.step()

        print(f"Epoch [{epoch + 1}/{epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}")

# Training ACGAN

def train_acgan(generator, discriminator, embedding_loader, data_loader, latent_dim, num_classes, epochs, device):
    """
    Trains the ACGAN.

    Args:
        generator (nn.Module): ACGAN Generator model.
        discriminator (nn.Module): ACGAN Discriminator model.
        embedding_loader (DataLoader): Embedding DataLoader.
        data_loader (DataLoader): Real data DataLoader.
        latent_dim (int): Dimension of latent noise.
        num_classes (int): Number of classes.
        epochs (int): Number of training epochs.
        device (torch.device): Device for training.
    """
    adversarial_loss = nn.BCELoss()
    auxiliary_loss = nn.CrossEntropyLoss()

    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(epochs):
        for embedd_batch, (real_images, real_labels) in zip(embedding_loader, data_loader):
            real_images = real_images.to(device)
            real_labels = real_labels.to(device)
            embedd_batch = embedd_batch.to(device)
            batch_size = real_images.size(0)

            # Adversarial labels
            valid = torch.ones(batch_size, 1, device=device)
            fake = torch.zeros(batch_size, 1, device=device)

            # Train Discriminator
            optimizer_d.zero_grad()

            real_validity, real_aux = discriminator(real_images)
            d_real_loss = adversarial_loss(real_validity, valid)
            aux_real_loss = auxiliary_loss(real_aux, real_labels)

            noise = torch.randn(batch_size, latent_dim, device=device)
            gen_labels = torch.randint(0, num_classes, (batch_size,), device=device)
            fake_images = generator(noise, gen_labels)
            fake_validity, fake_aux = discriminator(fake_images.detach())
            d_fake_loss = adversarial_loss(fake_validity, fake)
            aux_fake_loss = auxiliary_loss(fake_aux, gen_labels)

            d_loss = (d_real_loss + d_fake_loss) / 2 + (aux_real_loss + aux_fake_loss) / 2
            d_loss.backward()
            optimizer_d.step()

            # Train Generator
            optimizer_g.zero_grad()

            fake_validity, fake_aux = discriminator(fake_images)
            g_loss = adversarial_loss(fake_validity, valid) + auxiliary_loss(fake_aux, gen_labels)
            g_loss.backward()
            optimizer_g.step()

        print(f"Epoch [{epoch + 1}/{epochs}], Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}")

# Training InfoGAN

def train_infogan(generator, discriminator, embedding_loader, data_loader, latent_dim, continuous_dim, discrete_dim, epochs, device):
    """
    Trains the InfoGAN.

    Args:
        generator (nn.Module): InfoGAN Generator model.
        discriminator (nn.Module): InfoGAN Discriminator model.
        embedding_loader (DataLoader): Embedding DataLoader.
        data_loader (DataLoader): Real data DataLoader.
        latent_dim (int): Dimension of latent noise.
        continuous_dim (int): Dimension of continuous latent code.
        discrete_dim (int): Dimension of discrete latent code.
        epochs (int): Number of training epochs.
        device (torch.device): Device for training.
    """
    adversarial_loss = nn.BCELoss()
    continuous_loss = nn.MSELoss()
    discrete_loss = nn.CrossEntropyLoss()

    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(epochs):
        for embedd_batch, (real_images, _) in zip(embedding_loader, data_loader):
            real_images = real_images.to(device)
            embedd_batch = embedd_batch.to(device)
            batch_size = real_images.size(0)

            # Adversarial labels
            valid = torch.ones(batch_size, 1, device=device)
            fake = torch.zeros(batch_size, 1, device=device)

            # Train Discriminator
            optimizer_d.zero_grad()

            real_validity, _, _ = discriminator(real_images)
            d_real_loss = adversarial_loss(real_validity, valid)

            continuous_code = torch.randn(batch_size, continuous_dim, device=device)
            discrete_code = torch.randint(0, discrete_dim, (batch_size,), device=device)
            discrete_code_one_hot = F.one_hot(discrete_code, num_classes=discrete_dim).float()

            fake_images = generator(embedd_batch, continuous_code, discrete_code_one_hot)
            fake_validity, _, _ = discriminator(fake_images.detach())
            d_fake_loss = adversarial_loss(fake_validity, fake)

            d_loss = d_real_loss + d_fake_loss
            d_loss.backward()
            optimizer_d.step()

            # Train Generator
            optimizer_g.zero_grad()
            fake_validity, q_continuous, q_discrete = discriminator(fake_images)
            g_loss = adversarial_loss(fake_validity, valid)
            info_loss = continuous_loss(q_continuous, continuous_code) + discrete_loss(q_discrete, discrete_code)

            total_g_loss = g_loss + info_loss
            total_g_loss.backward()
            optimizer_g.step()

        print(f"Epoch [{epoch + 1}/{epochs}], Loss D: {d_loss.item():.4f}, Loss G: {total_g_loss.item():.4f}")