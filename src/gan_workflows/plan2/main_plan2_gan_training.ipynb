{"cells":[{"cell_type":"markdown","metadata":{"id":"6aVzhiUvYnnh"},"source":["# Plan 2: GAN Training with Embeddings\n","\n","## Objective\n","The goal of this plan is to train GANs using embeddings generated from Plan 1. This involves:\n","1. Setting up a GAN architecture that incorporates embeddings.\n","2. Training the GAN to generate high-quality synthetic embeddings or data.\n","3. Evaluating the GAN's performance using relevant metrics.\n","\n","## Key Steps\n","\n","### 1. Define GAN Architecture\n","- **Generator**: Accepts embeddings and noise as inputs, producing synthetic data.\n","- **Discriminator**: Evaluates the authenticity of generated data, optionally conditioned on embeddings.\n","\n","### 2. Load Pre-Generated Embeddings\n","- Load embeddings created in Plan 1 from their respective directories.\n","- Normalize and preprocess embeddings for GAN training.\n","\n","### 3. Train the GAN\n","- Set up training loops for the generator and discriminator.\n","- Use appropriate loss functions, such as adversarial loss (e.g., Wasserstein loss).\n","- Optionally, include auxiliary tasks (e.g., reconstruction loss) for better embedding alignment.\n","\n","### 4. Save Outputs\n","- Save trained GAN models.\n","- Save generated embeddings or data for downstream evaluation.\n","\n","### 5. Evaluate GAN Performance\n","- Use metrics like FID, IS, and qualitative visualization.\n","- Compare performance with baseline models or methods.\n","\n","---\n","\n","## Starting Point\n","\n","**Files Available**:\n","- `plan2_gan_models.py`: Contains the GAN architecture.\n","- `plan2_gan_training.py`: Implements the training pipeline.\n","- `main_plan2_gan_training.ipynb`: High-level control notebook.\n","- `plan2_experiments.ipynb`: For experimentation and evaluation.\n","\n","## Next Steps\n","1. **Review Architecture**:\n","   - Inspect `plan2_gan_models.py` to understand the generator and discriminator setup.\n","2. **Pipeline Setup**:\n","   - Review and prepare the training pipeline in `plan2_gan_training.py`.\n","3. **Experimentation**:\n","   - Use `plan2_experiments.ipynb` for controlled experiments.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56372,"status":"ok","timestamp":1738309948485,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"i3fgL_75bNh_","outputId":"1d619183-62db-4f78-b838-24b158db03dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/GAN-thesis-project\n","Using device: cpu\n"]}],"source":["# Plan 2 GAN Training Notebook\n","\n","import os\n","import sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import logging\n","import json\n","import datetime\n","from datetime import datetime\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from torchvision.models import inception_v3\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.linalg import sqrtm\n","from scipy.stats import wasserstein_distance, entropy, spearmanr, gaussian_kde\n","\n","\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Repository path (adjust if needed)\n","repo_path = \"/content/drive/MyDrive/GAN-thesis-project\"\n","\n","# Add repository path to sys.path for module imports\n","if repo_path not in sys.path:\n","    sys.path.append(repo_path)\n","\n","# Change working directory to the repository\n","os.chdir(repo_path)\n","\n","# Verify the working directory\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","\n","# Set random seed and device\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4657,"status":"ok","timestamp":1738309966176,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"WMNOlaoUC4c7","outputId":"1229c6ec-8d45-4336-c394-d788b4e9192f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Module: src.data_utils\n","  Functions:\n","    - analyze_embeddings\n","    - analyze_embeddings_v2\n","    - create_dataloader\n","    - create_embedding_loaders\n","    - generate_embeddings\n","    - kurtosis\n","    - load_data\n","    - load_embeddings\n","    - load_mnist_data\n","    - pdist\n","    - preprocess_images\n","    - save_embeddings\n","    - skew\n","    - split_dataset\n","    - train_test_split\n","    - visualize_embeddings\n","  Classes:\n","    - DataLoader\n","    - LocalOutlierFactor\n","    - TensorDataset\n","\n","Module: src.cl_loss_function\n","  Functions:\n","    - augment\n","    - compute_nt_xent_loss_with_augmentation\n","    - compute_triplet_loss_with_augmentation\n","    - contrastive_loss\n","    - hflip\n","    - info_nce_loss\n","    - resize\n","  Classes:\n","    - BYOLLoss\n","    - BarlowTwinsLoss\n","    - ContrastiveHead\n","    - DataLoader\n","    - NTXentLoss\n","    - PCA\n","    - Predictor\n","    - TensorDataset\n","    - TripletLoss\n","    - VicRegLoss\n","\n","Module: src.losses\n","  Functions:\n","    - add_noise\n","    - cyclical_beta_schedule\n","    - linear_beta_schedule\n","    - loss_function_dae_ssim\n","    - vae_loss\n","    - vae_ssim_loss\n","  Classes:\n","\n","Module: src.embeddings.encoder_models\n","  Functions:\n","    - compute_gradient_penalty\n","    - nt_xent_loss\n","  Classes:\n","    - ConditionalGANDiscriminator\n","    - ConditionalGANGenerator\n","    - ContrastiveDualGANDiscriminator\n","    - ContrastiveDualGANGenerator\n","    - ContrastiveGANDiscriminator\n","    - ContrastiveGANGenerator\n","    - CrossDomainDiscriminator\n","    - CrossDomainGenerator\n","    - CycleDiscriminator\n","    - CycleGenerator\n","    - DualGANDiscriminator\n","    - DualGANGenerator\n","    - InfoGANDiscriminator\n","    - InfoGANGenerator\n","    - LinearBlock\n","    - SemiSupervisedGANDiscriminator\n","    - SimpleGANDiscriminator\n","    - SimpleGANGenerator\n","    - VAEGANDiscriminator\n","    - VAEGANEncoder\n","    - VAEGANGenerator\n","    - WGANCritic\n","    - WGANGenerator\n","\n","Module: src.embeddings.encoder_training\n","  Functions:\n","    - compute_gradient_penalty\n","    - train_conditional_gan\n","    - train_contrastive_dual_gan\n","    - train_contrastive_gan\n","    - train_cross_domain_gan\n","    - train_cycle_gan\n","    - train_dual_gan\n","    - train_infogan\n","    - train_semi_supervised_gan\n","    - train_vae_gan\n","    - train_wgan_gp\n","  Classes:\n","\n"]}],"source":["# just for you to see what are the implemented function and classes that have been coded up...\n","\n","import inspect\n","\n","# Import the entire modules\n","import src.data_utils as data_utils\n","import src.cl_loss_function as cl_loss\n","import src.losses as losses\n","import src.gan_workflows.plan2.plan2_gan_models as gan_models\n","import src.gan_workflows.plan2.plan2_gan_training as gan_training\n","\n","# Function to list functions and classes in a module\n","def list_functions_and_classes(module):\n","    members = inspect.getmembers(module)\n","    functions = [name for name, obj in members if inspect.isfunction(obj)]\n","    classes = [name for name, obj in members if inspect.isclass(obj)]\n","    return functions, classes\n","\n","# Function to print functions and classes in a readable format\n","def print_functions_and_classes(module_name, module):\n","    functions, classes = list_functions_and_classes(module)\n","    print(f\"Module: {module_name}\")\n","    print(\"  Functions:\")\n","    for func in functions:\n","        print(f\"    - {func}\")\n","    print(\"  Classes:\")\n","    for cls in classes:\n","        print(f\"    - {cls}\")\n","    print()  # Add a blank line for separation\n","\n","# Print functions and classes for each module\n","print_functions_and_classes(\"src.data_utils\", data_utils)\n","print_functions_and_classes(\"src.cl_loss_function\", cl_loss)\n","print_functions_and_classes(\"src.losses\", losses)\n","print_functions_and_classes(\"src.embeddings.encoder_models\", gan_models)\n","print_functions_and_classes(\"src.embeddings.encoder_training\", gan_training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weigTIkfEl2N"},"outputs":[],"source":["# GAN Models and Training Functions\n","from src.gan_workflows.plan2.plan2_gan_models import (\n","    SimpleGANGenerator, SimpleGANDiscriminator,\n","    ContrastiveGANGenerator, ContrastiveGANDiscriminator,\n","    VAEGANEncoder, VAEGANGenerator, VAEGANDiscriminator,\n","    WGANGenerator, WGANCritic,\n","    CrossDomainGenerator, CrossDomainDiscriminator,\n","    CycleGenerator, CycleDiscriminator,\n","    DualGANGenerator, DualGANDiscriminator,\n","    ContrastiveDualGANGenerator, ContrastiveDualGANDiscriminator,\n","    SemiSupervisedGANDiscriminator,\n","    ConditionalGANGenerator, ConditionalGANDiscriminator,\n","    InfoGANGenerator, InfoGANDiscriminator,\n","    compute_gradient_penalty\n",")\n","\n","from src.gan_workflows.plan2.plan2_gan_training import (\n","    train_wgan_gp, train_vae_gan, train_contrastive_gan,\n","    train_cross_domain_gan, train_cycle_gan,\n","    train_dual_gan, train_contrastive_dual_gan,\n","    train_semi_supervised_gan,\n","    train_conditional_gan,\n","    train_infogan\n",")\n","\n","from src.data_utils import (\n","    load_embeddings, analyze_embeddings\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2484,"status":"ok","timestamp":1738258300666,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"Ww8mcFrOzsG_","outputId":"fabcb67e-00cb-4f2c-83e0-75c0eba83c85"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\n","/content/drive/MyDrive/GAN-thesis-project/src/data_utils.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]}],"source":["embedding_dir = \"./saved_embeddings/embeddings/\"  # Example embedding path\n","embedding_file = os.path.join(embedding_dir, \"autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\")\n","\n","# Load embeddings and labels\n","embeddings, labels, data_loader = load_embeddings(embedding_file, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"584Xs4_8EGLV"},"outputs":[],"source":["# ## OLD VERSION, I used to use this one for validation...\n","\n","# def validate_embeddings(embeddings):\n","#     \"\"\"\n","#     Validate and provide information about the shape and data type of embeddings.\n","#     \"\"\"\n","#     if embeddings is None or len(embeddings) == 0:\n","#         raise ValueError(\"Embeddings are empty or not properly generated.\")\n","#     print(f\"Embeddings are of shape: {embeddings.shape}\")\n","#     print(f\"Data type: {embeddings.dtype}\")\n","#     print(f\"Device: {embeddings.device}\")\n","#     if torch.isnan(embeddings).any():\n","#         raise ValueError(\"Embeddings contain NaN values.\")\n","#     if torch.isinf(embeddings).any():\n","#         raise ValueError(\"Embeddings contain infinite values.\")\n","#     if embeddings.ndim != 2:\n","#         raise ValueError(\"Embeddings should be a 2D tensor.\")\n","#     print(\"Embeddings validation passed.\")\n","\n","# validate_embeddings(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1738258300667,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"3IXt_LRiEfWo","outputId":"43ee12e8-f931-4068-adbf-1ea8cc740dd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["embedding batches torch.Size([64, 50])\n"]}],"source":["# Get a batch of training data to see how my batches are\n","embedding_batch = next(iter(data_loader))\n","print('embedding batches', embedding_batch.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4gSo_7Z6vMk"},"outputs":[],"source":["# GAN Configuration Dictionary\n","gan_configurations = {\n","    \"WGAN-GP\": {\n","        \"generator\": WGANGenerator,\n","        \"critic\": WGANCritic,\n","        \"train_function\": train_wgan_gp,\n","        \"train_kwargs\": {\"lambda_gp\": 10}\n","    },\n","    \"VAE-GAN\": {\n","        \"encoder\": VAEGANEncoder,\n","        \"generator\": VAEGANGenerator,\n","        \"discriminator\": VAEGANDiscriminator,\n","        \"train_function\": train_vae_gan\n","    },\n","    \"Contrastive-GAN\": {\n","        \"generator\": ContrastiveGANGenerator,\n","        \"discriminator\": ContrastiveGANDiscriminator,\n","        \"train_function\": train_contrastive_gan\n","    },\n","    \"Cross-Domain-GAN\": {\n","        \"generator\": CrossDomainGenerator,\n","        \"discriminator\": CrossDomainDiscriminator,\n","        \"train_function\": train_cross_domain_gan\n","    },\n","    \"Cycle-GAN\": {\n","        \"generator_a\": CycleGenerator,\n","        \"generator_b\": CycleGenerator,\n","        \"discriminator_a\": CycleDiscriminator,\n","        \"discriminator_b\": CycleDiscriminator,\n","        \"train_function\": train_cycle_gan\n","    },\n","    \"Dual-GAN\": {\n","        \"generator_a\": DualGANGenerator,\n","        \"generator_b\": DualGANGenerator,\n","        \"discriminator_a\": DualGANDiscriminator,\n","        \"discriminator_b\": DualGANDiscriminator,\n","        \"train_function\": train_dual_gan\n","    },\n","    \"Contrastive-Dual-GAN\": {\n","        \"generator_a\": ContrastiveDualGANGenerator,\n","        \"generator_b\": ContrastiveDualGANGenerator,\n","        \"discriminator_a\": ContrastiveDualGANDiscriminator,\n","        \"discriminator_b\": ContrastiveDualGANDiscriminator,\n","        \"train_function\": train_contrastive_dual_gan\n","    },\n","    \"Semi-Supervised-GAN\": {\n","        \"generator\": SimpleGANGenerator,\n","        \"discriminator\": SemiSupervisedGANDiscriminator,\n","        \"train_function\": train_semi_supervised_gan\n","    },\n","    \"Conditional-GAN\": {\n","        \"generator\": ConditionalGANGenerator,\n","        \"discriminator\": ConditionalGANDiscriminator,\n","        \"train_function\": train_conditional_gan\n","    },\n","    \"InfoGAN\": {\n","        \"generator\": InfoGANGenerator,\n","        \"discriminator\": InfoGANDiscriminator,\n","        \"train_function\": train_infogan\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1738258300667,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"nJ6ohBk4r-2k","outputId":"31e2244e-7d48-4a7f-9833-bd48b7d7b086"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['WGAN-GP', 'VAE-GAN', 'Contrastive-GAN', 'Cross-Domain-GAN', 'Cycle-GAN', 'Dual-GAN', 'Contrastive-Dual-GAN', 'Semi-Supervised-GAN', 'Conditional-GAN', 'InfoGAN'])"]},"metadata":{},"execution_count":9}],"source":["gan_configurations.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":641,"status":"ok","timestamp":1738258301301,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"1YIuHE2TvHW4","outputId":"17488b8e-1c9a-4ebc-caee-14abf037577c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸ“‚ Available Embeddings:\n","\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_barlow_twins\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_barlow_twins/AdvancedAutoencoder_barlow_twins_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_contrastive\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_contrastive/AdvancedAutoencoder_contrastive_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_info_nce\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_info_nce/AdvancedAutoencoder_info_nce_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_mse\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_ntxent\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_ntxent/AdvancedAutoencoder_ntxent_embeddings.pt\n","\n","ðŸ”¹ autoencoder_AdvancedAutoencoder_vicreg\n","   ðŸ“„ autoencoder_AdvancedAutoencoder_vicreg/AdvancedAutoencoder_vicreg_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_barlow_twins\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_barlow_twins/EnhancedAutoencoder_barlow_twins_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_contrastive\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_contrastive/EnhancedAutoencoder_contrastive_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_info_nce\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_info_nce/EnhancedAutoencoder_info_nce_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_mse\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_mse/EnhancedAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_ntxent\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_ntxent/EnhancedAutoencoder_ntxent_embeddings.pt\n","\n","ðŸ”¹ autoencoder_EnhancedAutoencoder_vicreg\n","   ðŸ“„ autoencoder_EnhancedAutoencoder_vicreg/EnhancedAutoencoder_vicreg_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_barlow_twins\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_barlow_twins/IntermediateAutoencoder_barlow_twins_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_contrastive\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_contrastive/IntermediateAutoencoder_contrastive_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_info_nce\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_info_nce/IntermediateAutoencoder_info_nce_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_mse\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_ntxent\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_ntxent/IntermediateAutoencoder_ntxent_embeddings.pt\n","\n","ðŸ”¹ autoencoder_IntermediateAutoencoder_vicreg\n","   ðŸ“„ autoencoder_IntermediateAutoencoder_vicreg/IntermediateAutoencoder_vicreg_embeddings.pt\n","\n","ðŸ”¹ autoencoders_AdvancedAutoencoder\n","   ðŸ“„ autoencoders_AdvancedAutoencoder/AdvancedAutoencoder_embeddings.pt\n","\n","ðŸ”¹ autoencoders_AdvancedAutoencoder_mse\n","   ðŸ“„ autoencoders_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoders_BasicAutoencoder\n","   ðŸ“„ autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\n","\n","ðŸ”¹ autoencoders_BasicAutoencoder_mse\n","   ðŸ“„ autoencoders_BasicAutoencoder_mse/BasicAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ autoencoders_IntermediateAutoencoder_mse\n","   ðŸ“„ autoencoders_IntermediateAutoencoder_mse/IntermediateAutoencoder_mse_embeddings.pt\n","\n","ðŸ”¹ kernel_pca_Kernel PCA\n","   ðŸ“„ kernel_pca_Kernel PCA/Kernel PCA_embeddings.pt\n","   ðŸ“„ kernel_pca_Kernel PCA/matrix_factorization_default_loss_kernel_pca_Kernel PCA_embeddings.pt\n","\n","ðŸ”¹ kernel_pca_SIFT\n","   ðŸ“„ kernel_pca_SIFT/SIFT_embeddings.pt\n","   ðŸ“„ kernel_pca_SIFT/matrix_factorization_default_loss_kernel_pca_SIFT_embeddings.pt\n","\n","ðŸ”¹ matrix_factorization_NMF\n","   ðŸ“„ matrix_factorization_NMF/NMF_embeddings.pt\n","   ðŸ“„ matrix_factorization_NMF/matrix_factorization_default_loss_NMF_embeddings.pt\n","\n","ðŸ”¹ matrix_factorization_PCA\n","   ðŸ“„ matrix_factorization_PCA/PCA_embeddings.pt\n","   ðŸ“„ matrix_factorization_PCA/matrix_factorization_default_loss_PCA_embeddings.pt\n","\n","ðŸ”¹ matrix_factorization_SVD\n","   ðŸ“„ matrix_factorization_SVD/SVD_embeddings.pt\n","   ðŸ“„ matrix_factorization_SVD/matrix_factorization_default_loss_SVD_embeddings.pt\n","\n","ðŸ”¹ normalizing_flow_NMF\n","   ðŸ“„ normalizing_flow_NMF/NMF_refined_embeddings.pt\n","   ðŸ“„ normalizing_flow_NMF/matrix_factorization_default_loss_normalizing_flow_NMF_refined_embeddings.pt\n","\n","ðŸ”¹ normalizing_flow_PCA\n","   ðŸ“„ normalizing_flow_PCA/PCA_refined_embeddings.pt\n","   ðŸ“„ normalizing_flow_PCA/matrix_factorization_default_loss_normalizing_flow_PCA_refined_embeddings.pt\n","\n","ðŸ”¹ normalizing_flow_SVD\n","   ðŸ“„ normalizing_flow_SVD/SVD_refined_embeddings.pt\n","   ðŸ“„ normalizing_flow_SVD/matrix_factorization_default_loss_normalizing_flow_SVD_refined_embeddings.pt\n","\n","ðŸ”¹ sift_features\n","   ðŸ“„ sift_features/matrix_factorization_default_loss_sift_embeddings.pt\n","   ðŸ“„ sift_features/sift_embeddings.pt\n","\n","ðŸ”¹ vae_BasicVAE_mse\n","   ðŸ“„ vae_BasicVAE_mse/BasicVAE_mse_embeddings.pt\n","\n","ðŸ”¹ vae_FlexibleVAE_mse\n","   ðŸ“„ vae_FlexibleVAE_mse/FlexibleVAE_mse_embeddings.pt\n","\n","ðŸ”¹ vae_ImprovedFlexibleVAE_mse\n","   ðŸ“„ vae_ImprovedFlexibleVAE_mse/ImprovedFlexibleVAE_mse_embeddings.pt\n","\n","ðŸ”¹ vae_ImprovedVAE_mse\n","   ðŸ“„ vae_ImprovedVAE_mse/ImprovedVAE_mse_embeddings.pt\n"]}],"source":["import os\n","\n","# Base directory where embeddings are stored\n","embedding_base_dir = \"./saved_embeddings/embeddings/\"\n","\n","def list_available_embeddings(base_dir, filter_by=None):\n","    \"\"\"\n","    List available embedding directories and files, optionally filtered by method.\n","\n","    Args:\n","        base_dir (str): The base directory containing embeddings.\n","        filter_by (str or list, optional): Method(s) to filter by (e.g., \"autoencoder\", \"vae\").\n","                                           If None, all embeddings are displayed.\n","    \"\"\"\n","    print(\"\\nðŸ“‚ Available Embeddings:\\n\")\n","\n","    if isinstance(filter_by, str):\n","        filter_by = [filter_by]  # Convert single filter to list\n","\n","    for method in sorted(os.listdir(base_dir)):\n","        method_path = os.path.join(base_dir, method)\n","\n","        # Check if it's a directory\n","        if os.path.isdir(method_path):\n","            if filter_by is None or any(f.lower() in method.lower() for f in filter_by):\n","                pt_files = [f for f in sorted(os.listdir(method_path)) if f.endswith(\".pt\")]\n","                if pt_files:\n","                    print(f\"\\nðŸ”¹ {method}\")  # Show only the category\n","                    for file in pt_files:\n","                        print(f\"   ðŸ“„ {method}/{file}\")  # Show category + filename\n","\n","# Default: Show everything\n","list_available_embeddings(embedding_base_dir)\n","\n","# Example: Show only autoencoder-related embeddings\n","# list_available_embeddings(embedding_base_dir, filter_by=\"vae\")\n","\n","# Example: Show both autoencoder and VAE embeddings\n","# list_available_embeddings(embedding_base_dir, filter_by=[\"autoencoder\", \"vae\"])"]},{"cell_type":"code","source":["# ==========================\n","# CONFIGURATION & EMBEDDING LOADING\n","# ==========================\n","\n","embedding_dir = \"./saved_embeddings/embeddings/\"\n","embedding_relative_path = \"autoencoder_EnhancedAutoencoder_ntxent/EnhancedAutoencoder_ntxent_embeddings.pt\"\n","\n","# Full path to the embedding file\n","embedding_file = os.path.join(embedding_dir, embedding_relative_path)\n","\n","# Extract identifier from the path (remove directory and _embeddings.pt)\n","embedding_identifier = embedding_relative_path.split(\"/\")[-1].replace(\"_embeddings.pt\", \"\")\n","\n","report_dir = \"./reports/GANs_Evaluations\"\n","os.makedirs(report_dir, exist_ok=True)\n","\n","# Configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","config = {\n","    \"gan_type\": 'WGAN-GP',\n","    \"embedding_identifier\": embedding_identifier,  # Dynamically extracted identifier\n","    \"latent_dim\": 100,\n","    \"embedding_dim\": None,  # Will be set after loading embeddings\n","    \"num_classes\": 10,\n","    \"categorical_dim\": 10,\n","    \"epochs\": 1,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-4,\n","    \"device\": device,\n","    \"lambda_gp\": 10,\n","    \"beta1\": 0.5,\n","    \"beta2\": 0.999,\n","    \"save_path\": \"gan_model.pth\",\n","    \"eval_fraction\": 0.1,  # Fraction of embeddings used for evaluation\n","    \"kl_method\": \"histogram\",  # Can be \"histogram\" or \"kde\"\n","    \"show_model_architecture\": True\n","}\n","\n","def load_embeddings(embedding_file, device, batch_size=64):\n","    \"\"\"Loads embeddings and labels from a specified file.\"\"\"\n","    data = torch.load(embedding_file)\n","    embeddings = data[\"embeddings\"].to(device)\n","    labels = data[\"labels\"].to(device)\n","    data_loader = DataLoader(embeddings, batch_size=batch_size, shuffle=True)\n","    return embeddings, labels, data_loader\n","\n","def split_embeddings(embeddings, labels, eval_fraction=0.1, batch_size=64):\n","    \"\"\"Splits embeddings into training and evaluation sets.\"\"\"\n","    num_samples = embeddings.size(0)\n","    num_eval = int(num_samples * eval_fraction)\n","    num_train = num_samples - num_eval\n","\n","    train_embeddings, eval_embeddings = random_split(embeddings, [num_train, num_eval])\n","    train_labels, eval_labels = random_split(labels, [num_train, num_eval])\n","\n","    train_loader = DataLoader(train_embeddings, batch_size=batch_size, shuffle=True)\n","    eval_loader = DataLoader(eval_embeddings, batch_size=batch_size, shuffle=False)\n","    return train_loader, eval_loader, train_embeddings, eval_embeddings\n","\n","# Load embeddings and split\n","embeddings, labels, full_data_loader = load_embeddings(embedding_file, device)\n","config[\"embedding_dim\"] = embeddings.size(1)\n","train_loader, eval_loader, train_embeddings, eval_embeddings = split_embeddings(embeddings, labels, config[\"eval_fraction\"], config[\"batch_size\"])\n","\n","# Update config with data loaders\n","config.update({\n","    \"data_loader\": train_loader,\n","    \"data_loader_a\": train_loader,\n","    \"data_loader_b\": train_loader,\n","    \"eval_loader\": eval_loader,\n","    \"original_embeddings\": embeddings  # Store original embeddings for evaluation\n","})\n","\n","# ==========================\n","# MODEL INITIALIZATION\n","# ==========================\n","\n","def initialize_gan_components(config, gan_configurations):\n","    \"\"\"Initialize GAN components based on type.\"\"\"\n","    components = {}\n","    gan_type = config[\"gan_type\"]\n","    multi_gan_types = [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]\n","\n","    if gan_type in multi_gan_types:\n","        gen_args = {\"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type != \"Cycle-GAN\":\n","            gen_args[\"latent_dim\"] = config[\"latent_dim\"]\n","\n","        components.update({\n","            \"generator_a\": gan_configurations[\"generator_a\"](**gen_args).to(config[\"device\"]),\n","            \"generator_b\": gan_configurations[\"generator_b\"](**gen_args).to(config[\"device\"]),\n","            \"discriminator_a\": gan_configurations[\"discriminator_a\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"]),\n","            \"discriminator_b\": gan_configurations[\"discriminator_b\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","        })\n","    else:\n","        gen_args = {\"latent_dim\": config[\"latent_dim\"], \"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type in [\"Conditional-GAN\", \"InfoGAN\"]:\n","            gen_args[\"num_classes\"] = config[\"num_classes\"]\n","        if gan_type == \"InfoGAN\":\n","            gen_args[\"categorical_dim\"] = config[\"categorical_dim\"]\n","\n","        components[\"generator\"] = gan_configurations[\"generator\"](**gen_args).to(config[\"device\"])\n","\n","    if gan_type == \"WGAN-GP\":\n","        components[\"critic\"] = gan_configurations[\"critic\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","    elif gan_type == \"VAE-GAN\":\n","        components[\"encoder\"] = gan_configurations[\"encoder\"](embedding_dim=config[\"embedding_dim\"], latent_dim=config[\"latent_dim\"]).to(config[\"device\"])\n","        components[\"discriminator\"] = gan_configurations[\"discriminator\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","    elif gan_type not in multi_gan_types:\n","        disc_args = {\"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type in [\"Conditional-GAN\", \"InfoGAN\", \"Semi-Supervised-GAN\"]:\n","            disc_args[\"num_classes\"] = config[\"num_classes\"]\n","        if gan_type == \"InfoGAN\":\n","            disc_args[\"categorical_dim\"] = config[\"categorical_dim\"]\n","\n","        components[\"discriminator\"] = gan_configurations[\"discriminator\"](**disc_args).to(config[\"device\"])\n","\n","    if config[\"show_model_architecture\"]:\n","        print(f\"Initialized components: {components}\")\n","\n","    return components\n","\n","def run_gan_training(config):\n","    \"\"\"Runs GAN training based on selected model.\"\"\"\n","    gan_type = config[\"gan_type\"]\n","    if gan_type not in gan_configurations:\n","        raise ValueError(f\"Unsupported GAN type: {gan_type}\")\n","\n","    gan_config = gan_configurations[gan_type]\n","    components = initialize_gan_components(config, gan_config)\n","    train_function = gan_config[\"train_function\"]\n","\n","    print(f\"ðŸš€ Training {gan_type}...\")\n","\n","    if gan_type == \"VAE-GAN\":\n","        train_function(components[\"encoder\"], components[\"generator\"], components[\"discriminator\"], **config)\n","    elif gan_type in [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]:\n","        train_function(components[\"generator_a\"], components[\"generator_b\"], components[\"discriminator_a\"], components[\"discriminator_b\"], **config)\n","    else:\n","        discriminator = components.get(\"discriminator\", components.get(\"critic\", None))\n","        train_function(components[\"generator\"], discriminator, **config)\n","\n","    print(f\"âœ… {gan_type} training completed!\")\n","\n","# ==========================\n","# EVALUATION METRICS\n","# ==========================\n","\n","def calculate_fid(real_embeddings, generated_embeddings, eps=1e-6):\n","    \"\"\"Compute FrÃ©chet Inception Distance (FID) with numerical stability.\"\"\"\n","    mu1, sigma1 = np.mean(real_embeddings, axis=0), np.cov(real_embeddings, rowvar=False)\n","    mu2, sigma2 = np.mean(generated_embeddings, axis=0), np.cov(generated_embeddings, rowvar=False)\n","\n","    diff = mu1 - mu2\n","    covmean = sqrtm(sigma1 @ sigma2 + np.eye(sigma1.shape[0]) * eps)  # Regularize singularity\n","\n","    if np.iscomplexobj(covmean):\n","        covmean = covmean.real  # Remove imaginary component\n","\n","    return np.sum(diff**2) + np.trace(sigma1 + sigma2 - 2 * covmean)\n","\n","def calculate_kl_divergence(real_embeddings, generated_embeddings, method=\"histogram\"):\n","    \"\"\"\n","    Compute KL divergence between real and generated embeddings.\n","\n","    Args:\n","        real_embeddings (np.ndarray): Real data embeddings.\n","        generated_embeddings (np.ndarray): Generated data embeddings.\n","        method (str): \"histogram\" (default) or \"kde\" for probability estimation.\n","\n","    Returns:\n","        float: KL divergence score.\n","    \"\"\"\n","    if method == \"histogram\":\n","        # Compute probability distributions using histograms\n","        real_prob = np.histogram(real_embeddings, bins=50, density=True)[0]\n","        gen_prob = np.histogram(generated_embeddings, bins=50, density=True)[0]\n","\n","        # Avoid division by zero\n","        real_prob += 1e-10\n","        gen_prob += 1e-10\n","\n","    elif method == \"kde\":\n","        # Use Kernel Density Estimation for smoother probability estimation\n","        real_kde = gaussian_kde(real_embeddings.T)\n","        gen_kde = gaussian_kde(generated_embeddings.T)\n","\n","        # Sample points for estimation\n","        sample_points = np.linspace(\n","            min(real_embeddings.min(), generated_embeddings.min()),\n","            max(real_embeddings.max(), generated_embeddings.max()),\n","            100\n","        )\n","\n","        real_prob = real_kde(sample_points) + 1e-10\n","        gen_prob = gen_kde(sample_points) + 1e-10\n","\n","    else:\n","        raise ValueError(\"Invalid method. Choose 'histogram' or 'kde'.\")\n","\n","    return entropy(real_prob, gen_prob)\n","\n","def calculate_cosine_similarity(real_embeddings, generated_embeddings):\n","    \"\"\"Compute cosine similarity between real and generated embeddings.\"\"\"\n","    return np.mean(cosine_similarity(real_embeddings, generated_embeddings))\n","\n","def rank_similarity(real_embeddings, generated_embeddings):\n","    \"\"\"Compute Spearman Rank Correlation between real and generated embeddings.\"\"\"\n","    min_size = min(real_embeddings.shape[0], generated_embeddings.shape[0])\n","\n","    # Select the first min_size embeddings instead of random sampling\n","    real_embeddings = real_embeddings[:min_size]\n","    generated_embeddings = generated_embeddings[:min_size]\n","\n","    real_rank = np.argsort(real_embeddings, axis=0)\n","    gen_rank = np.argsort(generated_embeddings, axis=0)\n","\n","    return spearmanr(real_rank.flatten(), gen_rank.flatten()).correlation\n","\n","def unique_embedding_ratio(generated_embeddings):\n","    \"\"\"Compute the ratio of unique embeddings in the generated set.\"\"\"\n","    unique_embeddings = np.unique(generated_embeddings, axis=0)\n","    return len(unique_embeddings) / len(generated_embeddings)\n","\n","def aggregate_quality_score(fid, kl, cosine, rank_corr):\n","    \"\"\"Compute a weighted quality score based on multiple metrics.\"\"\"\n","    # Normalize metrics (assuming lower is better for FID & KL)\n","    fid_norm = 1 / (1 + fid)\n","    kl_norm = 1 / (1 + kl)\n","    cosine_norm = cosine  # Higher is better, no need to invert\n","    rank_norm = (rank_corr + 1) / 2  # Convert [-1,1] to [0,1]\n","\n","    # Compute final weighted score\n","    return (0.4 * fid_norm) + (0.2 * kl_norm) + (0.2 * cosine_norm) + (0.2 * rank_norm)\n","\n","def calculate_wasserstein_distance(real_embeddings, generated_embeddings):\n","    \"\"\"Compute Wasserstein Distance between real and generated embeddings with normalization.\"\"\"\n","    real_embeddings = (real_embeddings - np.mean(real_embeddings)) / (np.std(real_embeddings) + 1e-8)\n","    generated_embeddings = (generated_embeddings - np.mean(generated_embeddings)) / (np.std(generated_embeddings) + 1e-8)\n","\n","    return wasserstein_distance(real_embeddings.flatten(), generated_embeddings.flatten())\n","\n","def calculate_coverage_score(real_embeddings, generated_embeddings, n_neighbors=5):\n","    \"\"\"Compute Coverage Score: Percentage of real embeddings with at least one close match in generated embeddings.\"\"\"\n","    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n","    neigh.fit(generated_embeddings)\n","    distances, _ = neigh.kneighbors(real_embeddings)\n","    return np.mean(distances[:, 0] < 0.1)  # Adjust threshold as needed\n","\n","def calculate_memorization_score(real_embeddings, generated_embeddings, n_neighbors=1, tolerance=1e-3):\n","    \"\"\"Compute Memorization Score: Percentage of generated embeddings that are close to real embeddings within a small tolerance.\"\"\"\n","    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n","    neigh.fit(real_embeddings)\n","    distances, _ = neigh.kneighbors(generated_embeddings)\n","\n","    return np.mean(distances[:, 0] < tolerance)  # Allow small tolerance for near-exact matches\n","\n","def generate_synthetic_samples(generator, num_samples=1000, latent_dim=100, device=\"cuda\"):\n","    \"\"\"Generate synthetic samples using a trained GAN generator.\"\"\"\n","    generator.eval()\n","    with torch.no_grad():\n","        latent_vectors = torch.randn(num_samples, latent_dim).to(device)\n","        generated_samples = generator(latent_vectors)\n","    return generated_samples.cpu()\n","\n","def create_dataloader_from_samples(samples, batch_size=64):\n","    return DataLoader(samples, batch_size=batch_size, shuffle=False)\n","\n","def convert_to_float(metrics):\n","    \"\"\"Ensure all values in the dictionary are JSON serializable.\"\"\"\n","    return {k: float(v) for k, v in metrics.items()}\n","\n","def evaluate_gan(gan_components, config):\n","    \"\"\"Evaluate GAN with multiple metrics, ensuring JSON serialization.\"\"\"\n","    device = config[\"device\"]\n","    generator = gan_components[\"generator\"]\n","\n","    generated_samples = generate_synthetic_samples(generator, num_samples=1000, latent_dim=config[\"latent_dim\"], device=device)\n","    generated_dataloader = create_dataloader_from_samples(generated_samples, batch_size=config[\"batch_size\"])\n","\n","    real_embeddings = config[\"original_embeddings\"].cpu().numpy()\n","    eval_embeddings = torch.cat([batch for batch in config[\"eval_loader\"]], dim=0).cpu().numpy()\n","    gen_embeddings = torch.cat([batch for batch in generated_dataloader], dim=0).cpu().numpy()\n","\n","    kl_method = config.get(\"kl_method\", \"histogram\")\n","\n","    metrics = {\n","        \"FID (Original vs Generated)\": calculate_fid(real_embeddings, gen_embeddings),\n","        \"FID (Original vs Eval)\": calculate_fid(real_embeddings, eval_embeddings),\n","        \"KL Divergence\": calculate_kl_divergence(real_embeddings, gen_embeddings),\n","        \"Cosine Similarity\": calculate_cosine_similarity(real_embeddings, gen_embeddings),\n","        \"Spearman Rank Correlation\": rank_similarity(real_embeddings, gen_embeddings),\n","        \"Wasserstein Distance\": calculate_wasserstein_distance(real_embeddings, gen_embeddings),\n","        \"Coverage Score\": calculate_coverage_score(real_embeddings, gen_embeddings),\n","        \"Memorization Score\": calculate_memorization_score(real_embeddings, gen_embeddings),\n","        \"Unique Embedding Ratio\": unique_embedding_ratio(gen_embeddings)\n","    }\n","\n","    # Convert to native Python floats for JSON compatibility\n","    metrics = convert_to_float(metrics)\n","\n","    # Compute aggregate quality score\n","    metrics[\"Aggregate Quality Score\"] = aggregate_quality_score(\n","        metrics[\"FID (Original vs Generated)\"],\n","        metrics[\"KL Divergence\"],\n","        metrics[\"Cosine Similarity\"],\n","        metrics[\"Spearman Rank Correlation\"]\n","    )\n","\n","    # Print results\n","    print(json.dumps(metrics, indent=4))\n","\n","    # Save results\n","    # with open(\"evaluation_results.json\", \"w\") as f:\n","    #     json.dump(metrics, f, indent=4)\n","    save_report(metrics, config[\"gan_type\"], config[\"embedding_identifier\"])\n","\n","def save_report(metrics, gan_type, embedding_identifier):\n","    \"\"\"Saves evaluation metrics as a JSON file with a meaningful name.\"\"\"\n","    report_filename = f\"evaluation_{gan_type}_{embedding_identifier}.json\"\n","    report_path = os.path.join(report_dir, report_filename)\n","\n","    with open(report_path, \"w\") as f:\n","        json.dump(metrics, f, indent=4)\n","\n","    print(f\"âœ… Report saved: {report_path}\")\n","\n","# Initialize GAN components\n","gan_components = initialize_gan_components(config, gan_configurations[config[\"gan_type\"]])\n","\n","# Run training\n","run_gan_training(config)\n","\n","# Evaluate using the original embeddings and evaluation set\n","evaluate_gan(gan_components, config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gv1M_UrPwy28","executionInfo":{"status":"ok","timestamp":1738310036089,"user_tz":-210,"elapsed":21331,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"a0071043-b1a5-46fb-8b61-6b0e878e0a5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a3b46d8deeb0>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["Initialized components: {'generator': WGANGenerator(\n","  (model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=100, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): Linear(in_features=256, out_features=50, bias=True)\n","  )\n","), 'critic': WGANCritic(\n","  (model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=50, out_features=512, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=512, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (2): Linear(in_features=256, out_features=1, bias=True)\n","  )\n",")}\n","Initialized components: {'generator': WGANGenerator(\n","  (model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=100, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): Linear(in_features=256, out_features=50, bias=True)\n","  )\n","), 'critic': WGANCritic(\n","  (model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=50, out_features=512, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=512, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (2): Linear(in_features=256, out_features=1, bias=True)\n","  )\n",")}\n","ðŸš€ Training WGAN-GP...\n","Epoch [1/1], Loss Critic: -88.6221, Loss Generator: -18.5080\n","âœ… WGAN-GP training completed!\n","{\n","    \"FID (Original vs Generated)\": 2363.8292081670443,\n","    \"FID (Original vs Eval)\": 4.337609225576131,\n","    \"KL Divergence\": 0.24260619389375027,\n","    \"Cosine Similarity\": -0.00027506318292580545,\n","    \"Spearman Rank Correlation\": -0.00039506799506799516,\n","    \"Wasserstein Distance\": 0.013566292576216972,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.26102666450362333\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_EnhancedAutoencoder_ntxent.json\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXdd5eaPXoct","executionInfo":{"status":"ok","timestamp":1738266464434,"user_tz":-210,"elapsed":70217,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"0b390438-4d5b-4c01-d5a8-a3e1d7494391"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-0dee6401e7d0>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n","Epoch [1/1], Loss Critic: -80.2475, Loss Generator: -19.5842\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2363.023703740346,\n","    \"FID (Original vs Eval)\": 3.9708659932341845,\n","    \"KL Divergence\": 0.11302049284623569,\n","    \"Cosine Similarity\": 0.002239581197500229,\n","    \"Spearman Rank Correlation\": -0.003032662712662713,\n","    \"Wasserstein Distance\": 0.02205769618025731,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.28000506380510365\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_EnhancedAutoencoder_ntxent.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/1], D Loss: 0.0087, G Loss: 325.4728\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2361.8393635776824,\n","    \"FID (Original vs Eval)\": 3.9708659932341845,\n","    \"KL Divergence\": 0.16535518631751334,\n","    \"Cosine Similarity\": 0.0055039022117853165,\n","    \"Spearman Rank Correlation\": -0.0059590117990118,\n","    \"Wasserstein Distance\": 0.010719093060108386,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.272295662730532\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_EnhancedAutoencoder_ntxent.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/1], D Loss: 0.0004, G Loss: 49.5831\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2445.567629217179,\n","    \"FID (Original vs Eval)\": 3.9708659932341845,\n","    \"KL Divergence\": 0.19003996549954297,\n","    \"Cosine Similarity\": 0.008312694728374481,\n","    \"Spearman Rank Correlation\": 0.011746433506433507,\n","    \"Wasserstein Distance\": 0.02581971163788697,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.27106225928711414\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_EnhancedAutoencoder_ntxent.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/1], D Loss: 0.0239, G Loss: 4.4496\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2446.538746900643,\n","    \"FID (Original vs Eval)\": 3.9708659932341845,\n","    \"KL Divergence\": 0.16704637954005,\n","    \"Cosine Similarity\": -0.009871372021734715,\n","    \"Spearman Rank Correlation\": -0.0013899114699114702,\n","    \"Wasserstein Distance\": 0.013962923559165764,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2694229589722703\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_EnhancedAutoencoder_ntxent.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n"]}],"source":["# ==========================\n","# CONFIGURATION & EMBEDDING LOADING\n","# ==========================\n","\n","embedding_dir = \"./saved_embeddings/embeddings/\"\n","embedding_relative_path = \"autoencoder_EnhancedAutoencoder_ntxent/EnhancedAutoencoder_ntxent_embeddings.pt\"\n","\n","# Full path to the embedding file\n","embedding_file = os.path.join(embedding_dir, embedding_relative_path)\n","\n","# Extract identifier from the path (remove directory and `_embeddings.pt`)\n","embedding_identifier = embedding_relative_path.split(\"/\")[-1].replace(\"_embeddings.pt\", \"\")\n","\n","report_dir = \"./reports/GANs_Evaluations\"\n","os.makedirs(report_dir, exist_ok=True)\n","\n","# Configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","config = {\n","    \"gan_types\": ['WGAN-GP', 'VAE-GAN', 'Contrastive-GAN', 'Cross-Domain-GAN'],\n","    \"embedding_identifier\": embedding_identifier,  # Dynamically extracted identifier\n","    \"latent_dim\": 100,\n","    \"embedding_dim\": None,  # Will be set after loading embeddings\n","    \"num_classes\": 10,\n","    \"categorical_dim\": 10,\n","    \"epochs\": 1,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-4,\n","    \"device\": device,\n","    \"lambda_gp\": 10,\n","    \"beta1\": 0.5,\n","    \"beta2\": 0.999,\n","    \"save_path\": \"gan_model.pth\",\n","    \"eval_fraction\": 0.1,  # Fraction of embeddings used for evaluation\n","    \"kl_method\": \"histogram\",  # Can be \"histogram\" or \"kde\"\n","    \"show_model_architecture\": False\n","}\n","\n","def load_embeddings(embedding_file, device, batch_size=64):\n","    \"\"\"Loads embeddings and labels from a specified file.\"\"\"\n","    data = torch.load(embedding_file)\n","    embeddings = data[\"embeddings\"].to(device)\n","    labels = data[\"labels\"].to(device)\n","    data_loader = DataLoader(embeddings, batch_size=batch_size, shuffle=True)\n","    return embeddings, labels, data_loader\n","\n","def split_embeddings(embeddings, labels, eval_fraction=0.1, batch_size=64):\n","    \"\"\"Splits embeddings into training and evaluation sets.\"\"\"\n","    num_samples = embeddings.size(0)\n","    num_eval = int(num_samples * eval_fraction)\n","    num_train = num_samples - num_eval\n","\n","    train_embeddings, eval_embeddings = random_split(embeddings, [num_train, num_eval])\n","    train_labels, eval_labels = random_split(labels, [num_train, num_eval])\n","\n","    train_loader = DataLoader(train_embeddings, batch_size=batch_size, shuffle=True)\n","    eval_loader = DataLoader(eval_embeddings, batch_size=batch_size, shuffle=False)\n","    return train_loader, eval_loader, train_embeddings, eval_embeddings\n","\n","# Load embeddings and split\n","embeddings, labels, full_data_loader = load_embeddings(embedding_file, device)\n","config[\"embedding_dim\"] = embeddings.size(1)\n","train_loader, eval_loader, train_embeddings, eval_embeddings = split_embeddings(embeddings, labels, config[\"eval_fraction\"], config[\"batch_size\"])\n","\n","# Update config with data loaders\n","config.update({\n","    \"data_loader\": train_loader,\n","    \"data_loader_a\": train_loader,\n","    \"data_loader_b\": train_loader,\n","    \"eval_loader\": eval_loader,\n","    \"original_embeddings\": embeddings  # Store original embeddings for evaluation\n","})\n","\n","# ==========================\n","# MODEL INITIALIZATION\n","# ==========================\n","\n","def initialize_gan_components(config, gan_configurations):\n","    \"\"\"Initialize GAN components based on type.\"\"\"\n","    components = {}\n","    gan_type = config[\"gan_type\"]\n","    multi_gan_types = [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]\n","\n","    if gan_type in multi_gan_types:\n","        gen_args = {\"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type != \"Cycle-GAN\":\n","            gen_args[\"latent_dim\"] = config[\"latent_dim\"]\n","\n","        components.update({\n","            \"generator_a\": gan_configurations[\"generator_a\"](**gen_args).to(config[\"device\"]),\n","            \"generator_b\": gan_configurations[\"generator_b\"](**gen_args).to(config[\"device\"]),\n","            \"discriminator_a\": gan_configurations[\"discriminator_a\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"]),\n","            \"discriminator_b\": gan_configurations[\"discriminator_b\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","        })\n","    else:\n","        gen_args = {\"latent_dim\": config[\"latent_dim\"], \"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type in [\"Conditional-GAN\", \"InfoGAN\"]:\n","            gen_args[\"num_classes\"] = config[\"num_classes\"]\n","        if gan_type == \"InfoGAN\":\n","            gen_args[\"categorical_dim\"] = config[\"categorical_dim\"]\n","\n","        components[\"generator\"] = gan_configurations[\"generator\"](**gen_args).to(config[\"device\"])\n","\n","    if gan_type == \"WGAN-GP\":\n","        components[\"critic\"] = gan_configurations[\"critic\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","    elif gan_type == \"VAE-GAN\":\n","        components[\"encoder\"] = gan_configurations[\"encoder\"](embedding_dim=config[\"embedding_dim\"], latent_dim=config[\"latent_dim\"]).to(config[\"device\"])\n","        components[\"discriminator\"] = gan_configurations[\"discriminator\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","    elif gan_type not in multi_gan_types:\n","        disc_args = {\"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type in [\"Conditional-GAN\", \"InfoGAN\", \"Semi-Supervised-GAN\"]:\n","            disc_args[\"num_classes\"] = config[\"num_classes\"]\n","        if gan_type == \"InfoGAN\":\n","            disc_args[\"categorical_dim\"] = config[\"categorical_dim\"]\n","\n","        components[\"discriminator\"] = gan_configurations[\"discriminator\"](**disc_args).to(config[\"device\"])\n","\n","    if config[\"show_model_architecture\"]:\n","        print(f\"Initialized components: {components}\")\n","\n","    return components\n","\n","\n","def run_gan_training(config):\n","    \"\"\"Runs GAN training based on selected model.\"\"\"\n","    gan_type = config[\"gan_type\"]\n","    if gan_type not in gan_configurations:\n","        raise ValueError(f\"Unsupported GAN type: {gan_type}\")\n","\n","    gan_config = gan_configurations[gan_type]\n","    components = initialize_gan_components(config, gan_config)\n","    train_function = gan_config[\"train_function\"]\n","\n","    print(f\"ðŸš€ Training {gan_type}...\")\n","\n","    if gan_type == \"VAE-GAN\":\n","        train_function(components[\"encoder\"], components[\"generator\"], components[\"discriminator\"], **config)\n","    elif gan_type in [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]:\n","        train_function(components[\"generator_a\"], components[\"generator_b\"], components[\"discriminator_a\"], components[\"discriminator_b\"], **config)\n","    else:\n","        discriminator = components.get(\"discriminator\", components.get(\"critic\", None))\n","        train_function(components[\"generator\"], discriminator, **config)\n","\n","    print(f\"âœ… {gan_type} training completed!\")\n","\n","\n","# ==========================\n","# EVALUATION METRICS\n","# ==========================\n","\n","def calculate_fid(real_embeddings, generated_embeddings, eps=1e-6):\n","    \"\"\"Compute FrÃ©chet Inception Distance (FID) with numerical stability.\"\"\"\n","    mu1, sigma1 = np.mean(real_embeddings, axis=0), np.cov(real_embeddings, rowvar=False)\n","    mu2, sigma2 = np.mean(generated_embeddings, axis=0), np.cov(generated_embeddings, rowvar=False)\n","\n","    diff = mu1 - mu2\n","    covmean = sqrtm(sigma1 @ sigma2 + np.eye(sigma1.shape[0]) * eps)  # Regularize singularity\n","\n","    if np.iscomplexobj(covmean):\n","        covmean = covmean.real  # Remove imaginary component\n","\n","    return np.sum(diff**2) + np.trace(sigma1 + sigma2 - 2 * covmean)\n","\n","def calculate_kl_divergence(real_embeddings, generated_embeddings, method=\"histogram\"):\n","    \"\"\"\n","    Compute KL divergence between real and generated embeddings.\n","\n","    Args:\n","        real_embeddings (np.ndarray): Real data embeddings.\n","        generated_embeddings (np.ndarray): Generated data embeddings.\n","        method (str): \"histogram\" (default) or \"kde\" for probability estimation.\n","\n","    Returns:\n","        float: KL divergence score.\n","    \"\"\"\n","    if method == \"histogram\":\n","        # Compute probability distributions using histograms\n","        real_prob = np.histogram(real_embeddings, bins=50, density=True)[0]\n","        gen_prob = np.histogram(generated_embeddings, bins=50, density=True)[0]\n","\n","        # Avoid division by zero\n","        real_prob += 1e-10\n","        gen_prob += 1e-10\n","\n","    elif method == \"kde\":\n","        # Use Kernel Density Estimation for smoother probability estimation\n","        real_kde = gaussian_kde(real_embeddings.T)\n","        gen_kde = gaussian_kde(generated_embeddings.T)\n","\n","        # Sample points for estimation\n","        sample_points = np.linspace(\n","            min(real_embeddings.min(), generated_embeddings.min()),\n","            max(real_embeddings.max(), generated_embeddings.max()),\n","            100\n","        )\n","\n","        real_prob = real_kde(sample_points) + 1e-10\n","        gen_prob = gen_kde(sample_points) + 1e-10\n","\n","    else:\n","        raise ValueError(\"Invalid method. Choose 'histogram' or 'kde'.\")\n","\n","    return entropy(real_prob, gen_prob)\n","\n","def calculate_cosine_similarity(real_embeddings, generated_embeddings):\n","    \"\"\"Compute cosine similarity between real and generated embeddings.\"\"\"\n","    return np.mean(cosine_similarity(real_embeddings, generated_embeddings))\n","\n","def rank_similarity(real_embeddings, generated_embeddings):\n","    \"\"\"Compute Spearman Rank Correlation between real and generated embeddings.\"\"\"\n","    min_size = min(real_embeddings.shape[0], generated_embeddings.shape[0])\n","\n","    # Select the first `min_size` embeddings instead of random sampling\n","    real_embeddings = real_embeddings[:min_size]\n","    generated_embeddings = generated_embeddings[:min_size]\n","\n","    real_rank = np.argsort(real_embeddings, axis=0)\n","    gen_rank = np.argsort(generated_embeddings, axis=0)\n","\n","    return spearmanr(real_rank.flatten(), gen_rank.flatten()).correlation\n","\n","def unique_embedding_ratio(generated_embeddings):\n","    \"\"\"Compute the ratio of unique embeddings in the generated set.\"\"\"\n","    unique_embeddings = np.unique(generated_embeddings, axis=0)\n","    return len(unique_embeddings) / len(generated_embeddings)\n","\n","def aggregate_quality_score(fid, kl, cosine, rank_corr):\n","    \"\"\"Compute a weighted quality score based on multiple metrics.\"\"\"\n","    # Normalize metrics (assuming lower is better for FID & KL)\n","    fid_norm = 1 / (1 + fid)\n","    kl_norm = 1 / (1 + kl)\n","    cosine_norm = cosine  # Higher is better, no need to invert\n","    rank_norm = (rank_corr + 1) / 2  # Convert [-1,1] to [0,1]\n","\n","    # Compute final weighted score\n","    return (0.4 * fid_norm) + (0.2 * kl_norm) + (0.2 * cosine_norm) + (0.2 * rank_norm)\n","\n","def calculate_wasserstein_distance(real_embeddings, generated_embeddings):\n","    \"\"\"Compute Wasserstein Distance between real and generated embeddings with normalization.\"\"\"\n","    real_embeddings = (real_embeddings - np.mean(real_embeddings)) / (np.std(real_embeddings) + 1e-8)\n","    generated_embeddings = (generated_embeddings - np.mean(generated_embeddings)) / (np.std(generated_embeddings) + 1e-8)\n","\n","    return wasserstein_distance(real_embeddings.flatten(), generated_embeddings.flatten())\n","\n","def calculate_coverage_score(real_embeddings, generated_embeddings, n_neighbors=5):\n","    \"\"\"Compute Coverage Score: Percentage of real embeddings with at least one close match in generated embeddings.\"\"\"\n","    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n","    neigh.fit(generated_embeddings)\n","    distances, _ = neigh.kneighbors(real_embeddings)\n","    return np.mean(distances[:, 0] < 0.1)  # Adjust threshold as needed\n","\n","def calculate_memorization_score(real_embeddings, generated_embeddings, n_neighbors=1, tolerance=1e-3):\n","    \"\"\"Compute Memorization Score: Percentage of generated embeddings that are close to real embeddings within a small tolerance.\"\"\"\n","    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n","    neigh.fit(real_embeddings)\n","    distances, _ = neigh.kneighbors(generated_embeddings)\n","\n","    return np.mean(distances[:, 0] < tolerance)  # Allow small tolerance for near-exact matches\n","\n","def generate_synthetic_samples(generator, num_samples=1000, latent_dim=100, device=\"cuda\"):\n","    \"\"\"Generate synthetic samples using a trained GAN generator.\"\"\"\n","    generator.eval()\n","    with torch.no_grad():\n","        latent_vectors = torch.randn(num_samples, latent_dim).to(device)\n","        generated_samples = generator(latent_vectors)\n","    return generated_samples.cpu()\n","\n","def create_dataloader_from_samples(samples, batch_size=64):\n","    return DataLoader(samples, batch_size=batch_size, shuffle=False)\n","\n","def convert_to_float(metrics):\n","    \"\"\"Ensure all values in the dictionary are JSON serializable.\"\"\"\n","    return {k: float(v) for k, v in metrics.items()}\n","\n","def evaluate_gan(gan_components, config):\n","    \"\"\"Evaluate GAN with multiple metrics, ensuring JSON serialization.\"\"\"\n","    device = config[\"device\"]\n","    gan_type = config[\"gan_type\"]\n","    multi_gan_types = [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]\n","\n","    # Check if the GAN type is a multi-generator GAN\n","    if gan_type in multi_gan_types:\n","        # Evaluate each generator separately\n","        metrics = {}\n","        for gen_key in [\"generator_a\", \"generator_b\"]:\n","            if gen_key in gan_components:\n","                print(f\"ðŸ“Š Evaluating {gen_key} for {gan_type}...\")\n","                generator = gan_components[gen_key]\n","                gen_metrics = evaluate_single_generator(generator, config)\n","                metrics[gen_key] = gen_metrics\n","    else:\n","        # Evaluate single-generator GAN\n","        generator = gan_components[\"generator\"]\n","        metrics = evaluate_single_generator(generator, config)\n","\n","    # Print results\n","    print(json.dumps(metrics, indent=4))\n","\n","    # Save results\n","    save_report(metrics, gan_type, config[\"embedding_identifier\"])\n","\n","def evaluate_single_generator(generator, config):\n","    \"\"\"Evaluate a single generator and return metrics.\"\"\"\n","    device = config[\"device\"]\n","    generated_samples = generate_synthetic_samples(generator, num_samples=1000, latent_dim=config[\"latent_dim\"], device=device)\n","    generated_dataloader = create_dataloader_from_samples(generated_samples, batch_size=config[\"batch_size\"])\n","\n","    real_embeddings = config[\"original_embeddings\"].cpu().numpy()\n","    eval_embeddings = torch.cat([batch for batch in config[\"eval_loader\"]], dim=0).cpu().numpy()\n","    gen_embeddings = torch.cat([batch for batch in generated_dataloader], dim=0).cpu().numpy()\n","\n","    kl_method = config.get(\"kl_method\", \"histogram\")\n","\n","    metrics = {\n","        \"FID (Original vs Generated)\": calculate_fid(real_embeddings, gen_embeddings),\n","        \"FID (Original vs Eval)\": calculate_fid(real_embeddings, eval_embeddings),\n","        \"KL Divergence\": calculate_kl_divergence(real_embeddings, gen_embeddings),\n","        \"Cosine Similarity\": calculate_cosine_similarity(real_embeddings, gen_embeddings),\n","        \"Spearman Rank Correlation\": rank_similarity(real_embeddings, gen_embeddings),\n","        \"Wasserstein Distance\": calculate_wasserstein_distance(real_embeddings, gen_embeddings),\n","        \"Coverage Score\": calculate_coverage_score(real_embeddings, gen_embeddings),\n","        \"Memorization Score\": calculate_memorization_score(real_embeddings, gen_embeddings),\n","        \"Unique Embedding Ratio\": unique_embedding_ratio(gen_embeddings)\n","    }\n","\n","    # Convert to native Python floats for JSON compatibility\n","    metrics = convert_to_float(metrics)\n","\n","    # Compute aggregate quality score\n","    metrics[\"Aggregate Quality Score\"] = aggregate_quality_score(\n","        metrics[\"FID (Original vs Generated)\"],\n","        metrics[\"KL Divergence\"],\n","        metrics[\"Cosine Similarity\"],\n","        metrics[\"Spearman Rank Correlation\"]\n","    )\n","\n","    return metrics\n","\n","def save_report(metrics, gan_type, embedding_identifier):\n","    \"\"\"Saves evaluation metrics as a JSON file with a meaningful name.\"\"\"\n","    report_filename = f\"evaluation_{gan_type}_{embedding_identifier}.json\"\n","    report_path = os.path.join(report_dir, report_filename)\n","\n","    with open(report_path, \"w\") as f:\n","        json.dump(metrics, f, indent=4)\n","\n","    print(f\"âœ… Report saved: {report_path}\")\n","\n","# Loop through each GAN type\n","for gan_type in config[\"gan_types\"]:\n","    print(f\"\\nðŸš€ Running pipeline for GAN type: {gan_type}\")\n","\n","    # Update the GAN type in the config\n","    config[\"gan_type\"] = gan_type\n","\n","    # Initialize GAN components\n","    gan_components = initialize_gan_components(config, gan_configurations[config[\"gan_type\"]])\n","\n","    # Run GAN training\n","    print(\"ðŸ”„ Running GAN training...\")\n","    run_gan_training(config)\n","\n","    # Evaluate GAN\n","    print(\"ðŸ“Š Evaluating GAN...\")\n","    evaluate_gan(gan_components, config)\n","\n","    print(f\"âœ… Completed pipeline for GAN type: {gan_type}\\n\")\n"]},{"cell_type":"code","source":["import os\n","\n","def list_available_embeddings(base_dir, filter_by=None):\n","    \"\"\"\n","    List available embedding directories and files, optionally filtered by method.\n","\n","    Args:\n","        base_dir (str): The base directory containing embeddings.\n","        filter_by (str or list, optional): Method(s) to filter by (e.g., \"autoencoder\", \"vae\").\n","                                           If None, all embeddings are returned.\n","    Returns:\n","        list: A list of paths to embedding files that match the filter.\n","    \"\"\"\n","    embedding_paths = []\n","\n","    if isinstance(filter_by, str):\n","        filter_by = [filter_by]  # Convert single filter to list\n","\n","    for method in sorted(os.listdir(base_dir)):\n","        method_path = os.path.join(base_dir, method)\n","\n","        # Check if it's a directory\n","        if os.path.isdir(method_path):\n","            if filter_by is None or any(f.lower() in method.lower() for f in filter_by):\n","                pt_files = [f for f in sorted(os.listdir(method_path)) if f.endswith(\".pt\")]\n","                for file in pt_files:\n","                    embedding_paths.append(os.path.join(method, file))  # Store relative path\n","\n","    return embedding_paths"],"metadata":{"id":"MynpLEsm0oA8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================\n","# CONFIGURATION & EMBEDDING LOADING\n","# ==========================\n","\n","report_dir = \"./reports/GANs_Evaluations\"\n","os.makedirs(report_dir, exist_ok=True)\n","\n","# Configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","config = {\n","    \"gan_types\": ['WGAN-GP', 'VAE-GAN', 'Contrastive-GAN', 'Cross-Domain-GAN'],\n","    \"embedding_identifier\": embedding_identifier,  # Dynamically extracted identifier\n","    \"latent_dim\": 100,\n","    \"embedding_dim\": None,  # Will be set after loading embeddings\n","    \"num_classes\": 10,\n","    \"categorical_dim\": 10,\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-4,\n","    \"device\": device,\n","    \"lambda_gp\": 10,\n","    \"beta1\": 0.5,\n","    \"beta2\": 0.999,\n","    \"save_path\": \"gan_model.pth\",\n","    \"eval_fraction\": 0.1,  # Fraction of embeddings used for evaluation\n","    \"kl_method\": \"histogram\",  # Can be \"histogram\" or \"kde\"\n","    \"show_model_architecture\": False\n","}\n","\n","# Base directory where embeddings are stored\n","embedding_base_dir = \"./saved_embeddings/embeddings/\"\n","\n","# List all autoencoder embeddings\n","autoencoder_embeddings = list_available_embeddings(embedding_base_dir, filter_by=\"autoencoder\")\n","\n","# Loop through each embedding and run the GAN pipeline\n","for embedding_relative_path in autoencoder_embeddings:\n","    print(f\"\\nðŸš€ Processing embedding: {embedding_relative_path}\")\n","\n","    # Full path to the embedding file\n","    embedding_file = os.path.join(embedding_base_dir, embedding_relative_path)\n","\n","    # Extract identifier from the path (remove directory and `_embeddings.pt`)\n","    embedding_identifier = embedding_relative_path.split(\"/\")[-1].replace(\"_embeddings.pt\", \"\")\n","\n","    # Update config with the current embedding\n","    config.update({\n","        \"embedding_identifier\": embedding_identifier,\n","        \"embedding_file\": embedding_file\n","    })\n","\n","    # Load embeddings and split\n","    embeddings, labels, full_data_loader = load_embeddings(embedding_file, device)\n","    config[\"embedding_dim\"] = embeddings.size(1)\n","    train_loader, eval_loader, train_embeddings, eval_embeddings = split_embeddings(embeddings, labels, config[\"eval_fraction\"], config[\"batch_size\"])\n","\n","    # Update config with data loaders\n","    config.update({\n","        \"data_loader\": train_loader,\n","        \"data_loader_a\": train_loader,\n","        \"data_loader_b\": train_loader,\n","        \"eval_loader\": eval_loader,\n","        \"original_embeddings\": embeddings  # Store original embeddings for evaluation\n","    })\n","\n","    # Loop through each GAN type\n","    for gan_type in config[\"gan_types\"]:\n","        print(f\"\\nðŸš€ Running pipeline for GAN type: {gan_type}\")\n","\n","        # Update the GAN type in the config\n","        config[\"gan_type\"] = gan_type\n","\n","        # Initialize GAN components\n","        gan_components = initialize_gan_components(config, gan_configurations[config[\"gan_type\"]])\n","\n","        # Run GAN training\n","        print(\"ðŸ”„ Running GAN training...\")\n","        run_gan_training(config)\n","\n","        # Evaluate GAN\n","        print(\"ðŸ“Š Evaluating GAN...\")\n","        evaluate_gan(gan_components, config)\n","\n","        print(f\"âœ… Completed pipeline for GAN type: {gan_type}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cl5S48b83H5k","executionInfo":{"status":"error","timestamp":1738296671418,"user_tz":-210,"elapsed":14063312,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"b1332c38-e748-4859-d1b1-ce95b0b4d713"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoder_AdvancedAutoencoder_barlow_twins/AdvancedAutoencoder_barlow_twins_embeddings.pt\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","ðŸš€ Processing embedding: autoencoder_AdvancedAutoencoder_barlow_twins/AdvancedAutoencoder_barlow_twins_embeddings.pt\n","\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/GAN-thesis-project/src/data_utils.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [1/100], Loss Critic: -239.6185, Loss Generator: -53.4397\n","Epoch [2/100], Loss Critic: -136.9057, Loss Generator: -55.1639\n","Epoch [3/100], Loss Critic: -125.6706, Loss Generator: -66.4566\n","Epoch [4/100], Loss Critic: -62.6210, Loss Generator: -64.3935\n","Epoch [5/100], Loss Critic: -61.0054, Loss Generator: -10.6017\n","Epoch [6/100], Loss Critic: -32.9666, Loss Generator: 0.6043\n","Epoch [7/100], Loss Critic: -39.3012, Loss Generator: -3.1413\n","Epoch [8/100], Loss Critic: -31.1133, Loss Generator: -5.5190\n","Epoch [9/100], Loss Critic: -34.1949, Loss Generator: -5.2892\n","Epoch [10/100], Loss Critic: -28.9618, Loss Generator: 4.1461\n","Epoch [11/100], Loss Critic: -34.5196, Loss Generator: 5.0132\n","Epoch [12/100], Loss Critic: -55.4187, Loss Generator: 8.3848\n","Epoch [13/100], Loss Critic: -82.0642, Loss Generator: -2.0862\n","Epoch [14/100], Loss Critic: -78.6131, Loss Generator: 0.4594\n","Epoch [15/100], Loss Critic: -76.8143, Loss Generator: 0.6475\n","Epoch [16/100], Loss Critic: -77.6656, Loss Generator: -6.1789\n","Epoch [17/100], Loss Critic: -84.7157, Loss Generator: -8.1457\n","Epoch [18/100], Loss Critic: -85.2509, Loss Generator: 6.9122\n","Epoch [19/100], Loss Critic: -104.7016, Loss Generator: 3.4651\n","Epoch [20/100], Loss Critic: -89.5050, Loss Generator: -3.9987\n","Epoch [21/100], Loss Critic: -121.7987, Loss Generator: -2.2230\n","Epoch [22/100], Loss Critic: -68.7042, Loss Generator: -4.6660\n","Epoch [23/100], Loss Critic: -114.7621, Loss Generator: -4.1630\n","Epoch [24/100], Loss Critic: -82.2613, Loss Generator: 0.4315\n","Epoch [25/100], Loss Critic: -118.4939, Loss Generator: -3.5096\n","Epoch [26/100], Loss Critic: -65.8903, Loss Generator: 1.8634\n","Epoch [27/100], Loss Critic: -62.5666, Loss Generator: 0.4854\n","Epoch [28/100], Loss Critic: -130.0068, Loss Generator: -6.8131\n","Epoch [29/100], Loss Critic: -76.3031, Loss Generator: -8.5761\n","Epoch [30/100], Loss Critic: -69.7283, Loss Generator: 0.4992\n","Epoch [31/100], Loss Critic: -74.8753, Loss Generator: -6.3324\n","Epoch [32/100], Loss Critic: -87.5787, Loss Generator: 2.9642\n","Epoch [33/100], Loss Critic: -79.7087, Loss Generator: -19.7346\n","Epoch [34/100], Loss Critic: -73.7434, Loss Generator: -4.0195\n","Epoch [35/100], Loss Critic: -56.6883, Loss Generator: -12.3016\n","Epoch [36/100], Loss Critic: -57.0159, Loss Generator: -7.3663\n","Epoch [37/100], Loss Critic: -78.7337, Loss Generator: -8.7053\n","Epoch [38/100], Loss Critic: -65.9676, Loss Generator: -6.9553\n","Epoch [39/100], Loss Critic: -70.1526, Loss Generator: -3.3592\n","Epoch [40/100], Loss Critic: -104.1080, Loss Generator: -6.4874\n","Epoch [41/100], Loss Critic: -66.4909, Loss Generator: -8.2914\n","Epoch [42/100], Loss Critic: -88.2497, Loss Generator: -15.4962\n","Epoch [43/100], Loss Critic: -41.1039, Loss Generator: -14.8384\n","Epoch [44/100], Loss Critic: -60.5398, Loss Generator: -21.7345\n","Epoch [45/100], Loss Critic: -51.6357, Loss Generator: -15.4016\n","Epoch [46/100], Loss Critic: -85.9103, Loss Generator: -13.4611\n","Epoch [47/100], Loss Critic: -85.4376, Loss Generator: -15.5908\n","Epoch [48/100], Loss Critic: -65.9017, Loss Generator: -8.3205\n","Epoch [49/100], Loss Critic: -44.2666, Loss Generator: -18.1880\n","Epoch [50/100], Loss Critic: -56.8833, Loss Generator: -16.0498\n","Epoch [51/100], Loss Critic: -69.4889, Loss Generator: -18.6335\n","Epoch [52/100], Loss Critic: -64.7984, Loss Generator: -24.7707\n","Epoch [53/100], Loss Critic: -70.7778, Loss Generator: -20.9200\n","Epoch [54/100], Loss Critic: -60.7317, Loss Generator: -22.9490\n","Epoch [55/100], Loss Critic: -30.6505, Loss Generator: -29.2715\n","Epoch [56/100], Loss Critic: -69.4690, Loss Generator: -28.4396\n","Epoch [57/100], Loss Critic: -72.1421, Loss Generator: -23.8914\n","Epoch [58/100], Loss Critic: -39.8116, Loss Generator: -21.5826\n","Epoch [59/100], Loss Critic: -68.3853, Loss Generator: -21.2336\n","Epoch [60/100], Loss Critic: -69.5807, Loss Generator: -19.7077\n","Epoch [61/100], Loss Critic: -44.6256, Loss Generator: -26.9463\n","Epoch [62/100], Loss Critic: -42.9768, Loss Generator: -23.5941\n","Epoch [63/100], Loss Critic: -57.0366, Loss Generator: -25.7787\n","Epoch [64/100], Loss Critic: -33.3254, Loss Generator: -27.6841\n","Epoch [65/100], Loss Critic: -44.3616, Loss Generator: -27.3486\n","Epoch [66/100], Loss Critic: -58.4535, Loss Generator: -26.5380\n","Epoch [67/100], Loss Critic: -63.7538, Loss Generator: -29.7728\n","Epoch [68/100], Loss Critic: -59.4133, Loss Generator: -27.7190\n","Epoch [69/100], Loss Critic: -53.4637, Loss Generator: -26.7900\n","Epoch [70/100], Loss Critic: -23.2366, Loss Generator: -30.6849\n","Epoch [71/100], Loss Critic: -42.5193, Loss Generator: -39.4381\n","Epoch [72/100], Loss Critic: -11.5017, Loss Generator: -41.1121\n","Epoch [73/100], Loss Critic: -54.3321, Loss Generator: -26.3123\n","Epoch [74/100], Loss Critic: -62.6248, Loss Generator: -18.5698\n","Epoch [75/100], Loss Critic: -65.2389, Loss Generator: -32.7159\n","Epoch [76/100], Loss Critic: -52.1586, Loss Generator: -34.3656\n","Epoch [77/100], Loss Critic: -64.2388, Loss Generator: -34.4912\n","Epoch [78/100], Loss Critic: -40.9512, Loss Generator: -33.1194\n","Epoch [79/100], Loss Critic: -61.8878, Loss Generator: -44.2246\n","Epoch [80/100], Loss Critic: -67.6087, Loss Generator: -31.4303\n","Epoch [81/100], Loss Critic: -48.3349, Loss Generator: -42.3673\n","Epoch [82/100], Loss Critic: -54.0152, Loss Generator: -37.8510\n","Epoch [83/100], Loss Critic: -50.7609, Loss Generator: -38.8310\n","Epoch [84/100], Loss Critic: -44.8592, Loss Generator: -31.9111\n","Epoch [85/100], Loss Critic: -34.9920, Loss Generator: -44.2139\n","Epoch [86/100], Loss Critic: -48.7237, Loss Generator: -45.9322\n","Epoch [87/100], Loss Critic: -31.6967, Loss Generator: -38.6883\n","Epoch [88/100], Loss Critic: -51.5630, Loss Generator: -46.7121\n","Epoch [89/100], Loss Critic: -66.6210, Loss Generator: -40.0338\n","Epoch [90/100], Loss Critic: -70.5507, Loss Generator: -39.4547\n","Epoch [91/100], Loss Critic: -20.4329, Loss Generator: -45.7227\n","Epoch [92/100], Loss Critic: -38.3076, Loss Generator: -45.7598\n","Epoch [93/100], Loss Critic: -68.2117, Loss Generator: -49.3847\n","Epoch [94/100], Loss Critic: -53.3151, Loss Generator: -41.2493\n","Epoch [95/100], Loss Critic: -56.5159, Loss Generator: -38.6585\n","Epoch [96/100], Loss Critic: -46.6730, Loss Generator: -36.9469\n","Epoch [97/100], Loss Critic: -32.2826, Loss Generator: -42.1141\n","Epoch [98/100], Loss Critic: -53.7086, Loss Generator: -41.4497\n","Epoch [99/100], Loss Critic: -41.1298, Loss Generator: -40.5621\n","Epoch [100/100], Loss Critic: -50.3745, Loss Generator: -32.9324\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 9647.695554748043,\n","    \"FID (Original vs Eval)\": 14.572063988153559,\n","    \"KL Divergence\": 1.1159703355312516,\n","    \"Cosine Similarity\": 0.038556553423404694,\n","    \"Spearman Rank Correlation\": 0.0038106261306261316,\n","    \"Wasserstein Distance\": 0.20555972157984304,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.20265311316859944\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_AdvancedAutoencoder_barlow_twins.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/100], D Loss: 0.0009, G Loss: 7438.8291\n","Epoch [2/100], D Loss: 0.0000, G Loss: 45851.8984\n","Epoch [3/100], D Loss: 0.0000, G Loss: 1648.8685\n","Epoch [4/100], D Loss: 0.0000, G Loss: 12271.3232\n","Epoch [5/100], D Loss: 0.0012, G Loss: 1098.4333\n","Epoch [6/100], D Loss: 0.0021, G Loss: 1189.0521\n","Epoch [7/100], D Loss: 0.0000, G Loss: 432.4606\n","Epoch [8/100], D Loss: 0.0000, G Loss: 2656.1887\n","Epoch [9/100], D Loss: 0.0001, G Loss: 941.0291\n","Epoch [10/100], D Loss: 0.0000, G Loss: 766.1501\n","Epoch [11/100], D Loss: 0.0008, G Loss: 375.7143\n","Epoch [12/100], D Loss: 0.0000, G Loss: 968.2567\n","Epoch [13/100], D Loss: 0.0057, G Loss: 286.1203\n","Epoch [14/100], D Loss: 0.0002, G Loss: 1942.1188\n","Epoch [15/100], D Loss: 0.0000, G Loss: 216.4883\n","Epoch [16/100], D Loss: 0.0013, G Loss: 237.3171\n","Epoch [17/100], D Loss: 0.0016, G Loss: 211.4236\n","Epoch [18/100], D Loss: 0.0004, G Loss: 197.2811\n","Epoch [19/100], D Loss: 0.0000, G Loss: 281.2646\n","Epoch [20/100], D Loss: 0.0001, G Loss: 372.9016\n","Epoch [21/100], D Loss: 0.0000, G Loss: 174.8627\n","Epoch [22/100], D Loss: 0.0001, G Loss: 194.0046\n","Epoch [23/100], D Loss: 0.0000, G Loss: 212.2092\n","Epoch [24/100], D Loss: 0.0001, G Loss: 234.5523\n","Epoch [25/100], D Loss: 0.0000, G Loss: 174.2582\n","Epoch [26/100], D Loss: 0.0006, G Loss: 184.0259\n","Epoch [27/100], D Loss: 0.0001, G Loss: 184.6652\n","Epoch [28/100], D Loss: 0.0004, G Loss: 183.5953\n","Epoch [29/100], D Loss: 0.0000, G Loss: 169.8531\n","Epoch [30/100], D Loss: 0.0001, G Loss: 195.4193\n","Epoch [31/100], D Loss: 0.0000, G Loss: 164.3022\n","Epoch [32/100], D Loss: 0.0000, G Loss: 182.4895\n","Epoch [33/100], D Loss: 0.0004, G Loss: 156.3977\n","Epoch [34/100], D Loss: 0.0000, G Loss: 176.0419\n","Epoch [35/100], D Loss: 0.0000, G Loss: 185.3694\n","Epoch [36/100], D Loss: 0.0106, G Loss: 153.9296\n","Epoch [37/100], D Loss: 0.0050, G Loss: 166.9141\n","Epoch [38/100], D Loss: 0.0000, G Loss: 205.9399\n","Epoch [39/100], D Loss: 0.0000, G Loss: 195.0285\n","Epoch [40/100], D Loss: 0.0007, G Loss: 183.3745\n","Epoch [41/100], D Loss: 0.0012, G Loss: 157.6821\n","Epoch [42/100], D Loss: 0.0001, G Loss: 170.8540\n","Epoch [43/100], D Loss: 0.0000, G Loss: 203.2002\n","Epoch [44/100], D Loss: 0.0000, G Loss: 169.1613\n","Epoch [45/100], D Loss: 0.0000, G Loss: 182.7479\n","Epoch [46/100], D Loss: 0.0001, G Loss: 152.4809\n","Epoch [47/100], D Loss: 0.0000, G Loss: 176.1705\n","Epoch [48/100], D Loss: 0.0000, G Loss: 163.5349\n","Epoch [49/100], D Loss: 0.0003, G Loss: 170.5076\n","Epoch [50/100], D Loss: 0.0117, G Loss: 185.6249\n","Epoch [51/100], D Loss: 0.0000, G Loss: 184.5504\n","Epoch [52/100], D Loss: 0.0000, G Loss: 217.4905\n","Epoch [53/100], D Loss: 0.0001, G Loss: 172.1862\n","Epoch [54/100], D Loss: 0.0000, G Loss: 183.2675\n","Epoch [55/100], D Loss: 0.0000, G Loss: 214.7354\n","Epoch [56/100], D Loss: 0.0000, G Loss: 168.3595\n","Epoch [57/100], D Loss: 0.0000, G Loss: 170.6279\n","Epoch [58/100], D Loss: 0.0009, G Loss: 174.4559\n","Epoch [59/100], D Loss: 0.0000, G Loss: 168.7388\n","Epoch [60/100], D Loss: 0.0000, G Loss: 169.0661\n","Epoch [61/100], D Loss: 0.0000, G Loss: 164.1010\n","Epoch [62/100], D Loss: 0.0002, G Loss: 171.1721\n","Epoch [63/100], D Loss: 0.0000, G Loss: 191.6134\n","Epoch [64/100], D Loss: 0.0000, G Loss: 193.6635\n","Epoch [65/100], D Loss: 0.0000, G Loss: 206.8918\n","Epoch [66/100], D Loss: 0.0000, G Loss: 183.7027\n","Epoch [67/100], D Loss: 0.0000, G Loss: 188.9746\n","Epoch [68/100], D Loss: 0.0000, G Loss: 158.9002\n","Epoch [69/100], D Loss: 0.0000, G Loss: 197.2036\n","Epoch [70/100], D Loss: 0.0000, G Loss: 192.1847\n","Epoch [71/100], D Loss: 0.0000, G Loss: 171.0951\n","Epoch [72/100], D Loss: 0.0000, G Loss: 175.5793\n","Epoch [73/100], D Loss: 0.0000, G Loss: 218.8210\n","Epoch [74/100], D Loss: 0.0000, G Loss: 174.3762\n","Epoch [75/100], D Loss: 0.0000, G Loss: 181.1579\n","Epoch [76/100], D Loss: 0.0000, G Loss: 206.3184\n","Epoch [77/100], D Loss: 0.0000, G Loss: 212.8614\n","Epoch [78/100], D Loss: 0.0000, G Loss: 186.7198\n","Epoch [79/100], D Loss: 0.0000, G Loss: 187.3035\n","Epoch [80/100], D Loss: 0.0000, G Loss: 200.8898\n","Epoch [81/100], D Loss: 0.0000, G Loss: 214.0554\n","Epoch [82/100], D Loss: 0.0000, G Loss: 200.8880\n","Epoch [83/100], D Loss: 0.0000, G Loss: 175.8325\n","Epoch [84/100], D Loss: 0.0000, G Loss: 181.5374\n","Epoch [85/100], D Loss: 0.0000, G Loss: 175.6190\n","Epoch [86/100], D Loss: 0.0000, G Loss: 190.2109\n","Epoch [87/100], D Loss: 0.0000, G Loss: 175.1908\n","Epoch [88/100], D Loss: 0.0000, G Loss: 154.0005\n","Epoch [89/100], D Loss: 0.0000, G Loss: 180.1033\n","Epoch [90/100], D Loss: 0.0000, G Loss: 168.6946\n","Epoch [91/100], D Loss: 0.0000, G Loss: 166.5633\n","Epoch [92/100], D Loss: 0.0000, G Loss: 171.6759\n","Epoch [93/100], D Loss: 0.0000, G Loss: 189.3770\n","Epoch [94/100], D Loss: 0.0000, G Loss: 174.7061\n","Epoch [95/100], D Loss: 0.0000, G Loss: 180.4293\n","Epoch [96/100], D Loss: 0.0000, G Loss: 175.3887\n","Epoch [97/100], D Loss: 0.0000, G Loss: 216.7280\n","Epoch [98/100], D Loss: 0.0000, G Loss: 202.9053\n","Epoch [99/100], D Loss: 0.0000, G Loss: 171.4384\n","Epoch [100/100], D Loss: 0.0000, G Loss: 196.6527\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 9662.710294316626,\n","    \"FID (Original vs Eval)\": 14.572063988153559,\n","    \"KL Divergence\": 0.9062385757942543,\n","    \"Cosine Similarity\": -0.0005070212064310908,\n","    \"Spearman Rank Correlation\": 0.003303351783351784,\n","    \"Wasserstein Distance\": 0.2038010062387873,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2051889844735152\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_AdvancedAutoencoder_barlow_twins.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/100], D Loss: 0.0004, G Loss: -2136.6143\n","Epoch [2/100], D Loss: 0.0001, G Loss: -6303.9023\n","Epoch [3/100], D Loss: 0.0000, G Loss: -15171.8496\n","Epoch [4/100], D Loss: 0.0000, G Loss: -20022.5293\n","Epoch [5/100], D Loss: 0.0000, G Loss: -28169.4102\n","Epoch [6/100], D Loss: 0.0000, G Loss: -42189.0625\n","Epoch [7/100], D Loss: 0.0000, G Loss: -58246.0273\n","Epoch [8/100], D Loss: 0.0000, G Loss: -65396.9492\n","Epoch [9/100], D Loss: 0.0000, G Loss: -89702.9766\n","Epoch [10/100], D Loss: 0.0000, G Loss: -99844.6406\n","Epoch [11/100], D Loss: 0.0000, G Loss: -126719.4141\n","Epoch [12/100], D Loss: 0.0000, G Loss: -156336.1406\n","Epoch [13/100], D Loss: 0.0000, G Loss: -194725.7031\n","Epoch [14/100], D Loss: 0.0000, G Loss: -182153.7500\n","Epoch [15/100], D Loss: 0.0000, G Loss: -208794.0469\n","Epoch [16/100], D Loss: 0.0000, G Loss: -267786.5938\n","Epoch [17/100], D Loss: 0.0000, G Loss: -257297.2031\n","Epoch [18/100], D Loss: 0.0000, G Loss: -219242.4688\n","Epoch [19/100], D Loss: 0.0000, G Loss: -352600.3125\n","Epoch [20/100], D Loss: 0.0000, G Loss: -349244.8750\n","Epoch [21/100], D Loss: 0.0000, G Loss: -419691.5000\n","Epoch [22/100], D Loss: 0.0000, G Loss: -436851.0312\n","Epoch [23/100], D Loss: 0.0000, G Loss: -432708.6562\n","Epoch [24/100], D Loss: 0.0000, G Loss: -499479.7812\n","Epoch [25/100], D Loss: 0.0000, G Loss: -416662.3750\n","Epoch [26/100], D Loss: 0.0000, G Loss: -561232.8125\n","Epoch [27/100], D Loss: 0.0000, G Loss: -742323.5000\n","Epoch [28/100], D Loss: 0.0000, G Loss: -632158.5000\n","Epoch [29/100], D Loss: 0.0000, G Loss: -657565.0625\n","Epoch [30/100], D Loss: 0.0000, G Loss: -798295.3125\n","Epoch [31/100], D Loss: 0.0000, G Loss: -771949.9375\n","Epoch [32/100], D Loss: 0.0000, G Loss: -718607.2500\n","Epoch [33/100], D Loss: 0.0000, G Loss: -774363.9375\n","Epoch [34/100], D Loss: 0.0000, G Loss: -989428.8125\n","Epoch [35/100], D Loss: 0.0000, G Loss: -932061.8125\n","Epoch [36/100], D Loss: 0.0000, G Loss: -979592.7500\n","Epoch [37/100], D Loss: 0.0000, G Loss: -1166371.6250\n","Epoch [38/100], D Loss: 0.0000, G Loss: -1181275.3750\n","Epoch [39/100], D Loss: 0.0000, G Loss: -1160075.8750\n","Epoch [40/100], D Loss: 0.0000, G Loss: -1318227.3750\n","Epoch [41/100], D Loss: 0.0000, G Loss: -1370646.3750\n","Epoch [42/100], D Loss: 0.0000, G Loss: -1332459.5000\n","Epoch [43/100], D Loss: 0.0000, G Loss: -1662711.8750\n","Epoch [44/100], D Loss: 0.0000, G Loss: -1417744.6250\n","Epoch [45/100], D Loss: 0.0000, G Loss: -1708433.5000\n","Epoch [46/100], D Loss: 0.0000, G Loss: -1348173.3750\n","Epoch [47/100], D Loss: 0.0000, G Loss: -2076016.5000\n","Epoch [48/100], D Loss: 0.0000, G Loss: -2119494.7500\n","Epoch [49/100], D Loss: 0.0000, G Loss: -1645077.5000\n","Epoch [50/100], D Loss: 0.0000, G Loss: -2085685.0000\n","Epoch [51/100], D Loss: 0.0000, G Loss: -1958815.5000\n","Epoch [52/100], D Loss: 0.0000, G Loss: -2334360.7500\n","Epoch [53/100], D Loss: 0.0000, G Loss: -1850692.8750\n","Epoch [54/100], D Loss: 0.0000, G Loss: -2297144.7500\n","Epoch [55/100], D Loss: 0.0000, G Loss: -2509013.7500\n","Epoch [56/100], D Loss: 0.0000, G Loss: -2286515.2500\n","Epoch [57/100], D Loss: 0.0000, G Loss: -2787344.7500\n","Epoch [58/100], D Loss: 0.0000, G Loss: -2210161.7500\n","Epoch [59/100], D Loss: 0.0000, G Loss: -2940878.0000\n","Epoch [60/100], D Loss: 0.0000, G Loss: -2924877.0000\n","Epoch [61/100], D Loss: 0.0000, G Loss: -2533594.5000\n","Epoch [62/100], D Loss: 0.0000, G Loss: -2974430.2500\n","Epoch [63/100], D Loss: 0.0000, G Loss: -3429400.0000\n","Epoch [64/100], D Loss: 0.0000, G Loss: -2813166.0000\n","Epoch [65/100], D Loss: 0.0000, G Loss: -3267871.7500\n","Epoch [66/100], D Loss: 0.0000, G Loss: -3565122.0000\n","Epoch [67/100], D Loss: 0.0000, G Loss: -3119164.0000\n","Epoch [68/100], D Loss: 0.0000, G Loss: -3231519.7500\n","Epoch [69/100], D Loss: 0.0000, G Loss: -3540379.2500\n","Epoch [70/100], D Loss: 0.0000, G Loss: -3576055.7500\n","Epoch [71/100], D Loss: 0.0000, G Loss: -4505266.5000\n","Epoch [72/100], D Loss: 0.0000, G Loss: -4023373.2500\n","Epoch [73/100], D Loss: 0.0000, G Loss: -4131400.2500\n","Epoch [74/100], D Loss: 0.0000, G Loss: -4580553.5000\n","Epoch [75/100], D Loss: 0.0000, G Loss: -4403522.5000\n","Epoch [76/100], D Loss: 0.0000, G Loss: -4479107.5000\n","Epoch [77/100], D Loss: 0.0000, G Loss: -4761971.0000\n","Epoch [78/100], D Loss: 0.0000, G Loss: -3864494.7500\n","Epoch [79/100], D Loss: 0.0000, G Loss: -4608992.0000\n","Epoch [80/100], D Loss: 0.0000, G Loss: -4723608.5000\n","Epoch [81/100], D Loss: 0.0000, G Loss: -4068327.2500\n","Epoch [82/100], D Loss: 0.0000, G Loss: -5080428.5000\n","Epoch [83/100], D Loss: 0.0000, G Loss: -4591965.0000\n","Epoch [84/100], D Loss: 0.0000, G Loss: -5701915.5000\n","Epoch [85/100], D Loss: 0.0000, G Loss: -5326579.5000\n","Epoch [86/100], D Loss: 0.0000, G Loss: -5166664.5000\n","Epoch [87/100], D Loss: 0.0000, G Loss: -5445500.0000\n","Epoch [88/100], D Loss: 0.0000, G Loss: -6395624.5000\n","Epoch [89/100], D Loss: 0.0000, G Loss: -5749143.5000\n","Epoch [90/100], D Loss: 0.0000, G Loss: -6717834.5000\n","Epoch [91/100], D Loss: 0.0000, G Loss: -6357826.5000\n","Epoch [92/100], D Loss: 0.0000, G Loss: -6115089.5000\n","Epoch [93/100], D Loss: 0.0000, G Loss: -6552743.5000\n","Epoch [94/100], D Loss: 0.0000, G Loss: -6370272.5000\n","Epoch [95/100], D Loss: 0.0000, G Loss: -6569221.5000\n","Epoch [96/100], D Loss: 0.0000, G Loss: -6438540.0000\n","Epoch [97/100], D Loss: 0.0000, G Loss: -6629629.5000\n","Epoch [98/100], D Loss: 0.0000, G Loss: -6309732.5000\n","Epoch [99/100], D Loss: 0.0000, G Loss: -6142176.5000\n","Epoch [100/100], D Loss: 0.0000, G Loss: -7091214.5000\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 9802.94045648129,\n","    \"FID (Original vs Eval)\": 14.572063988153559,\n","    \"KL Divergence\": 1.0404609410241177,\n","    \"Cosine Similarity\": 0.023893725126981735,\n","    \"Spearman Rank Correlation\": -0.006485252405252406,\n","    \"Wasserstein Distance\": 0.21150985527776694,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.20218808829059393\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_AdvancedAutoencoder_barlow_twins.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/100], D Loss: 0.0263, G Loss: 4.3284\n","Epoch [2/100], D Loss: 0.5144, G Loss: 4.1529\n","Epoch [3/100], D Loss: 0.0055, G Loss: 6.0388\n","Epoch [4/100], D Loss: 0.0477, G Loss: 4.8459\n","Epoch [5/100], D Loss: 0.0100, G Loss: 6.1989\n","Epoch [6/100], D Loss: 0.0225, G Loss: 5.9199\n","Epoch [7/100], D Loss: 0.0932, G Loss: 4.2004\n","Epoch [8/100], D Loss: 0.0556, G Loss: 4.4874\n","Epoch [9/100], D Loss: 0.1096, G Loss: 4.5784\n","Epoch [10/100], D Loss: 0.0397, G Loss: 4.8195\n","Epoch [11/100], D Loss: 0.0096, G Loss: 6.1838\n","Epoch [12/100], D Loss: 0.0121, G Loss: 5.1423\n","Epoch [13/100], D Loss: 0.0545, G Loss: 5.0169\n","Epoch [14/100], D Loss: 0.0466, G Loss: 5.5741\n","Epoch [15/100], D Loss: 0.0710, G Loss: 4.4877\n","Epoch [16/100], D Loss: 0.5801, G Loss: 2.5265\n","Epoch [17/100], D Loss: 0.1745, G Loss: 6.8086\n","Epoch [18/100], D Loss: 0.2590, G Loss: 6.1268\n","Epoch [19/100], D Loss: 0.2166, G Loss: 8.8148\n","Epoch [20/100], D Loss: 0.0220, G Loss: 6.8237\n","Epoch [21/100], D Loss: 0.2048, G Loss: 5.9103\n","Epoch [22/100], D Loss: 0.1091, G Loss: 6.0885\n","Epoch [23/100], D Loss: 0.0504, G Loss: 6.2637\n","Epoch [24/100], D Loss: 0.0561, G Loss: 4.4363\n","Epoch [25/100], D Loss: 0.1256, G Loss: 4.7882\n","Epoch [26/100], D Loss: 0.0607, G Loss: 4.4963\n","Epoch [27/100], D Loss: 0.0887, G Loss: 5.7716\n","Epoch [28/100], D Loss: 0.0164, G Loss: 5.4055\n","Epoch [29/100], D Loss: 0.1382, G Loss: 4.7115\n","Epoch [30/100], D Loss: 0.3055, G Loss: 4.9448\n","Epoch [31/100], D Loss: 0.1986, G Loss: 4.0662\n","Epoch [32/100], D Loss: 0.3725, G Loss: 4.5554\n","Epoch [33/100], D Loss: 0.0680, G Loss: 5.3803\n","Epoch [34/100], D Loss: 0.0932, G Loss: 3.9654\n","Epoch [35/100], D Loss: 0.0573, G Loss: 5.1478\n","Epoch [36/100], D Loss: 0.1001, G Loss: 4.6405\n","Epoch [37/100], D Loss: 0.2041, G Loss: 4.1968\n","Epoch [38/100], D Loss: 0.1676, G Loss: 3.5245\n","Epoch [39/100], D Loss: 0.1729, G Loss: 3.8753\n","Epoch [40/100], D Loss: 0.4376, G Loss: 3.6332\n","Epoch [41/100], D Loss: 0.3502, G Loss: 3.3917\n","Epoch [42/100], D Loss: 0.1768, G Loss: 4.5871\n","Epoch [43/100], D Loss: 0.0850, G Loss: 5.0175\n","Epoch [44/100], D Loss: 0.3082, G Loss: 4.2377\n","Epoch [45/100], D Loss: 0.1025, G Loss: 4.7036\n","Epoch [46/100], D Loss: 0.2821, G Loss: 3.0594\n","Epoch [47/100], D Loss: 0.2340, G Loss: 5.5399\n","Epoch [48/100], D Loss: 0.1518, G Loss: 4.0156\n","Epoch [49/100], D Loss: 0.1891, G Loss: 4.7546\n","Epoch [50/100], D Loss: 0.1028, G Loss: 5.1143\n","Epoch [51/100], D Loss: 0.1693, G Loss: 4.7635\n","Epoch [52/100], D Loss: 0.3454, G Loss: 2.8327\n","Epoch [53/100], D Loss: 0.0984, G Loss: 4.1123\n","Epoch [54/100], D Loss: 0.3467, G Loss: 3.5310\n","Epoch [55/100], D Loss: 0.2261, G Loss: 3.3396\n","Epoch [56/100], D Loss: 0.1335, G Loss: 4.1570\n","Epoch [57/100], D Loss: 0.3731, G Loss: 3.3057\n","Epoch [58/100], D Loss: 0.2230, G Loss: 4.9247\n","Epoch [59/100], D Loss: 0.3786, G Loss: 3.3098\n","Epoch [60/100], D Loss: 0.3074, G Loss: 3.6663\n","Epoch [61/100], D Loss: 0.1397, G Loss: 4.2710\n","Epoch [62/100], D Loss: 0.4653, G Loss: 5.7495\n","Epoch [63/100], D Loss: 0.3497, G Loss: 4.3551\n","Epoch [64/100], D Loss: 0.1982, G Loss: 4.2284\n","Epoch [65/100], D Loss: 0.4330, G Loss: 5.0193\n","Epoch [66/100], D Loss: 0.1808, G Loss: 4.0481\n","Epoch [67/100], D Loss: 0.1100, G Loss: 4.7897\n","Epoch [68/100], D Loss: 0.5431, G Loss: 2.8100\n","Epoch [69/100], D Loss: 0.3884, G Loss: 3.8951\n","Epoch [70/100], D Loss: 0.2883, G Loss: 3.6980\n","Epoch [71/100], D Loss: 0.3474, G Loss: 3.6801\n","Epoch [72/100], D Loss: 0.5961, G Loss: 2.8014\n","Epoch [73/100], D Loss: 0.1820, G Loss: 3.9354\n","Epoch [74/100], D Loss: 0.1972, G Loss: 3.7921\n","Epoch [75/100], D Loss: 0.8157, G Loss: 4.9455\n","Epoch [76/100], D Loss: 0.4409, G Loss: 3.4401\n","Epoch [77/100], D Loss: 0.2099, G Loss: 4.1210\n","Epoch [78/100], D Loss: 0.2106, G Loss: 3.8780\n","Epoch [79/100], D Loss: 0.3459, G Loss: 3.3312\n","Epoch [80/100], D Loss: 0.1685, G Loss: 3.2051\n","Epoch [81/100], D Loss: 0.4991, G Loss: 3.1458\n","Epoch [82/100], D Loss: 0.4305, G Loss: 3.2733\n","Epoch [83/100], D Loss: 0.1105, G Loss: 3.9922\n","Epoch [84/100], D Loss: 0.5132, G Loss: 3.2909\n","Epoch [85/100], D Loss: 0.2300, G Loss: 3.6176\n","Epoch [86/100], D Loss: 0.2424, G Loss: 4.1780\n","Epoch [87/100], D Loss: 0.1385, G Loss: 3.4134\n","Epoch [88/100], D Loss: 0.1419, G Loss: 3.1941\n","Epoch [89/100], D Loss: 0.8913, G Loss: 3.1310\n","Epoch [90/100], D Loss: 0.5707, G Loss: 2.9810\n","Epoch [91/100], D Loss: 0.4979, G Loss: 4.3224\n","Epoch [92/100], D Loss: 0.3161, G Loss: 3.1133\n","Epoch [93/100], D Loss: 0.3095, G Loss: 3.6190\n","Epoch [94/100], D Loss: 0.3101, G Loss: 3.5421\n","Epoch [95/100], D Loss: 0.4769, G Loss: 2.7244\n","Epoch [96/100], D Loss: 0.4477, G Loss: 3.4806\n","Epoch [97/100], D Loss: 0.5687, G Loss: 3.0816\n","Epoch [98/100], D Loss: 0.3429, G Loss: 3.2031\n","Epoch [99/100], D Loss: 0.3109, G Loss: 3.1479\n","Epoch [100/100], D Loss: 0.2497, G Loss: 3.1467\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoder_AdvancedAutoencoder_contrastive/AdvancedAutoencoder_contrastive_embeddings.pt\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{\n","    \"FID (Original vs Generated)\": 9805.101701114867,\n","    \"FID (Original vs Eval)\": 14.572063988153559,\n","    \"KL Divergence\": 1.087592931943656,\n","    \"Cosine Similarity\": 0.022753773257136345,\n","    \"Spearman Rank Correlation\": 0.0001342818142818143,\n","    \"Wasserstein Distance\": 0.20614021000235144,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2004090919594395\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_AdvancedAutoencoder_barlow_twins.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n","\n","ðŸš€ Processing embedding: autoencoder_AdvancedAutoencoder_contrastive/AdvancedAutoencoder_contrastive_embeddings.pt\n","\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/GAN-thesis-project/src/data_utils.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss Critic: -344.0791, Loss Generator: -32.8177\n","Epoch [2/100], Loss Critic: -254.3502, Loss Generator: -67.9747\n","Epoch [3/100], Loss Critic: -207.2059, Loss Generator: -83.5320\n","Epoch [4/100], Loss Critic: -198.7449, Loss Generator: -77.0375\n","Epoch [5/100], Loss Critic: -163.8967, Loss Generator: -69.1879\n","Epoch [6/100], Loss Critic: -133.4260, Loss Generator: -15.3523\n","Epoch [7/100], Loss Critic: -107.1184, Loss Generator: -15.4292\n","Epoch [8/100], Loss Critic: -69.3341, Loss Generator: -17.4251\n","Epoch [9/100], Loss Critic: -68.5634, Loss Generator: -23.3014\n","Epoch [10/100], Loss Critic: -68.8129, Loss Generator: -21.5212\n","Epoch [11/100], Loss Critic: -61.1024, Loss Generator: -21.2977\n","Epoch [12/100], Loss Critic: -39.8845, Loss Generator: -8.8829\n","Epoch [13/100], Loss Critic: -36.1329, Loss Generator: -0.5370\n","Epoch [14/100], Loss Critic: -32.8410, Loss Generator: -9.2272\n","Epoch [15/100], Loss Critic: -28.4055, Loss Generator: -8.9424\n","Epoch [16/100], Loss Critic: -16.9508, Loss Generator: -12.8667\n","Epoch [17/100], Loss Critic: -35.9034, Loss Generator: -6.4335\n","Epoch [18/100], Loss Critic: -30.2065, Loss Generator: -6.9560\n","Epoch [19/100], Loss Critic: -14.6790, Loss Generator: -2.2194\n","Epoch [20/100], Loss Critic: -5.5367, Loss Generator: 1.2835\n","Epoch [21/100], Loss Critic: -4.2808, Loss Generator: -0.5944\n","Epoch [22/100], Loss Critic: -3.6706, Loss Generator: 1.3387\n","Epoch [23/100], Loss Critic: -11.9815, Loss Generator: -2.3486\n","Epoch [24/100], Loss Critic: -9.2703, Loss Generator: -3.7775\n","Epoch [25/100], Loss Critic: -65.7789, Loss Generator: -3.6246\n","Epoch [26/100], Loss Critic: -4.1511, Loss Generator: -3.0209\n","Epoch [27/100], Loss Critic: -4.5339, Loss Generator: -0.5761\n","Epoch [28/100], Loss Critic: -24.8958, Loss Generator: -1.5792\n","Epoch [29/100], Loss Critic: -26.2951, Loss Generator: 0.3467\n","Epoch [30/100], Loss Critic: -38.7689, Loss Generator: -3.0898\n","Epoch [31/100], Loss Critic: -28.7418, Loss Generator: 2.3539\n","Epoch [32/100], Loss Critic: -20.2052, Loss Generator: 6.1445\n","Epoch [33/100], Loss Critic: -28.9402, Loss Generator: 0.5573\n","Epoch [34/100], Loss Critic: -56.9009, Loss Generator: 3.6175\n","Epoch [35/100], Loss Critic: -29.9740, Loss Generator: 3.5132\n","Epoch [36/100], Loss Critic: -36.9216, Loss Generator: -2.0289\n","Epoch [37/100], Loss Critic: -33.5707, Loss Generator: 0.9391\n","Epoch [38/100], Loss Critic: -43.8752, Loss Generator: 4.3333\n","Epoch [39/100], Loss Critic: -20.2011, Loss Generator: -2.5955\n","Epoch [40/100], Loss Critic: -23.3020, Loss Generator: -5.4588\n","Epoch [41/100], Loss Critic: -37.9782, Loss Generator: 3.7895\n","Epoch [42/100], Loss Critic: -22.7850, Loss Generator: -0.3805\n","Epoch [43/100], Loss Critic: -28.6639, Loss Generator: 4.2260\n","Epoch [44/100], Loss Critic: -28.5828, Loss Generator: 1.3705\n","Epoch [45/100], Loss Critic: -43.3841, Loss Generator: 3.5274\n","Epoch [46/100], Loss Critic: -32.0864, Loss Generator: -1.2519\n","Epoch [47/100], Loss Critic: -31.8705, Loss Generator: 5.4076\n","Epoch [48/100], Loss Critic: -25.6655, Loss Generator: 5.1318\n","Epoch [49/100], Loss Critic: -28.7118, Loss Generator: 0.0323\n","Epoch [50/100], Loss Critic: -31.2267, Loss Generator: 0.1649\n","Epoch [51/100], Loss Critic: -23.6866, Loss Generator: 9.7268\n","Epoch [52/100], Loss Critic: -20.6228, Loss Generator: 6.1132\n","Epoch [53/100], Loss Critic: -31.6657, Loss Generator: -0.1255\n","Epoch [54/100], Loss Critic: -32.3615, Loss Generator: 6.7102\n","Epoch [55/100], Loss Critic: -29.6869, Loss Generator: 3.4515\n","Epoch [56/100], Loss Critic: -42.0030, Loss Generator: 9.3411\n","Epoch [57/100], Loss Critic: -25.4511, Loss Generator: 3.7376\n","Epoch [58/100], Loss Critic: -40.4689, Loss Generator: 10.7146\n","Epoch [59/100], Loss Critic: -32.2846, Loss Generator: 13.9272\n","Epoch [60/100], Loss Critic: -25.3871, Loss Generator: 12.6209\n","Epoch [61/100], Loss Critic: -21.0454, Loss Generator: 9.7840\n","Epoch [62/100], Loss Critic: -24.8312, Loss Generator: 13.1371\n","Epoch [63/100], Loss Critic: -21.1013, Loss Generator: 16.1904\n","Epoch [64/100], Loss Critic: -37.5345, Loss Generator: 14.1213\n","Epoch [65/100], Loss Critic: -40.4160, Loss Generator: 17.6769\n","Epoch [66/100], Loss Critic: -37.3720, Loss Generator: 9.0843\n","Epoch [67/100], Loss Critic: -25.5508, Loss Generator: 12.6683\n","Epoch [68/100], Loss Critic: -16.9746, Loss Generator: 5.2128\n","Epoch [69/100], Loss Critic: -41.8062, Loss Generator: 14.9100\n","Epoch [70/100], Loss Critic: -45.4064, Loss Generator: 15.9374\n","Epoch [71/100], Loss Critic: -20.1256, Loss Generator: 13.1465\n","Epoch [72/100], Loss Critic: -16.5928, Loss Generator: 15.2826\n","Epoch [73/100], Loss Critic: -37.8938, Loss Generator: 22.9935\n","Epoch [74/100], Loss Critic: -30.5901, Loss Generator: 16.0268\n","Epoch [75/100], Loss Critic: -26.4255, Loss Generator: 17.9732\n","Epoch [76/100], Loss Critic: -20.4986, Loss Generator: 21.6821\n","Epoch [77/100], Loss Critic: -27.7900, Loss Generator: 15.3849\n","Epoch [78/100], Loss Critic: -33.1774, Loss Generator: 19.5600\n","Epoch [79/100], Loss Critic: -27.7334, Loss Generator: 18.7409\n","Epoch [80/100], Loss Critic: -21.5376, Loss Generator: 19.1413\n","Epoch [81/100], Loss Critic: -23.6572, Loss Generator: 19.3454\n","Epoch [82/100], Loss Critic: -25.6061, Loss Generator: 23.7962\n","Epoch [83/100], Loss Critic: -17.3455, Loss Generator: 12.6700\n","Epoch [84/100], Loss Critic: -29.8756, Loss Generator: 28.0539\n","Epoch [85/100], Loss Critic: -26.1241, Loss Generator: 26.1912\n","Epoch [86/100], Loss Critic: -19.3840, Loss Generator: 28.9860\n","Epoch [87/100], Loss Critic: -31.0648, Loss Generator: 30.1580\n","Epoch [88/100], Loss Critic: -25.0223, Loss Generator: 27.6668\n","Epoch [89/100], Loss Critic: -35.8815, Loss Generator: 23.1762\n","Epoch [90/100], Loss Critic: -37.3735, Loss Generator: 31.7302\n","Epoch [91/100], Loss Critic: -26.8138, Loss Generator: 36.7675\n","Epoch [92/100], Loss Critic: -23.3781, Loss Generator: 27.1526\n","Epoch [93/100], Loss Critic: -32.5823, Loss Generator: 31.1706\n","Epoch [94/100], Loss Critic: -25.6192, Loss Generator: 29.6764\n","Epoch [95/100], Loss Critic: -37.9297, Loss Generator: 37.4016\n","Epoch [96/100], Loss Critic: -35.9890, Loss Generator: 26.5634\n","Epoch [97/100], Loss Critic: -30.2975, Loss Generator: 39.7565\n","Epoch [98/100], Loss Critic: -32.5418, Loss Generator: 36.3250\n","Epoch [99/100], Loss Critic: -34.7122, Loss Generator: 31.9831\n","Epoch [100/100], Loss Critic: -24.1608, Loss Generator: 41.1930\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 11422.456626513691,\n","    \"FID (Original vs Eval)\": 21.660818827412584,\n","    \"KL Divergence\": 0.5385836585803124,\n","    \"Cosine Similarity\": -0.0008625093614682555,\n","    \"Spearman Rank Correlation\": -0.007888922368922371,\n","    \"Wasserstein Distance\": 0.02749999914740995,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2290633032309218\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_AdvancedAutoencoder_contrastive.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/100], D Loss: 0.0030, G Loss: 22036236.0000\n","Epoch [2/100], D Loss: 0.0005, G Loss: 14731.5020\n","Epoch [3/100], D Loss: 0.0001, G Loss: 176897.0469\n","Epoch [4/100], D Loss: 0.0005, G Loss: 16243.8584\n","Epoch [5/100], D Loss: 0.0000, G Loss: 14256.5889\n","Epoch [6/100], D Loss: 0.0000, G Loss: 8142.6880\n","Epoch [7/100], D Loss: 0.0000, G Loss: 6694.6802\n","Epoch [8/100], D Loss: 0.0000, G Loss: 5368.0625\n","Epoch [9/100], D Loss: 0.0004, G Loss: 5105.9385\n","Epoch [10/100], D Loss: 0.0000, G Loss: 1997.0907\n","Epoch [11/100], D Loss: 0.0001, G Loss: 4386.0776\n","Epoch [12/100], D Loss: 0.0000, G Loss: 2125.2168\n","Epoch [13/100], D Loss: 0.0000, G Loss: 1581.4341\n","Epoch [14/100], D Loss: 0.0002, G Loss: 1364.9323\n","Epoch [15/100], D Loss: 0.0000, G Loss: 1201.1709\n","Epoch [16/100], D Loss: 0.0000, G Loss: 1282.1815\n","Epoch [17/100], D Loss: 0.0042, G Loss: 1002.6202\n","Epoch [18/100], D Loss: 0.0001, G Loss: 705.9458\n","Epoch [19/100], D Loss: 0.0009, G Loss: 695.5097\n","Epoch [20/100], D Loss: 0.0000, G Loss: 552.7867\n","Epoch [21/100], D Loss: 0.2558, G Loss: 477.2167\n","Epoch [22/100], D Loss: 0.0010, G Loss: 462.8279\n","Epoch [23/100], D Loss: 0.0004, G Loss: 370.0580\n","Epoch [24/100], D Loss: 0.0000, G Loss: 444.1470\n","Epoch [25/100], D Loss: 0.0002, G Loss: 371.4540\n","Epoch [26/100], D Loss: 0.0014, G Loss: 342.7208\n","Epoch [27/100], D Loss: 0.0000, G Loss: 346.8980\n","Epoch [28/100], D Loss: 0.0000, G Loss: 256.7179\n","Epoch [29/100], D Loss: 0.0002, G Loss: 309.7329\n","Epoch [30/100], D Loss: 0.0005, G Loss: 324.8900\n","Epoch [31/100], D Loss: 0.0213, G Loss: 259.6848\n","Epoch [32/100], D Loss: 0.0000, G Loss: 268.3657\n","Epoch [33/100], D Loss: 0.0000, G Loss: 267.9676\n","Epoch [34/100], D Loss: 0.0000, G Loss: 266.7791\n","Epoch [35/100], D Loss: 0.0004, G Loss: 270.6078\n","Epoch [36/100], D Loss: 0.1377, G Loss: 218.5047\n","Epoch [37/100], D Loss: 0.0011, G Loss: 246.4955\n","Epoch [38/100], D Loss: 0.0000, G Loss: 292.8576\n","Epoch [39/100], D Loss: 0.0000, G Loss: 271.2516\n","Epoch [40/100], D Loss: 0.0000, G Loss: 280.9312\n","Epoch [41/100], D Loss: 0.0000, G Loss: 292.6929\n","Epoch [42/100], D Loss: 0.0000, G Loss: 298.8536\n","Epoch [43/100], D Loss: 0.0000, G Loss: 295.1578\n","Epoch [44/100], D Loss: 0.0000, G Loss: 287.9276\n","Epoch [45/100], D Loss: 0.0000, G Loss: 307.0367\n","Epoch [46/100], D Loss: 0.0000, G Loss: 254.2387\n","Epoch [47/100], D Loss: 0.0000, G Loss: 294.5971\n","Epoch [48/100], D Loss: 0.0000, G Loss: 304.6886\n","Epoch [49/100], D Loss: 0.0000, G Loss: 287.0536\n","Epoch [50/100], D Loss: 0.0000, G Loss: 288.4296\n","Epoch [51/100], D Loss: 0.0000, G Loss: 303.3741\n","Epoch [52/100], D Loss: 0.0000, G Loss: 339.1390\n","Epoch [53/100], D Loss: 0.0000, G Loss: 259.6354\n","Epoch [54/100], D Loss: 0.0000, G Loss: 266.2440\n","Epoch [55/100], D Loss: 0.0000, G Loss: 273.2750\n","Epoch [56/100], D Loss: 0.0000, G Loss: 319.2950\n","Epoch [57/100], D Loss: 0.0000, G Loss: 289.8373\n","Epoch [58/100], D Loss: 0.0000, G Loss: 304.6478\n","Epoch [59/100], D Loss: 0.0000, G Loss: 311.0883\n","Epoch [60/100], D Loss: 0.0000, G Loss: 321.8362\n","Epoch [61/100], D Loss: 0.0000, G Loss: 287.7906\n","Epoch [62/100], D Loss: 0.0000, G Loss: 269.5532\n","Epoch [63/100], D Loss: 0.0000, G Loss: 262.5133\n","Epoch [64/100], D Loss: 0.0000, G Loss: 300.4757\n","Epoch [65/100], D Loss: 0.0000, G Loss: 295.6358\n","Epoch [66/100], D Loss: 0.0000, G Loss: 313.6056\n","Epoch [67/100], D Loss: 0.0000, G Loss: 299.5019\n","Epoch [68/100], D Loss: 0.0000, G Loss: 299.0187\n","Epoch [69/100], D Loss: 0.0000, G Loss: 311.0664\n","Epoch [70/100], D Loss: 0.0000, G Loss: 284.4462\n","Epoch [71/100], D Loss: 0.0000, G Loss: 287.2965\n","Epoch [72/100], D Loss: 0.0000, G Loss: 323.7703\n","Epoch [73/100], D Loss: 0.0000, G Loss: 280.7871\n","Epoch [74/100], D Loss: 0.0000, G Loss: 306.2086\n","Epoch [75/100], D Loss: 0.0000, G Loss: 267.1542\n","Epoch [76/100], D Loss: 0.0000, G Loss: 320.5207\n","Epoch [77/100], D Loss: 0.0000, G Loss: 289.7945\n","Epoch [78/100], D Loss: 0.0000, G Loss: 304.9838\n","Epoch [79/100], D Loss: 0.0000, G Loss: 273.2490\n","Epoch [80/100], D Loss: 0.0000, G Loss: 277.2109\n","Epoch [81/100], D Loss: 0.0000, G Loss: 304.3268\n","Epoch [82/100], D Loss: 0.0000, G Loss: 329.5953\n","Epoch [83/100], D Loss: 0.0000, G Loss: 282.6150\n","Epoch [84/100], D Loss: 0.0000, G Loss: 282.7024\n","Epoch [85/100], D Loss: 0.0000, G Loss: 278.0886\n","Epoch [86/100], D Loss: 0.0000, G Loss: 280.6333\n","Epoch [87/100], D Loss: 0.0000, G Loss: 266.5715\n","Epoch [88/100], D Loss: 0.0000, G Loss: 266.5746\n","Epoch [89/100], D Loss: 0.0000, G Loss: 363.9122\n","Epoch [90/100], D Loss: 0.0000, G Loss: 291.5120\n","Epoch [91/100], D Loss: 0.0000, G Loss: 276.2694\n","Epoch [92/100], D Loss: 0.0000, G Loss: 268.5641\n","Epoch [93/100], D Loss: 0.0000, G Loss: 295.0168\n","Epoch [94/100], D Loss: 0.0000, G Loss: 287.3235\n","Epoch [95/100], D Loss: 0.0000, G Loss: 293.2922\n","Epoch [96/100], D Loss: 0.0000, G Loss: 354.7405\n","Epoch [97/100], D Loss: 0.0000, G Loss: 313.5555\n","Epoch [98/100], D Loss: 0.0000, G Loss: 269.2491\n","Epoch [99/100], D Loss: 0.0000, G Loss: 316.5331\n","Epoch [100/100], D Loss: 0.0000, G Loss: 317.8452\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 11421.763975285701,\n","    \"FID (Original vs Eval)\": 21.660818827412584,\n","    \"KL Divergence\": 0.5219620160200326,\n","    \"Cosine Similarity\": -0.0017687624786049128,\n","    \"Spearman Rank Correlation\": -0.003131080811080812,\n","    \"Wasserstein Distance\": 0.033017602568722985,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2307774814285365\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_AdvancedAutoencoder_contrastive.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/100], D Loss: 0.0010, G Loss: 5.8587\n","Epoch [2/100], D Loss: 0.0008, G Loss: -251.1283\n","Epoch [3/100], D Loss: 0.0000, G Loss: -437.0308\n","Epoch [4/100], D Loss: 0.0000, G Loss: -123.3962\n","Epoch [5/100], D Loss: 0.0000, G Loss: 337.0929\n","Epoch [6/100], D Loss: 0.0000, G Loss: -373.3537\n","Epoch [7/100], D Loss: 0.0000, G Loss: -1742.9695\n","Epoch [8/100], D Loss: 0.0000, G Loss: 498.9105\n","Epoch [9/100], D Loss: 0.0000, G Loss: -2192.7664\n","Epoch [10/100], D Loss: 0.0000, G Loss: 2730.1326\n","Epoch [11/100], D Loss: 0.0000, G Loss: -7947.8433\n","Epoch [12/100], D Loss: 0.0000, G Loss: -5590.4004\n","Epoch [13/100], D Loss: 0.0000, G Loss: -10288.8262\n","Epoch [14/100], D Loss: 0.0000, G Loss: -1209.4524\n","Epoch [15/100], D Loss: 0.0000, G Loss: -3065.2839\n","Epoch [16/100], D Loss: 0.0000, G Loss: -7867.5444\n","Epoch [17/100], D Loss: 0.0000, G Loss: -8487.8896\n","Epoch [18/100], D Loss: 0.0000, G Loss: -3319.4629\n","Epoch [19/100], D Loss: 0.0000, G Loss: -10568.8682\n","Epoch [20/100], D Loss: 0.0000, G Loss: -14189.2490\n","Epoch [21/100], D Loss: 0.0000, G Loss: -8109.3584\n","Epoch [22/100], D Loss: 0.0000, G Loss: -6410.4185\n","Epoch [23/100], D Loss: 0.0000, G Loss: -2545.3201\n","Epoch [24/100], D Loss: 0.0000, G Loss: -23192.5938\n","Epoch [25/100], D Loss: 0.0000, G Loss: -7120.3979\n","Epoch [26/100], D Loss: 0.0000, G Loss: 5981.5522\n","Epoch [27/100], D Loss: 0.0000, G Loss: -26810.4434\n","Epoch [28/100], D Loss: 0.0000, G Loss: -13499.7109\n","Epoch [29/100], D Loss: 0.0000, G Loss: -26335.3047\n","Epoch [30/100], D Loss: 0.0000, G Loss: -38068.2266\n","Epoch [31/100], D Loss: 0.0000, G Loss: -22766.3242\n","Epoch [32/100], D Loss: 0.0000, G Loss: -2838.7590\n","Epoch [33/100], D Loss: 0.0000, G Loss: -2927.6770\n","Epoch [34/100], D Loss: 0.0000, G Loss: -2026.4801\n","Epoch [35/100], D Loss: 0.0000, G Loss: -34235.8594\n","Epoch [36/100], D Loss: 0.0000, G Loss: -82477.9453\n","Epoch [37/100], D Loss: 0.0000, G Loss: -25807.6875\n","Epoch [38/100], D Loss: 0.0000, G Loss: 18624.7832\n","Epoch [39/100], D Loss: 0.0000, G Loss: -33175.6992\n","Epoch [40/100], D Loss: 0.0000, G Loss: -10284.3877\n","Epoch [41/100], D Loss: 0.0000, G Loss: -39058.0781\n","Epoch [42/100], D Loss: 0.0000, G Loss: -86069.7188\n","Epoch [43/100], D Loss: 0.0000, G Loss: 37504.3008\n","Epoch [44/100], D Loss: 0.0000, G Loss: -68277.2109\n","Epoch [45/100], D Loss: 0.0000, G Loss: -45798.8516\n","Epoch [46/100], D Loss: 0.0000, G Loss: -188679.2344\n","Epoch [47/100], D Loss: 0.0000, G Loss: -7527.9243\n","Epoch [48/100], D Loss: 0.0000, G Loss: 42793.0039\n","Epoch [49/100], D Loss: 0.0000, G Loss: -71984.7578\n","Epoch [50/100], D Loss: 0.0000, G Loss: -54734.6875\n","Epoch [51/100], D Loss: 0.0000, G Loss: -41311.9414\n","Epoch [52/100], D Loss: 0.0000, G Loss: 64417.2305\n","Epoch [53/100], D Loss: 0.0000, G Loss: -131816.0156\n","Epoch [54/100], D Loss: 0.0000, G Loss: -156444.3281\n","Epoch [55/100], D Loss: 0.0000, G Loss: 25470.7754\n","Epoch [56/100], D Loss: 0.0000, G Loss: 17625.1309\n","Epoch [57/100], D Loss: 0.0000, G Loss: -35663.4258\n","Epoch [58/100], D Loss: 0.0000, G Loss: -58373.5195\n","Epoch [59/100], D Loss: 0.0000, G Loss: -104218.7891\n","Epoch [60/100], D Loss: 0.0000, G Loss: 190411.5469\n","Epoch [61/100], D Loss: 0.0000, G Loss: -76530.7109\n","Epoch [62/100], D Loss: 0.0000, G Loss: -92001.4375\n","Epoch [63/100], D Loss: 0.0000, G Loss: -182722.7969\n","Epoch [64/100], D Loss: 0.0000, G Loss: -40073.8789\n","Epoch [65/100], D Loss: 0.0000, G Loss: -126193.7109\n","Epoch [66/100], D Loss: 0.0000, G Loss: -173049.8906\n","Epoch [67/100], D Loss: 0.0000, G Loss: -109928.0000\n","Epoch [68/100], D Loss: 0.0000, G Loss: -310590.9062\n","Epoch [69/100], D Loss: 0.0000, G Loss: -52726.0664\n","Epoch [70/100], D Loss: 0.0000, G Loss: -296809.5312\n","Epoch [71/100], D Loss: 0.0000, G Loss: -112292.7578\n","Epoch [72/100], D Loss: 0.0000, G Loss: 29430.0254\n","Epoch [73/100], D Loss: 0.0000, G Loss: -163003.8906\n","Epoch [74/100], D Loss: 0.0000, G Loss: -23356.6309\n","Epoch [75/100], D Loss: 0.0000, G Loss: -60024.7695\n","Epoch [76/100], D Loss: 0.0000, G Loss: -231324.5625\n","Epoch [77/100], D Loss: 0.0000, G Loss: -13697.0107\n","Epoch [78/100], D Loss: 0.0000, G Loss: 36843.1758\n","Epoch [79/100], D Loss: 0.0000, G Loss: -39554.5898\n","Epoch [80/100], D Loss: 0.0000, G Loss: -47527.6406\n","Epoch [81/100], D Loss: 0.0000, G Loss: -818.4062\n","Epoch [82/100], D Loss: 0.0000, G Loss: -291973.8750\n","Epoch [83/100], D Loss: 0.0000, G Loss: 50633.1758\n","Epoch [84/100], D Loss: 0.0000, G Loss: -148204.5000\n","Epoch [85/100], D Loss: 0.0000, G Loss: -64850.9492\n","Epoch [86/100], D Loss: 0.0000, G Loss: 21743.0566\n","Epoch [87/100], D Loss: 0.0000, G Loss: -154145.8594\n","Epoch [88/100], D Loss: 0.0000, G Loss: -185788.7656\n","Epoch [89/100], D Loss: 0.0000, G Loss: -204599.1094\n","Epoch [90/100], D Loss: 0.0000, G Loss: -359336.6562\n","Epoch [91/100], D Loss: 0.0000, G Loss: -130439.3438\n","Epoch [92/100], D Loss: 0.0000, G Loss: -102022.5859\n","Epoch [93/100], D Loss: 0.0000, G Loss: -380874.2500\n","Epoch [94/100], D Loss: 0.0000, G Loss: -250586.9375\n","Epoch [95/100], D Loss: 0.0000, G Loss: -189231.2500\n","Epoch [96/100], D Loss: 0.0000, G Loss: -98080.5312\n","Epoch [97/100], D Loss: 0.0000, G Loss: -109363.7891\n","Epoch [98/100], D Loss: 0.0000, G Loss: -379180.2500\n","Epoch [99/100], D Loss: 0.0000, G Loss: 49975.9688\n","Epoch [100/100], D Loss: 0.0000, G Loss: 19112.7500\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 11609.562187180441,\n","    \"FID (Original vs Eval)\": 21.660818827412584,\n","    \"KL Divergence\": 0.7741094945319502,\n","    \"Cosine Similarity\": -0.0021776636131107807,\n","    \"Spearman Rank Correlation\": 0.008395151035151037,\n","    \"Wasserstein Distance\": 0.025389144568691714,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.21317104729644265\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_AdvancedAutoencoder_contrastive.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/100], D Loss: 0.0317, G Loss: 4.3917\n","Epoch [2/100], D Loss: 0.0152, G Loss: 5.4423\n","Epoch [3/100], D Loss: 0.0290, G Loss: 5.0317\n","Epoch [4/100], D Loss: 0.4104, G Loss: 3.7442\n","Epoch [5/100], D Loss: 0.0117, G Loss: 5.8689\n","Epoch [6/100], D Loss: 0.0328, G Loss: 5.5958\n","Epoch [7/100], D Loss: 0.0241, G Loss: 5.1183\n","Epoch [8/100], D Loss: 0.1671, G Loss: 7.0076\n","Epoch [9/100], D Loss: 0.0603, G Loss: 5.5775\n","Epoch [10/100], D Loss: 0.0193, G Loss: 5.4047\n","Epoch [11/100], D Loss: 0.0756, G Loss: 6.1687\n","Epoch [12/100], D Loss: 0.4115, G Loss: 4.5553\n","Epoch [13/100], D Loss: 0.0295, G Loss: 5.9600\n","Epoch [14/100], D Loss: 0.0266, G Loss: 5.5864\n","Epoch [15/100], D Loss: 0.0820, G Loss: 7.9444\n","Epoch [16/100], D Loss: 0.1276, G Loss: 7.7706\n","Epoch [17/100], D Loss: 0.2097, G Loss: 6.4085\n","Epoch [18/100], D Loss: 0.4056, G Loss: 4.1112\n","Epoch [19/100], D Loss: 0.0943, G Loss: 5.4866\n","Epoch [20/100], D Loss: 0.0340, G Loss: 4.7216\n","Epoch [21/100], D Loss: 0.0276, G Loss: 5.1172\n","Epoch [22/100], D Loss: 0.0489, G Loss: 7.8919\n","Epoch [23/100], D Loss: 0.4033, G Loss: 5.0862\n","Epoch [24/100], D Loss: 0.0455, G Loss: 9.0785\n","Epoch [25/100], D Loss: 0.1023, G Loss: 5.1986\n","Epoch [26/100], D Loss: 0.1503, G Loss: 5.5176\n","Epoch [27/100], D Loss: 0.3663, G Loss: 7.6228\n","Epoch [28/100], D Loss: 0.0824, G Loss: 5.1395\n","Epoch [29/100], D Loss: 0.0281, G Loss: 6.0673\n","Epoch [30/100], D Loss: 0.0441, G Loss: 5.6152\n","Epoch [31/100], D Loss: 0.0732, G Loss: 5.2025\n","Epoch [32/100], D Loss: 0.2205, G Loss: 4.2618\n","Epoch [33/100], D Loss: 0.1654, G Loss: 4.9797\n","Epoch [34/100], D Loss: 0.0829, G Loss: 5.3871\n","Epoch [35/100], D Loss: 0.1060, G Loss: 5.9875\n","Epoch [36/100], D Loss: 0.5068, G Loss: 5.5813\n","Epoch [37/100], D Loss: 0.0845, G Loss: 5.0425\n","Epoch [38/100], D Loss: 0.3357, G Loss: 4.8708\n","Epoch [39/100], D Loss: 0.3809, G Loss: 5.8220\n","Epoch [40/100], D Loss: 0.0486, G Loss: 6.3999\n","Epoch [41/100], D Loss: 0.5535, G Loss: 4.8589\n","Epoch [42/100], D Loss: 0.0742, G Loss: 3.9552\n","Epoch [43/100], D Loss: 0.1777, G Loss: 5.1543\n","Epoch [44/100], D Loss: 0.2269, G Loss: 4.4994\n","Epoch [45/100], D Loss: 0.3869, G Loss: 4.2048\n","Epoch [46/100], D Loss: 0.1960, G Loss: 4.6910\n","Epoch [47/100], D Loss: 0.1159, G Loss: 4.6599\n","Epoch [48/100], D Loss: 0.2157, G Loss: 4.1794\n","Epoch [49/100], D Loss: 0.0555, G Loss: 4.4994\n","Epoch [50/100], D Loss: 0.6614, G Loss: 4.1532\n","Epoch [51/100], D Loss: 0.3677, G Loss: 5.0994\n","Epoch [52/100], D Loss: 0.2900, G Loss: 3.9972\n","Epoch [53/100], D Loss: 0.1212, G Loss: 4.5192\n","Epoch [54/100], D Loss: 0.2825, G Loss: 5.1395\n","Epoch [55/100], D Loss: 0.3387, G Loss: 4.6258\n","Epoch [56/100], D Loss: 0.1610, G Loss: 4.1995\n","Epoch [57/100], D Loss: 0.6021, G Loss: 3.3105\n","Epoch [58/100], D Loss: 0.4522, G Loss: 5.1199\n","Epoch [59/100], D Loss: 0.1904, G Loss: 4.0541\n","Epoch [60/100], D Loss: 0.0908, G Loss: 4.7146\n","Epoch [61/100], D Loss: 0.1770, G Loss: 4.2428\n","Epoch [62/100], D Loss: 0.1535, G Loss: 4.3309\n","Epoch [63/100], D Loss: 0.4309, G Loss: 4.2727\n","Epoch [64/100], D Loss: 0.1567, G Loss: 3.9231\n","Epoch [65/100], D Loss: 0.4936, G Loss: 3.9716\n","Epoch [66/100], D Loss: 0.4006, G Loss: 4.2699\n","Epoch [67/100], D Loss: 0.2830, G Loss: 4.2372\n","Epoch [68/100], D Loss: 0.3298, G Loss: 4.0662\n","Epoch [69/100], D Loss: 0.4078, G Loss: 3.4133\n","Epoch [70/100], D Loss: 0.4435, G Loss: 3.6208\n","Epoch [71/100], D Loss: 0.2506, G Loss: 3.7240\n","Epoch [72/100], D Loss: 0.3765, G Loss: 3.9422\n","Epoch [73/100], D Loss: 0.5234, G Loss: 3.6605\n","Epoch [74/100], D Loss: 0.4316, G Loss: 5.3124\n","Epoch [75/100], D Loss: 0.2365, G Loss: 3.8707\n","Epoch [76/100], D Loss: 0.5375, G Loss: 3.8364\n","Epoch [77/100], D Loss: 0.2500, G Loss: 3.7972\n","Epoch [78/100], D Loss: 0.1755, G Loss: 4.9923\n","Epoch [79/100], D Loss: 0.1698, G Loss: 4.0100\n","Epoch [80/100], D Loss: 0.1894, G Loss: 4.8040\n","Epoch [81/100], D Loss: 0.4400, G Loss: 4.4994\n","Epoch [82/100], D Loss: 0.3209, G Loss: 3.3670\n","Epoch [83/100], D Loss: 0.2236, G Loss: 4.7935\n","Epoch [84/100], D Loss: 0.2387, G Loss: 5.1785\n","Epoch [85/100], D Loss: 0.2719, G Loss: 3.4059\n","Epoch [86/100], D Loss: 0.3090, G Loss: 3.5490\n","Epoch [87/100], D Loss: 0.4137, G Loss: 3.7100\n","Epoch [88/100], D Loss: 0.2943, G Loss: 4.1577\n","Epoch [89/100], D Loss: 0.2319, G Loss: 4.2702\n","Epoch [90/100], D Loss: 0.3747, G Loss: 3.4561\n","Epoch [91/100], D Loss: 0.4073, G Loss: 3.8664\n","Epoch [92/100], D Loss: 0.2845, G Loss: 4.6529\n","Epoch [93/100], D Loss: 0.2833, G Loss: 4.6100\n","Epoch [94/100], D Loss: 0.1807, G Loss: 4.2952\n","Epoch [95/100], D Loss: 0.2828, G Loss: 3.9340\n","Epoch [96/100], D Loss: 0.3461, G Loss: 4.4384\n","Epoch [97/100], D Loss: 0.2697, G Loss: 4.7205\n","Epoch [98/100], D Loss: 0.4523, G Loss: 4.1230\n","Epoch [99/100], D Loss: 0.2657, G Loss: 3.7809\n","Epoch [100/100], D Loss: 0.3252, G Loss: 4.2663\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n"]},{"output_type":"stream","name":"stderr","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoder_AdvancedAutoencoder_info_nce/AdvancedAutoencoder_info_nce_embeddings.pt\n"]},{"output_type":"stream","name":"stdout","text":["{\n","    \"FID (Original vs Generated)\": 11610.250352752015,\n","    \"FID (Original vs Eval)\": 21.660818827412584,\n","    \"KL Divergence\": 0.48849150040044664,\n","    \"Cosine Similarity\": 0.003447829745709896,\n","    \"Spearman Rank Correlation\": -0.003566887886887888,\n","    \"Wasserstein Distance\": 0.0318328487486994,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.23473154689242354\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_AdvancedAutoencoder_contrastive.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n","\n","ðŸš€ Processing embedding: autoencoder_AdvancedAutoencoder_info_nce/AdvancedAutoencoder_info_nce_embeddings.pt\n","\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/GAN-thesis-project/src/data_utils.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss Critic: -537.3763, Loss Generator: -49.0260\n","Epoch [2/100], Loss Critic: -413.9948, Loss Generator: -103.7260\n","Epoch [3/100], Loss Critic: -379.7723, Loss Generator: -115.3143\n","Epoch [4/100], Loss Critic: -326.1129, Loss Generator: -110.3955\n","Epoch [5/100], Loss Critic: -265.7595, Loss Generator: -109.8174\n","Epoch [6/100], Loss Critic: -259.5085, Loss Generator: -94.4175\n","Epoch [7/100], Loss Critic: -191.3212, Loss Generator: -79.1310\n","Epoch [8/100], Loss Critic: -142.6521, Loss Generator: -10.7109\n","Epoch [9/100], Loss Critic: -122.6847, Loss Generator: -30.4980\n","Epoch [10/100], Loss Critic: -97.8936, Loss Generator: -25.4067\n","Epoch [11/100], Loss Critic: -90.9831, Loss Generator: -22.1375\n","Epoch [12/100], Loss Critic: -84.2026, Loss Generator: -48.8489\n","Epoch [13/100], Loss Critic: -78.8044, Loss Generator: -35.7615\n","Epoch [14/100], Loss Critic: -55.5019, Loss Generator: -9.2877\n","Epoch [15/100], Loss Critic: -58.6011, Loss Generator: 1.6077\n","Epoch [16/100], Loss Critic: -53.6036, Loss Generator: -1.0168\n","Epoch [17/100], Loss Critic: -46.2010, Loss Generator: -11.9946\n","Epoch [18/100], Loss Critic: -57.3276, Loss Generator: -16.1608\n","Epoch [19/100], Loss Critic: -23.7030, Loss Generator: -12.5987\n","Epoch [20/100], Loss Critic: -26.6360, Loss Generator: -13.0182\n","Epoch [21/100], Loss Critic: -24.0795, Loss Generator: -5.9983\n","Epoch [22/100], Loss Critic: -13.5562, Loss Generator: 2.3231\n","Epoch [23/100], Loss Critic: -5.8854, Loss Generator: -1.0299\n","Epoch [24/100], Loss Critic: -10.8637, Loss Generator: 3.3755\n","Epoch [25/100], Loss Critic: -9.9131, Loss Generator: 7.1146\n","Epoch [26/100], Loss Critic: -31.6142, Loss Generator: 2.2292\n","Epoch [27/100], Loss Critic: -24.9670, Loss Generator: 2.0884\n","Epoch [28/100], Loss Critic: -25.1981, Loss Generator: -6.4169\n","Epoch [29/100], Loss Critic: -46.3578, Loss Generator: 15.4899\n","Epoch [30/100], Loss Critic: -52.3268, Loss Generator: 13.9623\n","Epoch [31/100], Loss Critic: -42.3687, Loss Generator: 9.6098\n","Epoch [32/100], Loss Critic: -68.3250, Loss Generator: 4.4240\n","Epoch [33/100], Loss Critic: -65.2655, Loss Generator: 2.7706\n","Epoch [34/100], Loss Critic: -49.1092, Loss Generator: 10.1579\n","Epoch [35/100], Loss Critic: -70.5053, Loss Generator: 3.8916\n","Epoch [36/100], Loss Critic: -74.8989, Loss Generator: 14.5846\n","Epoch [37/100], Loss Critic: -82.6397, Loss Generator: 4.5858\n","Epoch [38/100], Loss Critic: -74.7542, Loss Generator: 19.2328\n","Epoch [39/100], Loss Critic: -79.3504, Loss Generator: 10.1615\n","Epoch [40/100], Loss Critic: -93.7469, Loss Generator: 4.0771\n","Epoch [41/100], Loss Critic: -76.4160, Loss Generator: 6.1120\n","Epoch [42/100], Loss Critic: -72.2584, Loss Generator: -1.2912\n","Epoch [43/100], Loss Critic: -83.1933, Loss Generator: 10.8386\n","Epoch [44/100], Loss Critic: -98.5198, Loss Generator: 6.4568\n","Epoch [45/100], Loss Critic: -72.4169, Loss Generator: 7.6873\n","Epoch [46/100], Loss Critic: -92.1626, Loss Generator: 15.0731\n","Epoch [47/100], Loss Critic: -85.3506, Loss Generator: 18.0573\n","Epoch [48/100], Loss Critic: -76.0908, Loss Generator: 30.1775\n","Epoch [49/100], Loss Critic: -92.3926, Loss Generator: 25.7472\n","Epoch [50/100], Loss Critic: -60.2558, Loss Generator: 22.3641\n","Epoch [51/100], Loss Critic: -72.6061, Loss Generator: 18.5303\n","Epoch [52/100], Loss Critic: -117.5865, Loss Generator: 22.5273\n","Epoch [53/100], Loss Critic: -74.6798, Loss Generator: 21.4253\n","Epoch [54/100], Loss Critic: -80.1188, Loss Generator: 26.9076\n","Epoch [55/100], Loss Critic: -113.2341, Loss Generator: 30.9350\n","Epoch [56/100], Loss Critic: -124.3553, Loss Generator: 41.4171\n","Epoch [57/100], Loss Critic: -115.2301, Loss Generator: 25.6406\n","Epoch [58/100], Loss Critic: -130.2677, Loss Generator: 29.8498\n","Epoch [59/100], Loss Critic: -85.0884, Loss Generator: 25.8238\n","Epoch [60/100], Loss Critic: -108.5839, Loss Generator: 35.0459\n","Epoch [61/100], Loss Critic: -83.5749, Loss Generator: 27.5922\n","Epoch [62/100], Loss Critic: -84.6138, Loss Generator: 18.8271\n","Epoch [63/100], Loss Critic: -93.1927, Loss Generator: 30.4729\n","Epoch [64/100], Loss Critic: -86.7831, Loss Generator: 20.1269\n","Epoch [65/100], Loss Critic: -91.2279, Loss Generator: 33.8164\n","Epoch [66/100], Loss Critic: -77.4183, Loss Generator: 19.7716\n","Epoch [67/100], Loss Critic: -82.9205, Loss Generator: 19.7977\n","Epoch [68/100], Loss Critic: -94.9181, Loss Generator: 39.6050\n","Epoch [69/100], Loss Critic: -67.2418, Loss Generator: 22.1272\n","Epoch [70/100], Loss Critic: -61.5019, Loss Generator: 27.7509\n","Epoch [71/100], Loss Critic: -78.4346, Loss Generator: 23.2148\n","Epoch [72/100], Loss Critic: -85.6044, Loss Generator: 19.4538\n","Epoch [73/100], Loss Critic: -93.7272, Loss Generator: 19.1651\n","Epoch [74/100], Loss Critic: -99.6210, Loss Generator: 33.5090\n","Epoch [75/100], Loss Critic: -82.6237, Loss Generator: 36.5767\n","Epoch [76/100], Loss Critic: -68.7970, Loss Generator: 27.2264\n","Epoch [77/100], Loss Critic: -66.0170, Loss Generator: 11.7414\n","Epoch [78/100], Loss Critic: -85.8853, Loss Generator: 26.7917\n","Epoch [79/100], Loss Critic: -85.7001, Loss Generator: 27.0969\n","Epoch [80/100], Loss Critic: -77.9103, Loss Generator: 35.5509\n","Epoch [81/100], Loss Critic: -79.3365, Loss Generator: 19.0600\n","Epoch [82/100], Loss Critic: -72.5435, Loss Generator: 28.2847\n","Epoch [83/100], Loss Critic: -61.9639, Loss Generator: 24.7656\n","Epoch [84/100], Loss Critic: -70.5573, Loss Generator: 33.7797\n","Epoch [85/100], Loss Critic: -72.5963, Loss Generator: 29.8410\n","Epoch [86/100], Loss Critic: -73.9851, Loss Generator: 33.7047\n","Epoch [87/100], Loss Critic: -69.7099, Loss Generator: 37.5913\n","Epoch [88/100], Loss Critic: -93.3747, Loss Generator: 24.9474\n","Epoch [89/100], Loss Critic: -76.2264, Loss Generator: 56.6391\n","Epoch [90/100], Loss Critic: -59.5176, Loss Generator: 38.0252\n","Epoch [91/100], Loss Critic: -77.8154, Loss Generator: 35.0095\n","Epoch [92/100], Loss Critic: -74.3792, Loss Generator: 43.5102\n","Epoch [93/100], Loss Critic: -73.4152, Loss Generator: 27.8145\n","Epoch [94/100], Loss Critic: -61.5537, Loss Generator: 36.3239\n","Epoch [95/100], Loss Critic: -89.2347, Loss Generator: 32.2884\n","Epoch [96/100], Loss Critic: -80.4148, Loss Generator: 55.2968\n","Epoch [97/100], Loss Critic: -73.0803, Loss Generator: 42.9730\n","Epoch [98/100], Loss Critic: -70.0887, Loss Generator: 36.6149\n","Epoch [99/100], Loss Critic: -69.2010, Loss Generator: 56.2550\n","Epoch [100/100], Loss Critic: -63.1304, Loss Generator: 48.3598\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 20589.548948479656,\n","    \"FID (Original vs Eval)\": 43.55558403556802,\n","    \"KL Divergence\": 0.5767309709516806,\n","    \"Cosine Similarity\": 0.0019365616608411074,\n","    \"Spearman Rank Correlation\": 0.00396918228918229,\n","    \"Wasserstein Distance\": 0.03733598021116155,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.22764837788446574\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_AdvancedAutoencoder_info_nce.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/100], D Loss: 0.0030, G Loss: 35096332.0000\n","Epoch [2/100], D Loss: 0.0008, G Loss: 522363.7500\n","Epoch [3/100], D Loss: 0.0002, G Loss: 6219659.0000\n","Epoch [4/100], D Loss: 0.0002, G Loss: 16232022.0000\n","Epoch [5/100], D Loss: 0.0001, G Loss: 455646.6250\n","Epoch [6/100], D Loss: 0.0003, G Loss: 1443455.2500\n","Epoch [7/100], D Loss: 0.0000, G Loss: 798071.0625\n","Epoch [8/100], D Loss: 0.0000, G Loss: 55537648.0000\n","Epoch [9/100], D Loss: 0.0000, G Loss: 361806.9062\n","Epoch [10/100], D Loss: 0.0001, G Loss: 196338.2812\n","Epoch [11/100], D Loss: 0.0000, G Loss: 73747688.0000\n","Epoch [12/100], D Loss: 0.0000, G Loss: 266135.1562\n","Epoch [13/100], D Loss: 0.0000, G Loss: 104496.3125\n","Epoch [14/100], D Loss: 0.0001, G Loss: 37370.0898\n","Epoch [15/100], D Loss: 0.0000, G Loss: 33745.1328\n","Epoch [16/100], D Loss: 0.0000, G Loss: 504511.5938\n","Epoch [17/100], D Loss: 0.0000, G Loss: 43111.4648\n","Epoch [18/100], D Loss: 0.0000, G Loss: 139089.7656\n","Epoch [19/100], D Loss: 0.0000, G Loss: 19973.4180\n","Epoch [20/100], D Loss: 0.0002, G Loss: 213579.0312\n","Epoch [21/100], D Loss: 0.0000, G Loss: 12639.6797\n","Epoch [22/100], D Loss: 0.0000, G Loss: 37224.3125\n","Epoch [23/100], D Loss: 0.0000, G Loss: 6976.3286\n","Epoch [24/100], D Loss: 0.0000, G Loss: 5646.0264\n","Epoch [25/100], D Loss: 0.0000, G Loss: 5183.5664\n","Epoch [26/100], D Loss: 0.0000, G Loss: 7730.7539\n","Epoch [27/100], D Loss: 0.0000, G Loss: 4073.2981\n","Epoch [28/100], D Loss: 0.0000, G Loss: 1944.5536\n","Epoch [29/100], D Loss: 0.0000, G Loss: 2477.4717\n","Epoch [30/100], D Loss: 0.0002, G Loss: 2221.2776\n","Epoch [31/100], D Loss: 0.0000, G Loss: 2052.8159\n","Epoch [32/100], D Loss: 0.0000, G Loss: 1218.8965\n","Epoch [33/100], D Loss: 0.0000, G Loss: 1212.3535\n","Epoch [34/100], D Loss: 0.0000, G Loss: 2864.5608\n","Epoch [35/100], D Loss: 0.0000, G Loss: 1278.1134\n","Epoch [36/100], D Loss: 0.0000, G Loss: 839.9468\n","Epoch [37/100], D Loss: 0.0001, G Loss: 746.5927\n","Epoch [38/100], D Loss: 0.0000, G Loss: 884.7282\n","Epoch [39/100], D Loss: 0.0000, G Loss: 661.0286\n","Epoch [40/100], D Loss: 0.0001, G Loss: 717.8834\n","Epoch [41/100], D Loss: 0.0001, G Loss: 568.8737\n","Epoch [42/100], D Loss: 0.0000, G Loss: 463.6178\n","Epoch [43/100], D Loss: 0.0009, G Loss: 460.5427\n","Epoch [44/100], D Loss: 0.0001, G Loss: 443.4869\n","Epoch [45/100], D Loss: 0.0000, G Loss: 487.1406\n","Epoch [46/100], D Loss: 0.0000, G Loss: 514.9681\n","Epoch [47/100], D Loss: 0.0000, G Loss: 514.2440\n","Epoch [48/100], D Loss: 0.0000, G Loss: 538.9144\n","Epoch [49/100], D Loss: 0.0000, G Loss: 506.9218\n","Epoch [50/100], D Loss: 0.0000, G Loss: 558.1798\n","Epoch [51/100], D Loss: 0.0000, G Loss: 446.8852\n","Epoch [52/100], D Loss: 0.0000, G Loss: 472.9604\n","Epoch [53/100], D Loss: 0.0000, G Loss: 386.9868\n","Epoch [54/100], D Loss: 0.0000, G Loss: 472.0303\n","Epoch [55/100], D Loss: 0.0000, G Loss: 422.0265\n","Epoch [56/100], D Loss: 0.0000, G Loss: 423.8239\n","Epoch [57/100], D Loss: 0.0000, G Loss: 442.7016\n","Epoch [58/100], D Loss: 0.0000, G Loss: 449.4642\n","Epoch [59/100], D Loss: 0.0000, G Loss: 498.4225\n","Epoch [60/100], D Loss: 0.0000, G Loss: 526.8108\n","Epoch [61/100], D Loss: 0.0000, G Loss: 500.6814\n","Epoch [62/100], D Loss: 0.0000, G Loss: 472.3918\n","Epoch [63/100], D Loss: 0.0000, G Loss: 468.5008\n","Epoch [64/100], D Loss: 0.0000, G Loss: 465.0197\n","Epoch [65/100], D Loss: 0.0000, G Loss: 431.0308\n","Epoch [66/100], D Loss: 0.0000, G Loss: 441.2800\n","Epoch [67/100], D Loss: 0.0000, G Loss: 500.8327\n","Epoch [68/100], D Loss: 0.0000, G Loss: 434.8499\n","Epoch [69/100], D Loss: 0.0000, G Loss: 472.3174\n","Epoch [70/100], D Loss: 0.0000, G Loss: 522.8711\n","Epoch [71/100], D Loss: 0.0000, G Loss: 444.3765\n","Epoch [72/100], D Loss: 0.0000, G Loss: 476.9400\n","Epoch [73/100], D Loss: 0.0000, G Loss: 473.6064\n","Epoch [74/100], D Loss: 0.0000, G Loss: 412.3851\n","Epoch [75/100], D Loss: 0.0000, G Loss: 527.1147\n","Epoch [76/100], D Loss: 0.0000, G Loss: 483.1357\n","Epoch [77/100], D Loss: 0.0000, G Loss: 487.3646\n","Epoch [78/100], D Loss: 0.0000, G Loss: 438.3196\n","Epoch [79/100], D Loss: 0.0000, G Loss: 430.9001\n","Epoch [80/100], D Loss: 0.0000, G Loss: 621.9346\n","Epoch [81/100], D Loss: 0.0000, G Loss: 468.3181\n","Epoch [82/100], D Loss: 0.0000, G Loss: 501.2195\n","Epoch [83/100], D Loss: 0.0000, G Loss: 456.0431\n","Epoch [84/100], D Loss: 0.0000, G Loss: 443.0005\n","Epoch [85/100], D Loss: 0.0000, G Loss: 411.5693\n","Epoch [86/100], D Loss: 0.0000, G Loss: 444.8636\n","Epoch [87/100], D Loss: 0.0000, G Loss: 489.3027\n","Epoch [88/100], D Loss: 0.0000, G Loss: 509.7797\n","Epoch [89/100], D Loss: 0.0000, G Loss: 419.1478\n","Epoch [90/100], D Loss: 0.0000, G Loss: 479.1899\n","Epoch [91/100], D Loss: 0.0000, G Loss: 509.9667\n","Epoch [92/100], D Loss: 0.0000, G Loss: 491.9855\n","Epoch [93/100], D Loss: 0.0000, G Loss: 443.1281\n","Epoch [94/100], D Loss: 0.0000, G Loss: 382.5352\n","Epoch [95/100], D Loss: 0.0000, G Loss: 397.1189\n","Epoch [96/100], D Loss: 0.0000, G Loss: 486.9758\n","Epoch [97/100], D Loss: 0.0000, G Loss: 497.2578\n","Epoch [98/100], D Loss: 0.0000, G Loss: 496.2626\n","Epoch [99/100], D Loss: 0.0000, G Loss: 566.5778\n","Epoch [100/100], D Loss: 0.0000, G Loss: 449.2335\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 20596.969282385453,\n","    \"FID (Original vs Eval)\": 43.55558403556802,\n","    \"KL Divergence\": 0.6245546649854156,\n","    \"Cosine Similarity\": -0.004621445666998625,\n","    \"Spearman Rank Correlation\": -0.004149417909417911,\n","    \"Wasserstein Distance\": 0.037585414211867764,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.221790850304728\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_AdvancedAutoencoder_info_nce.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/100], D Loss: 0.0017, G Loss: -738.2960\n","Epoch [2/100], D Loss: 0.0014, G Loss: -339.5332\n","Epoch [3/100], D Loss: 0.0000, G Loss: -3833.0691\n","Epoch [4/100], D Loss: 0.0000, G Loss: 1276.8351\n","Epoch [5/100], D Loss: 0.0002, G Loss: -5713.7900\n","Epoch [6/100], D Loss: 0.0000, G Loss: -14398.1113\n","Epoch [7/100], D Loss: 0.0000, G Loss: -3422.4192\n","Epoch [8/100], D Loss: 0.0000, G Loss: -7798.2227\n","Epoch [9/100], D Loss: 0.0000, G Loss: -20389.5039\n","Epoch [10/100], D Loss: 0.0000, G Loss: -16626.5566\n","Epoch [11/100], D Loss: 0.0000, G Loss: -50703.1836\n","Epoch [12/100], D Loss: 0.0000, G Loss: -36136.1445\n","Epoch [13/100], D Loss: 0.0000, G Loss: -37646.7617\n","Epoch [14/100], D Loss: 0.0000, G Loss: -28867.2051\n","Epoch [15/100], D Loss: 0.0000, G Loss: -86324.8047\n","Epoch [16/100], D Loss: 0.0000, G Loss: -52320.5742\n","Epoch [17/100], D Loss: 0.0000, G Loss: -23402.0449\n","Epoch [18/100], D Loss: 0.0000, G Loss: -44872.6914\n","Epoch [19/100], D Loss: 0.0000, G Loss: 16571.4980\n","Epoch [20/100], D Loss: 0.0000, G Loss: -49039.0625\n","Epoch [21/100], D Loss: 0.0000, G Loss: -148024.6094\n","Epoch [22/100], D Loss: 0.0000, G Loss: -58907.8633\n","Epoch [23/100], D Loss: 0.0000, G Loss: -86869.6484\n","Epoch [24/100], D Loss: 0.0000, G Loss: -128069.7188\n","Epoch [25/100], D Loss: 0.0000, G Loss: -243449.6094\n","Epoch [26/100], D Loss: 0.0000, G Loss: -188452.1094\n","Epoch [27/100], D Loss: 0.0000, G Loss: -183311.9531\n","Epoch [28/100], D Loss: 0.0000, G Loss: -68218.1094\n","Epoch [29/100], D Loss: 0.0000, G Loss: -127459.0391\n","Epoch [30/100], D Loss: 0.0000, G Loss: -136928.3906\n","Epoch [31/100], D Loss: 0.0000, G Loss: -178580.5625\n","Epoch [32/100], D Loss: 0.0000, G Loss: -224496.6875\n","Epoch [33/100], D Loss: 0.0000, G Loss: -216809.8906\n","Epoch [34/100], D Loss: 0.0000, G Loss: -163341.9531\n","Epoch [35/100], D Loss: 0.0000, G Loss: -166397.7031\n","Epoch [36/100], D Loss: 0.0000, G Loss: -103148.3438\n","Epoch [37/100], D Loss: 0.0000, G Loss: -314854.5625\n","Epoch [38/100], D Loss: 0.0000, G Loss: -45228.7812\n","Epoch [39/100], D Loss: 0.0000, G Loss: -194314.3125\n","Epoch [40/100], D Loss: 0.0000, G Loss: -82687.3750\n","Epoch [41/100], D Loss: 0.0000, G Loss: -300093.7812\n","Epoch [42/100], D Loss: 0.0000, G Loss: -335783.7188\n","Epoch [43/100], D Loss: 0.0000, G Loss: -207722.7500\n","Epoch [44/100], D Loss: 0.0000, G Loss: -106591.5859\n","Epoch [45/100], D Loss: 0.0000, G Loss: -179450.5781\n","Epoch [46/100], D Loss: 0.0000, G Loss: -369611.0312\n","Epoch [47/100], D Loss: 0.0000, G Loss: -241726.9531\n","Epoch [48/100], D Loss: 0.0000, G Loss: 210856.5156\n","Epoch [49/100], D Loss: 0.0000, G Loss: -17680.3848\n","Epoch [50/100], D Loss: 0.0000, G Loss: -357592.7500\n","Epoch [51/100], D Loss: 0.0000, G Loss: -575675.3125\n","Epoch [52/100], D Loss: 0.0000, G Loss: -353800.7188\n","Epoch [53/100], D Loss: 0.0000, G Loss: -374326.4062\n","Epoch [54/100], D Loss: 0.0000, G Loss: -480228.2500\n","Epoch [55/100], D Loss: 0.0000, G Loss: -1095368.6250\n","Epoch [56/100], D Loss: 0.0000, G Loss: -426382.5312\n","Epoch [57/100], D Loss: 0.0000, G Loss: -60297.6133\n","Epoch [58/100], D Loss: 0.0000, G Loss: -825862.5000\n","Epoch [59/100], D Loss: 0.0000, G Loss: 17059.9375\n","Epoch [60/100], D Loss: 0.0000, G Loss: -1022285.3125\n","Epoch [61/100], D Loss: 0.0000, G Loss: -640743.2500\n","Epoch [62/100], D Loss: 0.0000, G Loss: -1132353.1250\n","Epoch [63/100], D Loss: 0.0000, G Loss: -718944.3125\n","Epoch [64/100], D Loss: 0.0000, G Loss: -776687.0000\n","Epoch [65/100], D Loss: 0.0000, G Loss: -725753.7500\n","Epoch [66/100], D Loss: 0.0000, G Loss: -834117.8125\n","Epoch [67/100], D Loss: 0.0000, G Loss: -532949.8125\n","Epoch [68/100], D Loss: 0.0000, G Loss: -810862.1875\n","Epoch [69/100], D Loss: 0.0000, G Loss: -1034223.1875\n","Epoch [70/100], D Loss: 0.0000, G Loss: -1214166.6250\n","Epoch [71/100], D Loss: 0.0000, G Loss: -1182347.7500\n","Epoch [72/100], D Loss: 0.0000, G Loss: -1090964.0000\n","Epoch [73/100], D Loss: 0.0000, G Loss: -1040862.7500\n","Epoch [74/100], D Loss: 0.0000, G Loss: -193000.0156\n","Epoch [75/100], D Loss: 0.0000, G Loss: -980402.2500\n","Epoch [76/100], D Loss: 0.0000, G Loss: -1723201.0000\n","Epoch [77/100], D Loss: 0.0000, G Loss: -1292203.0000\n","Epoch [78/100], D Loss: 0.0000, G Loss: -1635805.6250\n","Epoch [79/100], D Loss: 0.0000, G Loss: -414622.7812\n","Epoch [80/100], D Loss: 0.0000, G Loss: -1397828.5000\n","Epoch [81/100], D Loss: 0.0000, G Loss: -1338716.8750\n","Epoch [82/100], D Loss: 0.0000, G Loss: -854373.0000\n","Epoch [83/100], D Loss: 0.0000, G Loss: -1109196.8750\n","Epoch [84/100], D Loss: 0.0000, G Loss: -1452104.5000\n","Epoch [85/100], D Loss: 0.0000, G Loss: -868164.0625\n","Epoch [86/100], D Loss: 0.0000, G Loss: -1806363.3750\n","Epoch [87/100], D Loss: 0.0000, G Loss: -1344948.8750\n","Epoch [88/100], D Loss: 0.0000, G Loss: -633882.6875\n","Epoch [89/100], D Loss: 0.0000, G Loss: -2072353.5000\n","Epoch [90/100], D Loss: 0.0000, G Loss: -1027084.9375\n","Epoch [91/100], D Loss: 0.0000, G Loss: -1578889.0000\n","Epoch [92/100], D Loss: 0.0000, G Loss: 386076.5000\n","Epoch [93/100], D Loss: 0.0000, G Loss: -212637.1250\n","Epoch [94/100], D Loss: 0.0000, G Loss: -1843738.6250\n","Epoch [95/100], D Loss: 0.0000, G Loss: -2740675.5000\n","Epoch [96/100], D Loss: 0.0000, G Loss: -1101240.1250\n","Epoch [97/100], D Loss: 0.0000, G Loss: -1258404.0000\n","Epoch [98/100], D Loss: 0.0000, G Loss: -1053806.5000\n","Epoch [99/100], D Loss: 0.0000, G Loss: -1602836.8750\n","Epoch [100/100], D Loss: 0.0000, G Loss: -1500378.8750\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 20843.552161006286,\n","    \"FID (Original vs Eval)\": 43.55558403556802,\n","    \"KL Divergence\": 0.5348989446704192,\n","    \"Cosine Similarity\": -0.006855238229036331,\n","    \"Spearman Rank Correlation\": -0.003076420036420037,\n","    \"Wasserstein Distance\": 0.031303928991596584,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2286422379224263\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_AdvancedAutoencoder_info_nce.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/100], D Loss: 0.0701, G Loss: 3.5898\n","Epoch [2/100], D Loss: 0.1381, G Loss: 3.7675\n","Epoch [3/100], D Loss: 0.0262, G Loss: 5.4552\n","Epoch [4/100], D Loss: 0.1186, G Loss: 8.2783\n","Epoch [5/100], D Loss: 0.0030, G Loss: 6.4038\n","Epoch [6/100], D Loss: 0.0062, G Loss: 6.0190\n","Epoch [7/100], D Loss: 0.0393, G Loss: 5.9368\n","Epoch [8/100], D Loss: 0.0160, G Loss: 5.5728\n","Epoch [9/100], D Loss: 0.0152, G Loss: 5.9077\n","Epoch [10/100], D Loss: 0.3752, G Loss: 5.0603\n","Epoch [11/100], D Loss: 0.0225, G Loss: 6.2058\n","Epoch [12/100], D Loss: 0.0222, G Loss: 5.3955\n","Epoch [13/100], D Loss: 0.2727, G Loss: 5.4590\n","Epoch [14/100], D Loss: 0.0368, G Loss: 5.5360\n","Epoch [15/100], D Loss: 0.0104, G Loss: 5.9906\n","Epoch [16/100], D Loss: 0.0084, G Loss: 5.8450\n","Epoch [17/100], D Loss: 0.0090, G Loss: 5.6000\n","Epoch [18/100], D Loss: 0.0491, G Loss: 5.9163\n","Epoch [19/100], D Loss: 0.0275, G Loss: 5.2810\n","Epoch [20/100], D Loss: 0.0056, G Loss: 6.1786\n","Epoch [21/100], D Loss: 0.0250, G Loss: 5.2953\n","Epoch [22/100], D Loss: 0.0132, G Loss: 5.7630\n","Epoch [23/100], D Loss: 0.0249, G Loss: 6.0087\n","Epoch [24/100], D Loss: 0.3558, G Loss: 4.8274\n","Epoch [25/100], D Loss: 0.0275, G Loss: 5.3614\n","Epoch [26/100], D Loss: 0.0272, G Loss: 6.0306\n","Epoch [27/100], D Loss: 0.0104, G Loss: 6.6724\n","Epoch [28/100], D Loss: 0.0127, G Loss: 5.6736\n","Epoch [29/100], D Loss: 0.0403, G Loss: 6.3088\n","Epoch [30/100], D Loss: 0.0821, G Loss: 5.5213\n","Epoch [31/100], D Loss: 0.0175, G Loss: 5.6905\n","Epoch [32/100], D Loss: 0.0110, G Loss: 6.4822\n","Epoch [33/100], D Loss: 0.0477, G Loss: 6.0137\n","Epoch [34/100], D Loss: 0.0422, G Loss: 6.0810\n","Epoch [35/100], D Loss: 0.0406, G Loss: 5.7217\n","Epoch [36/100], D Loss: 0.1535, G Loss: 6.7682\n","Epoch [37/100], D Loss: 0.0215, G Loss: 7.2404\n","Epoch [38/100], D Loss: 0.1526, G Loss: 5.7307\n","Epoch [39/100], D Loss: 0.1147, G Loss: 6.3573\n","Epoch [40/100], D Loss: 0.2108, G Loss: 5.3597\n","Epoch [41/100], D Loss: 0.0240, G Loss: 5.5469\n","Epoch [42/100], D Loss: 0.0213, G Loss: 5.9779\n","Epoch [43/100], D Loss: 0.0217, G Loss: 6.0546\n","Epoch [44/100], D Loss: 0.0265, G Loss: 6.7070\n","Epoch [45/100], D Loss: 0.0993, G Loss: 6.4704\n","Epoch [46/100], D Loss: 0.0186, G Loss: 5.8572\n","Epoch [47/100], D Loss: 0.1140, G Loss: 6.0570\n","Epoch [48/100], D Loss: 0.0271, G Loss: 6.1695\n","Epoch [49/100], D Loss: 0.0098, G Loss: 6.4476\n","Epoch [50/100], D Loss: 0.1280, G Loss: 7.5063\n","Epoch [51/100], D Loss: 0.0420, G Loss: 6.2416\n","Epoch [52/100], D Loss: 0.0935, G Loss: 8.3229\n","Epoch [53/100], D Loss: 0.0455, G Loss: 6.1665\n","Epoch [54/100], D Loss: 0.1057, G Loss: 6.3235\n","Epoch [55/100], D Loss: 0.2092, G Loss: 4.7591\n","Epoch [56/100], D Loss: 0.1467, G Loss: 8.3558\n","Epoch [57/100], D Loss: 0.0633, G Loss: 5.1330\n","Epoch [58/100], D Loss: 0.0203, G Loss: 6.0323\n","Epoch [59/100], D Loss: 0.0507, G Loss: 6.2248\n","Epoch [60/100], D Loss: 0.0081, G Loss: 7.2932\n","Epoch [61/100], D Loss: 0.1603, G Loss: 5.7040\n","Epoch [62/100], D Loss: 0.0269, G Loss: 5.3445\n","Epoch [63/100], D Loss: 0.3431, G Loss: 5.7760\n","Epoch [64/100], D Loss: 0.1023, G Loss: 5.5888\n","Epoch [65/100], D Loss: 0.1419, G Loss: 4.8754\n","Epoch [66/100], D Loss: 0.0793, G Loss: 7.1250\n","Epoch [67/100], D Loss: 0.0945, G Loss: 5.6297\n","Epoch [68/100], D Loss: 0.0820, G Loss: 7.8808\n","Epoch [69/100], D Loss: 0.0990, G Loss: 5.3386\n","Epoch [70/100], D Loss: 0.3517, G Loss: 4.5972\n","Epoch [71/100], D Loss: 0.1056, G Loss: 6.2914\n","Epoch [72/100], D Loss: 0.0374, G Loss: 5.5832\n","Epoch [73/100], D Loss: 0.0430, G Loss: 4.7507\n","Epoch [74/100], D Loss: 0.3273, G Loss: 5.5444\n","Epoch [75/100], D Loss: 0.0448, G Loss: 6.7034\n","Epoch [76/100], D Loss: 0.1386, G Loss: 5.1382\n","Epoch [77/100], D Loss: 0.0728, G Loss: 4.8163\n","Epoch [78/100], D Loss: 0.2166, G Loss: 4.7857\n","Epoch [79/100], D Loss: 0.3419, G Loss: 4.7635\n","Epoch [80/100], D Loss: 0.1553, G Loss: 4.9319\n","Epoch [81/100], D Loss: 0.3033, G Loss: 5.6127\n","Epoch [82/100], D Loss: 0.3295, G Loss: 5.0191\n","Epoch [83/100], D Loss: 0.1479, G Loss: 4.9867\n","Epoch [84/100], D Loss: 0.1836, G Loss: 5.0727\n","Epoch [85/100], D Loss: 0.1544, G Loss: 5.0520\n","Epoch [86/100], D Loss: 0.1011, G Loss: 5.6896\n","Epoch [87/100], D Loss: 0.0822, G Loss: 5.0126\n","Epoch [88/100], D Loss: 0.2118, G Loss: 5.2503\n","Epoch [89/100], D Loss: 0.0958, G Loss: 5.3650\n","Epoch [90/100], D Loss: 0.1121, G Loss: 5.5216\n","Epoch [91/100], D Loss: 0.1824, G Loss: 4.7393\n","Epoch [92/100], D Loss: 0.0817, G Loss: 6.4013\n","Epoch [93/100], D Loss: 0.1309, G Loss: 5.1134\n","Epoch [94/100], D Loss: 0.0498, G Loss: 5.2136\n","Epoch [95/100], D Loss: 0.1353, G Loss: 6.1931\n","Epoch [96/100], D Loss: 0.0839, G Loss: 4.9901\n","Epoch [97/100], D Loss: 0.1152, G Loss: 5.3535\n","Epoch [98/100], D Loss: 0.1864, G Loss: 4.5415\n","Epoch [99/100], D Loss: 0.1961, G Loss: 6.3823\n","Epoch [100/100], D Loss: 0.7125, G Loss: 4.8308\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n"]},{"output_type":"stream","name":"stderr","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoder_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n"]},{"output_type":"stream","name":"stdout","text":["{\n","    \"FID (Original vs Generated)\": 20836.58342176412,\n","    \"FID (Original vs Eval)\": 43.55558403556802,\n","    \"KL Divergence\": 0.504732735377227,\n","    \"Cosine Similarity\": -0.0037232432514429092,\n","    \"Spearman Rank Correlation\": 0.009121525441525443,\n","    \"Wasserstein Distance\": 0.03150981391654281,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2331006688826057\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_AdvancedAutoencoder_info_nce.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n","\n","ðŸš€ Processing embedding: autoencoder_AdvancedAutoencoder_mse/AdvancedAutoencoder_mse_embeddings.pt\n","\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n","Epoch [1/100], Loss Critic: -84.3829, Loss Generator: -0.0751\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/GAN-thesis-project/src/data_utils.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/100], Loss Critic: -187.2560, Loss Generator: -0.2011\n","Epoch [3/100], Loss Critic: -316.5254, Loss Generator: -0.4725\n","Epoch [4/100], Loss Critic: -474.1641, Loss Generator: -0.9036\n","Epoch [5/100], Loss Critic: -639.9299, Loss Generator: -1.5585\n","Epoch [6/100], Loss Critic: -842.8292, Loss Generator: -2.6515\n","Epoch [7/100], Loss Critic: -1099.3115, Loss Generator: -3.7667\n","Epoch [8/100], Loss Critic: -1363.0565, Loss Generator: -5.8119\n","Epoch [9/100], Loss Critic: -1625.7433, Loss Generator: -7.9601\n","Epoch [10/100], Loss Critic: -1981.9419, Loss Generator: -10.3709\n","Epoch [11/100], Loss Critic: -2242.9861, Loss Generator: -14.2733\n","Epoch [12/100], Loss Critic: -2630.1753, Loss Generator: -18.6024\n","Epoch [13/100], Loss Critic: -2990.2808, Loss Generator: -23.1401\n","Epoch [14/100], Loss Critic: -3458.2817, Loss Generator: -29.0569\n","Epoch [15/100], Loss Critic: -3907.2100, Loss Generator: -36.1066\n","Epoch [16/100], Loss Critic: -4408.2520, Loss Generator: -42.2402\n","Epoch [17/100], Loss Critic: -4883.5054, Loss Generator: -52.0557\n","Epoch [18/100], Loss Critic: -5305.9951, Loss Generator: -62.5125\n","Epoch [19/100], Loss Critic: -5787.5957, Loss Generator: -72.7043\n","Epoch [20/100], Loss Critic: -6474.9541, Loss Generator: -86.7693\n","Epoch [21/100], Loss Critic: -6786.7563, Loss Generator: -100.8177\n","Epoch [22/100], Loss Critic: -7529.1206, Loss Generator: -117.9322\n","Epoch [23/100], Loss Critic: -7816.3560, Loss Generator: -134.3281\n","Epoch [24/100], Loss Critic: -8621.2930, Loss Generator: -150.2416\n","Epoch [25/100], Loss Critic: -8839.4688, Loss Generator: -163.7523\n","Epoch [26/100], Loss Critic: -9661.8750, Loss Generator: -189.6758\n","Epoch [27/100], Loss Critic: -10129.8164, Loss Generator: -214.6655\n","Epoch [28/100], Loss Critic: -10541.0332, Loss Generator: -243.4156\n","Epoch [29/100], Loss Critic: -10884.8496, Loss Generator: -266.5163\n","Epoch [30/100], Loss Critic: -11259.0918, Loss Generator: -291.9338\n","Epoch [31/100], Loss Critic: -11796.2695, Loss Generator: -324.2098\n","Epoch [32/100], Loss Critic: -12277.6074, Loss Generator: -352.7541\n","Epoch [33/100], Loss Critic: -12669.7285, Loss Generator: -388.0264\n","Epoch [34/100], Loss Critic: -12886.1738, Loss Generator: -412.8275\n","Epoch [35/100], Loss Critic: -12942.3604, Loss Generator: -447.4680\n","Epoch [36/100], Loss Critic: -13426.1768, Loss Generator: -490.4322\n","Epoch [37/100], Loss Critic: -13785.0107, Loss Generator: -517.8348\n","Epoch [38/100], Loss Critic: -13488.7354, Loss Generator: -535.9256\n","Epoch [39/100], Loss Critic: -13898.2295, Loss Generator: -582.4683\n","Epoch [40/100], Loss Critic: -13821.6084, Loss Generator: -595.3423\n","Epoch [41/100], Loss Critic: -14180.7422, Loss Generator: -600.1431\n","Epoch [42/100], Loss Critic: -13923.0635, Loss Generator: -675.3027\n","Epoch [43/100], Loss Critic: -14042.6689, Loss Generator: -674.8223\n","Epoch [44/100], Loss Critic: -13904.9961, Loss Generator: -704.1848\n","Epoch [45/100], Loss Critic: -14143.9307, Loss Generator: -716.6683\n","Epoch [46/100], Loss Critic: -13918.6357, Loss Generator: -733.3203\n","Epoch [47/100], Loss Critic: -14515.0762, Loss Generator: -751.6808\n","Epoch [48/100], Loss Critic: -13823.1162, Loss Generator: -787.0236\n","Epoch [49/100], Loss Critic: -13805.6611, Loss Generator: -785.5922\n","Epoch [50/100], Loss Critic: -14529.9736, Loss Generator: -803.0580\n","Epoch [51/100], Loss Critic: -14057.4912, Loss Generator: -813.0137\n","Epoch [52/100], Loss Critic: -13478.2861, Loss Generator: -873.7225\n","Epoch [53/100], Loss Critic: -14252.0459, Loss Generator: -864.6121\n","Epoch [54/100], Loss Critic: -14384.1475, Loss Generator: -888.3583\n","Epoch [55/100], Loss Critic: -13658.0303, Loss Generator: -888.3618\n","Epoch [56/100], Loss Critic: -14111.5732, Loss Generator: -954.2704\n","Epoch [57/100], Loss Critic: -13192.9512, Loss Generator: -951.7241\n","Epoch [58/100], Loss Critic: -14248.3408, Loss Generator: -990.7907\n","Epoch [59/100], Loss Critic: -14261.1416, Loss Generator: -994.4016\n","Epoch [60/100], Loss Critic: -13695.0322, Loss Generator: -1007.4692\n","Epoch [61/100], Loss Critic: -13339.2295, Loss Generator: -1075.6997\n","Epoch [62/100], Loss Critic: -14353.4971, Loss Generator: -1056.3048\n","Epoch [63/100], Loss Critic: -13639.1865, Loss Generator: -1057.2515\n","Epoch [64/100], Loss Critic: -13869.9736, Loss Generator: -1110.1820\n","Epoch [65/100], Loss Critic: -13710.0186, Loss Generator: -1089.7935\n","Epoch [66/100], Loss Critic: -13329.4951, Loss Generator: -1160.5338\n","Epoch [67/100], Loss Critic: -13493.6045, Loss Generator: -1153.8925\n","Epoch [68/100], Loss Critic: -13807.9248, Loss Generator: -1212.6708\n","Epoch [69/100], Loss Critic: -13743.5791, Loss Generator: -1218.1853\n","Epoch [70/100], Loss Critic: -13256.4541, Loss Generator: -1232.5366\n","Epoch [71/100], Loss Critic: -13895.4971, Loss Generator: -1271.5005\n","Epoch [72/100], Loss Critic: -13214.5381, Loss Generator: -1341.1431\n","Epoch [73/100], Loss Critic: -13177.5029, Loss Generator: -1324.0530\n","Epoch [74/100], Loss Critic: -13331.3291, Loss Generator: -1300.1085\n","Epoch [75/100], Loss Critic: -13049.7158, Loss Generator: -1318.7258\n","Epoch [76/100], Loss Critic: -13498.0898, Loss Generator: -1411.8969\n","Epoch [77/100], Loss Critic: -13553.8242, Loss Generator: -1372.0834\n","Epoch [78/100], Loss Critic: -13270.6807, Loss Generator: -1411.7437\n","Epoch [79/100], Loss Critic: -12738.2061, Loss Generator: -1445.9741\n","Epoch [80/100], Loss Critic: -14030.9033, Loss Generator: -1459.3911\n","Epoch [81/100], Loss Critic: -13025.6104, Loss Generator: -1458.3624\n","Epoch [82/100], Loss Critic: -13047.6533, Loss Generator: -1441.7360\n","Epoch [83/100], Loss Critic: -13195.7627, Loss Generator: -1467.2101\n","Epoch [84/100], Loss Critic: -13431.4229, Loss Generator: -1535.0615\n","Epoch [85/100], Loss Critic: -13079.2080, Loss Generator: -1581.2931\n","Epoch [86/100], Loss Critic: -13185.1250, Loss Generator: -1602.4088\n","Epoch [87/100], Loss Critic: -12182.2500, Loss Generator: -1628.9209\n","Epoch [88/100], Loss Critic: -12195.2109, Loss Generator: -1641.3435\n","Epoch [89/100], Loss Critic: -13128.1562, Loss Generator: -1679.1482\n","Epoch [90/100], Loss Critic: -12776.4580, Loss Generator: -1746.2128\n","Epoch [91/100], Loss Critic: -12825.5029, Loss Generator: -1657.3872\n","Epoch [92/100], Loss Critic: -12986.5420, Loss Generator: -1733.5175\n","Epoch [93/100], Loss Critic: -12515.3389, Loss Generator: -1784.6558\n","Epoch [94/100], Loss Critic: -12354.8252, Loss Generator: -1743.8429\n","Epoch [95/100], Loss Critic: -12605.9619, Loss Generator: -1822.7289\n","Epoch [96/100], Loss Critic: -13325.3838, Loss Generator: -1807.7296\n","Epoch [97/100], Loss Critic: -13085.3867, Loss Generator: -1858.5610\n","Epoch [98/100], Loss Critic: -12674.1865, Loss Generator: -1876.1156\n","Epoch [99/100], Loss Critic: -12988.3037, Loss Generator: -1943.0721\n","Epoch [100/100], Loss Critic: -12163.8135, Loss Generator: -1960.9037\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 569275.9241653184,\n","    \"FID (Original vs Eval)\": 321.0567368940282,\n","    \"KL Divergence\": 1.4126618770582793,\n","    \"Cosine Similarity\": 0.14028097689151764,\n","    \"Spearman Rank Correlation\": -0.008291633481467746,\n","    \"Wasserstein Distance\": 0.15631798223145868,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.21012372681708852\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_AdvancedAutoencoder_mse.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"all elements of input should be between 0 and 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-9f950e3f1317>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Run GAN training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ”„ Running GAN training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mrun_gan_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Evaluate GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-1c86f0ee2a21>\u001b[0m in \u001b[0;36mrun_gan_training\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgan_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"VAE-GAN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"discriminator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mgan_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Cycle-GAN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dual-GAN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Contrastive-Dual-GAN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generator_a\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generator_b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"discriminator_a\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"discriminator_b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/GAN-thesis-project/src/gan_workflows/plan2/plan2_gan_training.py\u001b[0m in \u001b[0;36mtrain_vae_gan\u001b[0;34m(encoder, generator, discriminator, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mfake_validity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_validity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_validity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                      \u001b[0mbce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_validity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_validity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         return F.binary_cross_entropy(\n\u001b[0m\u001b[1;32m    698\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3552\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3554\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"]}]},{"cell_type":"code","source":["# ==========================\n","# CONFIGURATION & EMBEDDING LOADING\n","# ==========================\n","\n","report_dir = \"./reports/GANs_Evaluations\"\n","os.makedirs(report_dir, exist_ok=True)\n","\n","# Configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","config = {\n","    \"gan_types\": ['WGAN-GP', 'VAE-GAN', 'Contrastive-GAN', 'Cross-Domain-GAN'],\n","    \"embedding_identifier\": embedding_identifier,  # Dynamically extracted identifier\n","    \"latent_dim\": 100,\n","    \"embedding_dim\": None,  # Will be set after loading embeddings\n","    \"num_classes\": 10,\n","    \"categorical_dim\": 10,\n","    \"epochs\": 100,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-4,\n","    \"device\": device,\n","    \"lambda_gp\": 10,\n","    \"beta1\": 0.5,\n","    \"beta2\": 0.999,\n","    \"save_path\": \"gan_model.pth\",\n","    \"eval_fraction\": 0.1,  # Fraction of embeddings used for evaluation\n","    \"kl_method\": \"histogram\",  # Can be \"histogram\" or \"kde\"\n","    \"show_model_architecture\": False\n","}\n","\n","# Base directory where embeddings are stored\n","embedding_base_dir = \"./saved_embeddings/embeddings/\"\n","\n","# List all autoencoder embeddings\n","autoencoder_embeddings = list_available_embeddings(embedding_base_dir, filter_by=\"vae\")\n","\n","# Loop through each embedding and run the GAN pipeline\n","for embedding_relative_path in autoencoder_embeddings:\n","    print(f\"\\nðŸš€ Processing embedding: {embedding_relative_path}\")\n","\n","    # Full path to the embedding file\n","    embedding_file = os.path.join(embedding_base_dir, embedding_relative_path)\n","\n","    # Extract identifier from the path (remove directory and `_embeddings.pt`)\n","    embedding_identifier = embedding_relative_path.split(\"/\")[-1].replace(\"_embeddings.pt\", \"\")\n","\n","    # Update config with the current embedding\n","    config.update({\n","        \"embedding_identifier\": embedding_identifier,\n","        \"embedding_file\": embedding_file\n","    })\n","\n","    # Load embeddings and split\n","    embeddings, labels, full_data_loader = load_embeddings(embedding_file, device)\n","    config[\"embedding_dim\"] = embeddings.size(1)\n","    train_loader, eval_loader, train_embeddings, eval_embeddings = split_embeddings(embeddings, labels, config[\"eval_fraction\"], config[\"batch_size\"])\n","\n","    # Update config with data loaders\n","    config.update({\n","        \"data_loader\": train_loader,\n","        \"data_loader_a\": train_loader,\n","        \"data_loader_b\": train_loader,\n","        \"eval_loader\": eval_loader,\n","        \"original_embeddings\": embeddings  # Store original embeddings for evaluation\n","    })\n","\n","    # Loop through each GAN type\n","    for gan_type in config[\"gan_types\"]:\n","        print(f\"\\nðŸš€ Running pipeline for GAN type: {gan_type}\")\n","\n","        # Update the GAN type in the config\n","        config[\"gan_type\"] = gan_type\n","\n","        # Initialize GAN components\n","        gan_components = initialize_gan_components(config, gan_configurations[config[\"gan_type\"]])\n","\n","        # Run GAN training\n","        print(\"ðŸ”„ Running GAN training...\")\n","        run_gan_training(config)\n","\n","        # Evaluate GAN\n","        print(\"ðŸ“Š Evaluating GAN...\")\n","        evaluate_gan(gan_components, config)\n","\n","        print(f\"âœ… Completed pipeline for GAN type: {gan_type}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfWEumyJ0jFw","executionInfo":{"status":"ok","timestamp":1738310280175,"user_tz":-210,"elapsed":227295,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"bfeddffd-22f8-4194-d039-198bda45d926"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸš€ Processing embedding: vae_BasicVAE_mse/BasicVAE_mse_embeddings.pt\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a3b46d8deeb0>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n","Epoch [1/100], Loss Critic: 6.0706, Loss Generator: -0.0817\n","Epoch [2/100], Loss Critic: 4.3931, Loss Generator: -0.2675\n","Epoch [3/100], Loss Critic: 3.1304, Loss Generator: -0.5523\n","Epoch [4/100], Loss Critic: 2.2276, Loss Generator: -0.9789\n","Epoch [5/100], Loss Critic: 1.8222, Loss Generator: -1.4033\n","Epoch [6/100], Loss Critic: 1.8292, Loss Generator: -1.6371\n","Epoch [7/100], Loss Critic: 2.1240, Loss Generator: -2.0117\n","Epoch [8/100], Loss Critic: 2.1363, Loss Generator: -2.0090\n","Epoch [9/100], Loss Critic: 2.1470, Loss Generator: -2.0071\n","Epoch [10/100], Loss Critic: 1.9713, Loss Generator: -1.8577\n","Epoch [11/100], Loss Critic: 1.9361, Loss Generator: -1.7511\n","Epoch [12/100], Loss Critic: 1.6118, Loss Generator: -1.4822\n","Epoch [13/100], Loss Critic: 1.3775, Loss Generator: -1.2311\n","Epoch [14/100], Loss Critic: 0.9190, Loss Generator: -0.7885\n","Epoch [15/100], Loss Critic: 0.4088, Loss Generator: -0.2661\n","Epoch [16/100], Loss Critic: -0.0657, Loss Generator: 0.2263\n","Epoch [17/100], Loss Critic: -0.4954, Loss Generator: 0.6916\n","Epoch [18/100], Loss Critic: -0.8327, Loss Generator: 1.0834\n","Epoch [19/100], Loss Critic: -1.2371, Loss Generator: 1.3817\n","Epoch [20/100], Loss Critic: -1.5023, Loss Generator: 1.6163\n","Epoch [21/100], Loss Critic: -1.6662, Loss Generator: 1.7934\n","Epoch [22/100], Loss Critic: -1.6074, Loss Generator: 1.7377\n","Epoch [23/100], Loss Critic: -1.6911, Loss Generator: 1.7865\n","Epoch [24/100], Loss Critic: -1.5723, Loss Generator: 1.6807\n","Epoch [25/100], Loss Critic: -1.4677, Loss Generator: 1.5748\n","Epoch [26/100], Loss Critic: -1.2862, Loss Generator: 1.3753\n","Epoch [27/100], Loss Critic: -1.1940, Loss Generator: 1.3416\n","Epoch [28/100], Loss Critic: -1.1221, Loss Generator: 1.2626\n","Epoch [29/100], Loss Critic: -0.8682, Loss Generator: 1.0129\n","Epoch [30/100], Loss Critic: -0.7301, Loss Generator: 0.8926\n","Epoch [31/100], Loss Critic: -0.6000, Loss Generator: 0.7836\n","Epoch [32/100], Loss Critic: -0.6433, Loss Generator: 0.8338\n","Epoch [33/100], Loss Critic: -0.4883, Loss Generator: 0.6708\n","Epoch [34/100], Loss Critic: -0.4592, Loss Generator: 0.6435\n","Epoch [35/100], Loss Critic: -0.3038, Loss Generator: 0.4941\n","Epoch [36/100], Loss Critic: -0.2380, Loss Generator: 0.4218\n","Epoch [37/100], Loss Critic: -0.0564, Loss Generator: 0.3576\n","Epoch [38/100], Loss Critic: -0.0014, Loss Generator: 0.2728\n","Epoch [39/100], Loss Critic: 0.0753, Loss Generator: 0.1987\n","Epoch [40/100], Loss Critic: 0.3882, Loss Generator: 0.0762\n","Epoch [41/100], Loss Critic: 0.7050, Loss Generator: -0.1126\n","Epoch [42/100], Loss Critic: 1.0384, Loss Generator: -0.3317\n","Epoch [43/100], Loss Critic: 0.9577, Loss Generator: -0.5144\n","Epoch [44/100], Loss Critic: 0.9467, Loss Generator: -0.6104\n","Epoch [45/100], Loss Critic: 0.8512, Loss Generator: -0.6902\n","Epoch [46/100], Loss Critic: 0.7790, Loss Generator: -0.7126\n","Epoch [47/100], Loss Critic: 0.8342, Loss Generator: -0.7569\n","Epoch [48/100], Loss Critic: 0.8184, Loss Generator: -0.8234\n","Epoch [49/100], Loss Critic: 0.8804, Loss Generator: -0.8838\n","Epoch [50/100], Loss Critic: 0.8700, Loss Generator: -0.8741\n","Epoch [51/100], Loss Critic: 0.8450, Loss Generator: -0.8985\n","Epoch [52/100], Loss Critic: 0.9262, Loss Generator: -0.9552\n","Epoch [53/100], Loss Critic: 0.9330, Loss Generator: -0.9300\n","Epoch [54/100], Loss Critic: 0.8118, Loss Generator: -0.8539\n","Epoch [55/100], Loss Critic: 0.7045, Loss Generator: -0.7485\n","Epoch [56/100], Loss Critic: 0.6451, Loss Generator: -0.6700\n","Epoch [57/100], Loss Critic: 0.4359, Loss Generator: -0.4364\n","Epoch [58/100], Loss Critic: 0.2108, Loss Generator: -0.1696\n","Epoch [59/100], Loss Critic: 0.0298, Loss Generator: 0.0890\n","Epoch [60/100], Loss Critic: -0.0823, Loss Generator: 0.4159\n","Epoch [61/100], Loss Critic: -0.2855, Loss Generator: 0.5723\n","Epoch [62/100], Loss Critic: -0.4153, Loss Generator: 0.7583\n","Epoch [63/100], Loss Critic: -0.6269, Loss Generator: 0.8682\n","Epoch [64/100], Loss Critic: -0.7759, Loss Generator: 0.9733\n","Epoch [65/100], Loss Critic: -0.8191, Loss Generator: 1.0214\n","Epoch [66/100], Loss Critic: -0.7592, Loss Generator: 0.9695\n","Epoch [67/100], Loss Critic: -0.7278, Loss Generator: 0.9407\n","Epoch [68/100], Loss Critic: -0.6834, Loss Generator: 0.9193\n","Epoch [69/100], Loss Critic: -0.6537, Loss Generator: 0.8568\n","Epoch [70/100], Loss Critic: -0.5733, Loss Generator: 0.7992\n","Epoch [71/100], Loss Critic: -0.5224, Loss Generator: 0.7670\n","Epoch [72/100], Loss Critic: -0.4920, Loss Generator: 0.7003\n","Epoch [73/100], Loss Critic: -0.3219, Loss Generator: 0.5548\n","Epoch [74/100], Loss Critic: -0.3207, Loss Generator: 0.5599\n","Epoch [75/100], Loss Critic: -0.2818, Loss Generator: 0.5276\n","Epoch [76/100], Loss Critic: -0.2559, Loss Generator: 0.4775\n","Epoch [77/100], Loss Critic: -0.1462, Loss Generator: 0.4098\n","Epoch [78/100], Loss Critic: -0.0882, Loss Generator: 0.3205\n","Epoch [79/100], Loss Critic: 0.0215, Loss Generator: 0.2460\n","Epoch [80/100], Loss Critic: 0.2296, Loss Generator: 0.1332\n","Epoch [81/100], Loss Critic: 0.3440, Loss Generator: 0.0482\n","Epoch [82/100], Loss Critic: 0.6983, Loss Generator: -0.0551\n","Epoch [83/100], Loss Critic: 0.6966, Loss Generator: -0.1820\n","Epoch [84/100], Loss Critic: 0.7085, Loss Generator: -0.3184\n","Epoch [85/100], Loss Critic: 0.5573, Loss Generator: -0.3914\n","Epoch [86/100], Loss Critic: 0.5266, Loss Generator: -0.4134\n","Epoch [87/100], Loss Critic: 0.5207, Loss Generator: -0.4589\n","Epoch [88/100], Loss Critic: 0.4788, Loss Generator: -0.4473\n","Epoch [89/100], Loss Critic: 0.5435, Loss Generator: -0.5391\n","Epoch [90/100], Loss Critic: 0.5967, Loss Generator: -0.5911\n","Epoch [91/100], Loss Critic: 0.6090, Loss Generator: -0.6122\n","Epoch [92/100], Loss Critic: 0.6626, Loss Generator: -0.6644\n","Epoch [93/100], Loss Critic: 0.7777, Loss Generator: -0.7477\n","Epoch [94/100], Loss Critic: 0.7968, Loss Generator: -0.8044\n","Epoch [95/100], Loss Critic: 0.8733, Loss Generator: -0.8684\n","Epoch [96/100], Loss Critic: 0.8606, Loss Generator: -0.9387\n","Epoch [97/100], Loss Critic: 0.8928, Loss Generator: -0.9523\n","Epoch [98/100], Loss Critic: 0.9187, Loss Generator: -0.9844\n","Epoch [99/100], Loss Critic: 0.8851, Loss Generator: -0.9899\n","Epoch [100/100], Loss Critic: 0.9152, Loss Generator: -1.0540\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.140759432847812,\n","    \"FID (Original vs Eval)\": -0.000248866827249462,\n","    \"KL Divergence\": 0.02143897425025708,\n","    \"Cosine Similarity\": 0.0018253264715895057,\n","    \"Spearman Rank Correlation\": 0.008133785229299301,\n","    \"Wasserstein Distance\": 0.03023198008683133,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.4243383780550818\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_BasicVAE_mse.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/100], D Loss: 1.3570, G Loss: 8.8734\n","Epoch [2/100], D Loss: 1.3170, G Loss: 6.4662\n","Epoch [3/100], D Loss: 1.2763, G Loss: 4.9340\n","Epoch [4/100], D Loss: 1.2267, G Loss: 3.8778\n","Epoch [5/100], D Loss: 1.1574, G Loss: 3.1547\n","Epoch [6/100], D Loss: 1.0507, G Loss: 2.7384\n","Epoch [7/100], D Loss: 0.9575, G Loss: 2.5369\n","Epoch [8/100], D Loss: 0.8648, G Loss: 2.3742\n","Epoch [9/100], D Loss: 0.7816, G Loss: 2.2911\n","Epoch [10/100], D Loss: 0.7163, G Loss: 2.1671\n","Epoch [11/100], D Loss: 0.6623, G Loss: 2.1389\n","Epoch [12/100], D Loss: 0.6380, G Loss: 2.1711\n","Epoch [13/100], D Loss: 0.6062, G Loss: 2.0873\n","Epoch [14/100], D Loss: 0.5704, G Loss: 2.1335\n","Epoch [15/100], D Loss: 0.5367, G Loss: 2.1656\n","Epoch [16/100], D Loss: 0.5219, G Loss: 2.2016\n","Epoch [17/100], D Loss: 0.4661, G Loss: 2.2743\n","Epoch [18/100], D Loss: 0.4808, G Loss: 2.2004\n","Epoch [19/100], D Loss: 0.4749, G Loss: 2.2481\n","Epoch [20/100], D Loss: 0.4376, G Loss: 2.2718\n","Epoch [21/100], D Loss: 0.3830, G Loss: 2.4232\n","Epoch [22/100], D Loss: 0.3931, G Loss: 2.4368\n","Epoch [23/100], D Loss: 0.3853, G Loss: 2.6273\n","Epoch [24/100], D Loss: 0.3801, G Loss: 2.5236\n","Epoch [25/100], D Loss: 0.3722, G Loss: 2.7139\n","Epoch [26/100], D Loss: 0.3240, G Loss: 2.7746\n","Epoch [27/100], D Loss: 0.4269, G Loss: 2.4157\n","Epoch [28/100], D Loss: 0.3335, G Loss: 2.5965\n","Epoch [29/100], D Loss: 0.3514, G Loss: 2.7521\n","Epoch [30/100], D Loss: 0.3581, G Loss: 2.6559\n","Epoch [31/100], D Loss: 0.3005, G Loss: 2.7069\n","Epoch [32/100], D Loss: 0.3824, G Loss: 3.0005\n","Epoch [33/100], D Loss: 0.2883, G Loss: 3.0784\n","Epoch [34/100], D Loss: 0.2969, G Loss: 2.8445\n","Epoch [35/100], D Loss: 0.3419, G Loss: 2.8392\n","Epoch [36/100], D Loss: 0.3205, G Loss: 3.0802\n","Epoch [37/100], D Loss: 0.4523, G Loss: 2.7835\n","Epoch [38/100], D Loss: 0.3969, G Loss: 2.8073\n","Epoch [39/100], D Loss: 0.4377, G Loss: 2.6894\n","Epoch [40/100], D Loss: 0.3350, G Loss: 3.0578\n","Epoch [41/100], D Loss: 0.3664, G Loss: 2.8401\n","Epoch [42/100], D Loss: 0.3298, G Loss: 2.9215\n","Epoch [43/100], D Loss: 0.4528, G Loss: 2.8854\n","Epoch [44/100], D Loss: 0.4451, G Loss: 2.7761\n","Epoch [45/100], D Loss: 0.4011, G Loss: 3.0053\n","Epoch [46/100], D Loss: 0.5065, G Loss: 2.6351\n","Epoch [47/100], D Loss: 0.4328, G Loss: 2.8117\n","Epoch [48/100], D Loss: 0.4707, G Loss: 2.7735\n","Epoch [49/100], D Loss: 0.5452, G Loss: 2.7604\n","Epoch [50/100], D Loss: 0.5343, G Loss: 2.8366\n","Epoch [51/100], D Loss: 0.6113, G Loss: 2.7742\n","Epoch [52/100], D Loss: 0.5897, G Loss: 2.4544\n","Epoch [53/100], D Loss: 0.5917, G Loss: 2.6527\n","Epoch [54/100], D Loss: 0.7106, G Loss: 2.2962\n","Epoch [55/100], D Loss: 0.6930, G Loss: 2.4017\n","Epoch [56/100], D Loss: 0.6148, G Loss: 2.3563\n","Epoch [57/100], D Loss: 0.6874, G Loss: 2.2613\n","Epoch [58/100], D Loss: 0.7779, G Loss: 2.1232\n","Epoch [59/100], D Loss: 0.7051, G Loss: 1.9330\n","Epoch [60/100], D Loss: 0.7486, G Loss: 2.0258\n","Epoch [61/100], D Loss: 0.7620, G Loss: 2.1309\n","Epoch [62/100], D Loss: 0.7893, G Loss: 1.7773\n","Epoch [63/100], D Loss: 1.0485, G Loss: 1.6410\n","Epoch [64/100], D Loss: 1.0368, G Loss: 1.7206\n","Epoch [65/100], D Loss: 1.0652, G Loss: 1.7426\n","Epoch [66/100], D Loss: 1.0061, G Loss: 1.7954\n","Epoch [67/100], D Loss: 0.9808, G Loss: 1.6557\n","Epoch [68/100], D Loss: 1.1118, G Loss: 1.5660\n","Epoch [69/100], D Loss: 1.2803, G Loss: 1.4259\n","Epoch [70/100], D Loss: 1.1869, G Loss: 1.4209\n","Epoch [71/100], D Loss: 1.1563, G Loss: 1.4646\n","Epoch [72/100], D Loss: 1.3592, G Loss: 1.1743\n","Epoch [73/100], D Loss: 1.2680, G Loss: 1.4242\n","Epoch [74/100], D Loss: 1.2826, G Loss: 1.3601\n","Epoch [75/100], D Loss: 1.3329, G Loss: 1.2330\n","Epoch [76/100], D Loss: 1.4202, G Loss: 1.1546\n","Epoch [77/100], D Loss: 1.3184, G Loss: 1.1636\n","Epoch [78/100], D Loss: 1.3143, G Loss: 1.0889\n","Epoch [79/100], D Loss: 1.3751, G Loss: 1.0753\n","Epoch [80/100], D Loss: 1.3160, G Loss: 1.0648\n","Epoch [81/100], D Loss: 1.2851, G Loss: 1.1291\n","Epoch [82/100], D Loss: 1.4445, G Loss: 1.0634\n","Epoch [83/100], D Loss: 1.3395, G Loss: 1.0353\n","Epoch [84/100], D Loss: 1.4129, G Loss: 1.0083\n","Epoch [85/100], D Loss: 1.3696, G Loss: 1.0229\n","Epoch [86/100], D Loss: 1.4601, G Loss: 0.9525\n","Epoch [87/100], D Loss: 1.4432, G Loss: 0.8148\n","Epoch [88/100], D Loss: 1.3788, G Loss: 0.9165\n","Epoch [89/100], D Loss: 1.4239, G Loss: 0.8668\n","Epoch [90/100], D Loss: 1.4756, G Loss: 0.8710\n","Epoch [91/100], D Loss: 1.5296, G Loss: 0.7910\n","Epoch [92/100], D Loss: 1.4377, G Loss: 0.8532\n","Epoch [93/100], D Loss: 1.5121, G Loss: 0.8067\n","Epoch [94/100], D Loss: 1.3828, G Loss: 0.8788\n","Epoch [95/100], D Loss: 1.4696, G Loss: 0.8138\n","Epoch [96/100], D Loss: 1.4224, G Loss: 0.8769\n","Epoch [97/100], D Loss: 1.6140, G Loss: 0.6649\n","Epoch [98/100], D Loss: 1.4964, G Loss: 0.7887\n","Epoch [99/100], D Loss: 1.5947, G Loss: 0.7428\n","Epoch [100/100], D Loss: 1.4904, G Loss: 0.7379\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.1375683956252765,\n","    \"FID (Original vs Eval)\": -0.000248866827249462,\n","    \"KL Divergence\": 0.1015138973613721,\n","    \"Cosine Similarity\": 0.00039600543095730245,\n","    \"Spearman Rank Correlation\": 0.0003887483152297979,\n","    \"Wasserstein Distance\": 0.0342695325990933,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.40917363137374674\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_BasicVAE_mse.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/100], D Loss: 1.3473, G Loss: 0.8178\n","Epoch [2/100], D Loss: 1.2934, G Loss: 0.8801\n","Epoch [3/100], D Loss: 1.2297, G Loss: 0.8590\n","Epoch [4/100], D Loss: 1.1973, G Loss: 0.9207\n","Epoch [5/100], D Loss: 1.1590, G Loss: 0.7013\n","Epoch [6/100], D Loss: 1.0642, G Loss: 0.8458\n","Epoch [7/100], D Loss: 0.9460, G Loss: 0.7944\n","Epoch [8/100], D Loss: 0.8813, G Loss: 1.3470\n","Epoch [9/100], D Loss: 0.8412, G Loss: 1.3858\n","Epoch [10/100], D Loss: 0.7930, G Loss: 1.4084\n","Epoch [11/100], D Loss: 0.7463, G Loss: 1.1397\n","Epoch [12/100], D Loss: 0.6797, G Loss: 1.5760\n","Epoch [13/100], D Loss: 0.6239, G Loss: 1.3970\n","Epoch [14/100], D Loss: 0.6215, G Loss: 1.3120\n","Epoch [15/100], D Loss: 0.5483, G Loss: 1.3621\n","Epoch [16/100], D Loss: 0.5078, G Loss: 1.6666\n","Epoch [17/100], D Loss: 0.4526, G Loss: 1.8411\n","Epoch [18/100], D Loss: 0.4064, G Loss: 1.7771\n","Epoch [19/100], D Loss: 0.3977, G Loss: 1.8022\n","Epoch [20/100], D Loss: 0.3926, G Loss: 1.9717\n","Epoch [21/100], D Loss: 0.3816, G Loss: 1.9549\n","Epoch [22/100], D Loss: 0.3359, G Loss: 2.3019\n","Epoch [23/100], D Loss: 0.2894, G Loss: 2.2583\n","Epoch [24/100], D Loss: 0.3490, G Loss: 2.0763\n","Epoch [25/100], D Loss: 0.2926, G Loss: 2.3296\n","Epoch [26/100], D Loss: 0.2890, G Loss: 2.4333\n","Epoch [27/100], D Loss: 0.2887, G Loss: 2.5510\n","Epoch [28/100], D Loss: 0.2845, G Loss: 2.2410\n","Epoch [29/100], D Loss: 0.2469, G Loss: 2.0879\n","Epoch [30/100], D Loss: 0.2797, G Loss: 2.2486\n","Epoch [31/100], D Loss: 0.2509, G Loss: 2.7403\n","Epoch [32/100], D Loss: 0.3053, G Loss: 2.4275\n","Epoch [33/100], D Loss: 0.2504, G Loss: 2.6699\n","Epoch [34/100], D Loss: 0.3667, G Loss: 2.4740\n","Epoch [35/100], D Loss: 0.3141, G Loss: 2.6609\n","Epoch [36/100], D Loss: 0.3244, G Loss: 2.6345\n","Epoch [37/100], D Loss: 0.3460, G Loss: 2.4361\n","Epoch [38/100], D Loss: 0.2934, G Loss: 2.9427\n","Epoch [39/100], D Loss: 0.2746, G Loss: 2.6829\n","Epoch [40/100], D Loss: 0.4115, G Loss: 2.3597\n","Epoch [41/100], D Loss: 0.3262, G Loss: 2.4973\n","Epoch [42/100], D Loss: 0.3555, G Loss: 2.8982\n","Epoch [43/100], D Loss: 0.3559, G Loss: 2.3934\n","Epoch [44/100], D Loss: 0.5021, G Loss: 2.6031\n","Epoch [45/100], D Loss: 0.4405, G Loss: 2.3919\n","Epoch [46/100], D Loss: 0.4267, G Loss: 2.9691\n","Epoch [47/100], D Loss: 0.5922, G Loss: 2.5760\n","Epoch [48/100], D Loss: 0.5854, G Loss: 2.5717\n","Epoch [49/100], D Loss: 0.5294, G Loss: 2.5675\n","Epoch [50/100], D Loss: 0.5475, G Loss: 2.1487\n","Epoch [51/100], D Loss: 0.7043, G Loss: 1.9578\n","Epoch [52/100], D Loss: 0.6820, G Loss: 2.2845\n","Epoch [53/100], D Loss: 0.6776, G Loss: 2.1274\n","Epoch [54/100], D Loss: 0.7143, G Loss: 1.7472\n","Epoch [55/100], D Loss: 0.6911, G Loss: 1.7169\n","Epoch [56/100], D Loss: 0.7826, G Loss: 1.8688\n","Epoch [57/100], D Loss: 0.9431, G Loss: 1.4787\n","Epoch [58/100], D Loss: 0.8167, G Loss: 1.9149\n","Epoch [59/100], D Loss: 1.1597, G Loss: 1.5830\n","Epoch [60/100], D Loss: 0.9732, G Loss: 1.9889\n","Epoch [61/100], D Loss: 1.1122, G Loss: 1.2807\n","Epoch [62/100], D Loss: 0.9993, G Loss: 1.6075\n","Epoch [63/100], D Loss: 1.0652, G Loss: 1.4757\n","Epoch [64/100], D Loss: 1.2875, G Loss: 1.2700\n","Epoch [65/100], D Loss: 1.1993, G Loss: 1.5440\n","Epoch [66/100], D Loss: 1.1704, G Loss: 1.3514\n","Epoch [67/100], D Loss: 1.2139, G Loss: 0.8517\n","Epoch [68/100], D Loss: 1.2237, G Loss: 1.1455\n","Epoch [69/100], D Loss: 1.3407, G Loss: 1.3551\n","Epoch [70/100], D Loss: 1.3500, G Loss: 1.4192\n","Epoch [71/100], D Loss: 1.4268, G Loss: 0.6664\n","Epoch [72/100], D Loss: 1.3502, G Loss: 1.0140\n","Epoch [73/100], D Loss: 1.3311, G Loss: 0.8470\n","Epoch [74/100], D Loss: 1.3998, G Loss: 0.9186\n","Epoch [75/100], D Loss: 1.4758, G Loss: 0.6121\n","Epoch [76/100], D Loss: 1.3272, G Loss: 1.0118\n","Epoch [77/100], D Loss: 1.3837, G Loss: 0.4980\n","Epoch [78/100], D Loss: 1.4651, G Loss: 0.8028\n","Epoch [79/100], D Loss: 1.5338, G Loss: 0.7019\n","Epoch [80/100], D Loss: 1.5192, G Loss: 0.7142\n","Epoch [81/100], D Loss: 1.5684, G Loss: 0.6577\n","Epoch [82/100], D Loss: 1.4092, G Loss: 1.0281\n","Epoch [83/100], D Loss: 1.4324, G Loss: 0.4939\n","Epoch [84/100], D Loss: 1.5410, G Loss: 0.4921\n","Epoch [85/100], D Loss: 1.5390, G Loss: 0.9078\n","Epoch [86/100], D Loss: 1.5085, G Loss: 1.3485\n","Epoch [87/100], D Loss: 1.5898, G Loss: 0.9815\n","Epoch [88/100], D Loss: 1.4966, G Loss: 0.5201\n","Epoch [89/100], D Loss: 1.4873, G Loss: 1.0092\n","Epoch [90/100], D Loss: 1.6233, G Loss: 0.3867\n","Epoch [91/100], D Loss: 1.4729, G Loss: 1.0429\n","Epoch [92/100], D Loss: 1.4711, G Loss: 0.7707\n","Epoch [93/100], D Loss: 1.5562, G Loss: 0.4123\n","Epoch [94/100], D Loss: 1.5265, G Loss: 0.5622\n","Epoch [95/100], D Loss: 1.4607, G Loss: 0.6306\n","Epoch [96/100], D Loss: 1.4891, G Loss: 1.2338\n","Epoch [97/100], D Loss: 1.4631, G Loss: 0.8005\n","Epoch [98/100], D Loss: 1.4461, G Loss: 0.5149\n","Epoch [99/100], D Loss: 1.4218, G Loss: 0.8607\n","Epoch [100/100], D Loss: 1.5368, G Loss: 0.7732\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.25874080972617863,\n","    \"FID (Original vs Eval)\": -0.000248866827249462,\n","    \"KL Divergence\": 0.034564014457700416,\n","    \"Cosine Similarity\": -3.9655504224356264e-05,\n","    \"Spearman Rank Correlation\": -0.002234832082747691,\n","    \"Wasserstein Distance\": 0.023192471127142053,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.6108646255385543\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_BasicVAE_mse.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/100], D Loss: 1.3822, G Loss: 0.6831\n","Epoch [2/100], D Loss: 1.3901, G Loss: 0.6811\n","Epoch [3/100], D Loss: 1.3865, G Loss: 0.6820\n","Epoch [4/100], D Loss: 1.3820, G Loss: 0.6867\n","Epoch [5/100], D Loss: 1.3870, G Loss: 0.6844\n","Epoch [6/100], D Loss: 1.3890, G Loss: 0.6845\n","Epoch [7/100], D Loss: 1.3842, G Loss: 0.6934\n","Epoch [8/100], D Loss: 1.3853, G Loss: 0.6915\n","Epoch [9/100], D Loss: 1.3902, G Loss: 0.6857\n","Epoch [10/100], D Loss: 1.3860, G Loss: 0.6927\n","Epoch [11/100], D Loss: 1.3890, G Loss: 0.6917\n","Epoch [12/100], D Loss: 1.3895, G Loss: 0.6904\n","Epoch [13/100], D Loss: 1.3883, G Loss: 0.6914\n","Epoch [14/100], D Loss: 1.3877, G Loss: 0.6918\n","Epoch [15/100], D Loss: 1.3895, G Loss: 0.6936\n","Epoch [16/100], D Loss: 1.3874, G Loss: 0.6932\n","Epoch [17/100], D Loss: 1.3878, G Loss: 0.6928\n","Epoch [18/100], D Loss: 1.3879, G Loss: 0.6947\n","Epoch [19/100], D Loss: 1.3884, G Loss: 0.6912\n","Epoch [20/100], D Loss: 1.3864, G Loss: 0.6939\n","Epoch [21/100], D Loss: 1.3869, G Loss: 0.6957\n","Epoch [22/100], D Loss: 1.3881, G Loss: 0.6952\n","Epoch [23/100], D Loss: 1.3903, G Loss: 0.6913\n","Epoch [24/100], D Loss: 1.3845, G Loss: 0.6977\n","Epoch [25/100], D Loss: 1.3870, G Loss: 0.6984\n","Epoch [26/100], D Loss: 1.3924, G Loss: 0.6928\n","Epoch [27/100], D Loss: 1.3861, G Loss: 0.6947\n","Epoch [28/100], D Loss: 1.3843, G Loss: 0.6965\n","Epoch [29/100], D Loss: 1.3904, G Loss: 0.6940\n","Epoch [30/100], D Loss: 1.3908, G Loss: 0.6937\n","Epoch [31/100], D Loss: 1.3867, G Loss: 0.6984\n","Epoch [32/100], D Loss: 1.3863, G Loss: 0.6971\n","Epoch [33/100], D Loss: 1.3877, G Loss: 0.6925\n","Epoch [34/100], D Loss: 1.3913, G Loss: 0.6911\n","Epoch [35/100], D Loss: 1.3878, G Loss: 0.6994\n","Epoch [36/100], D Loss: 1.3853, G Loss: 0.6998\n","Epoch [37/100], D Loss: 1.3907, G Loss: 0.6900\n","Epoch [38/100], D Loss: 1.3912, G Loss: 0.6878\n","Epoch [39/100], D Loss: 1.3854, G Loss: 0.6932\n","Epoch [40/100], D Loss: 1.3872, G Loss: 0.6972\n","Epoch [41/100], D Loss: 1.3874, G Loss: 0.7021\n","Epoch [42/100], D Loss: 1.3866, G Loss: 0.7014\n","Epoch [43/100], D Loss: 1.3914, G Loss: 0.6889\n","Epoch [44/100], D Loss: 1.3889, G Loss: 0.6885\n","Epoch [45/100], D Loss: 1.3813, G Loss: 0.6957\n","Epoch [46/100], D Loss: 1.3911, G Loss: 0.6883\n","Epoch [47/100], D Loss: 1.3908, G Loss: 0.6934\n","Epoch [48/100], D Loss: 1.3847, G Loss: 0.7003\n","Epoch [49/100], D Loss: 1.3866, G Loss: 0.6948\n","Epoch [50/100], D Loss: 1.3920, G Loss: 0.6911\n","Epoch [51/100], D Loss: 1.3835, G Loss: 0.7004\n","Epoch [52/100], D Loss: 1.3896, G Loss: 0.6909\n","Epoch [53/100], D Loss: 1.3908, G Loss: 0.6904\n","Epoch [54/100], D Loss: 1.3854, G Loss: 0.6978\n","Epoch [55/100], D Loss: 1.3845, G Loss: 0.6987\n","Epoch [56/100], D Loss: 1.3940, G Loss: 0.6896\n","Epoch [57/100], D Loss: 1.3894, G Loss: 0.6970\n","Epoch [58/100], D Loss: 1.3828, G Loss: 0.7002\n","Epoch [59/100], D Loss: 1.3923, G Loss: 0.6875\n","Epoch [60/100], D Loss: 1.3880, G Loss: 0.6924\n","Epoch [61/100], D Loss: 1.3892, G Loss: 0.6953\n","Epoch [62/100], D Loss: 1.3849, G Loss: 0.7011\n","Epoch [63/100], D Loss: 1.3864, G Loss: 0.6980\n","Epoch [64/100], D Loss: 1.3940, G Loss: 0.6902\n","Epoch [65/100], D Loss: 1.3847, G Loss: 0.6990\n","Epoch [66/100], D Loss: 1.3868, G Loss: 0.6930\n","Epoch [67/100], D Loss: 1.3870, G Loss: 0.6890\n","Epoch [68/100], D Loss: 1.3890, G Loss: 0.6896\n","Epoch [69/100], D Loss: 1.3850, G Loss: 0.6978\n","Epoch [70/100], D Loss: 1.3926, G Loss: 0.6934\n","Epoch [71/100], D Loss: 1.3836, G Loss: 0.6992\n","Epoch [72/100], D Loss: 1.3899, G Loss: 0.6911\n","Epoch [73/100], D Loss: 1.3874, G Loss: 0.6932\n","Epoch [74/100], D Loss: 1.3843, G Loss: 0.6970\n","Epoch [75/100], D Loss: 1.3887, G Loss: 0.6929\n","Epoch [76/100], D Loss: 1.3909, G Loss: 0.6922\n","Epoch [77/100], D Loss: 1.3853, G Loss: 0.7009\n","Epoch [78/100], D Loss: 1.3846, G Loss: 0.6992\n","Epoch [79/100], D Loss: 1.3882, G Loss: 0.6900\n","Epoch [80/100], D Loss: 1.3882, G Loss: 0.6909\n","Epoch [81/100], D Loss: 1.3882, G Loss: 0.6939\n","Epoch [82/100], D Loss: 1.3892, G Loss: 0.6944\n","Epoch [83/100], D Loss: 1.3865, G Loss: 0.6989\n","Epoch [84/100], D Loss: 1.3849, G Loss: 0.6978\n","Epoch [85/100], D Loss: 1.3914, G Loss: 0.6898\n","Epoch [86/100], D Loss: 1.3884, G Loss: 0.6917\n","Epoch [87/100], D Loss: 1.3869, G Loss: 0.6931\n","Epoch [88/100], D Loss: 1.3868, G Loss: 0.6937\n","Epoch [89/100], D Loss: 1.3885, G Loss: 0.6932\n","Epoch [90/100], D Loss: 1.3845, G Loss: 0.6958\n","Epoch [91/100], D Loss: 1.3885, G Loss: 0.6923\n","Epoch [92/100], D Loss: 1.3897, G Loss: 0.6919\n","Epoch [93/100], D Loss: 1.3850, G Loss: 0.6977\n","Epoch [94/100], D Loss: 1.3846, G Loss: 0.6991\n","Epoch [95/100], D Loss: 1.3876, G Loss: 0.6936\n","Epoch [96/100], D Loss: 1.3935, G Loss: 0.6891\n","Epoch [97/100], D Loss: 1.3826, G Loss: 0.6987\n","Epoch [98/100], D Loss: 1.3840, G Loss: 0.6955\n","Epoch [99/100], D Loss: 1.3913, G Loss: 0.6882\n","Epoch [100/100], D Loss: 1.3882, G Loss: 0.6933\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.3155493450173351,\n","    \"FID (Original vs Eval)\": -0.000248866827249462,\n","    \"KL Divergence\": 0.0876550463167096,\n","    \"Cosine Similarity\": -0.0008131973445415497,\n","    \"Spearman Rank Correlation\": 0.001636258266416287,\n","    \"Wasserstein Distance\": 0.012978850113863285,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.5879383055124079\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_BasicVAE_mse.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n","\n","ðŸš€ Processing embedding: vae_FlexibleVAE_mse/FlexibleVAE_mse_embeddings.pt\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a3b46d8deeb0>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n","Epoch [1/100], Loss Critic: 6.1957, Loss Generator: -0.0264\n","Epoch [2/100], Loss Critic: 4.4998, Loss Generator: -0.1535\n","Epoch [3/100], Loss Critic: 3.2681, Loss Generator: -0.4068\n","Epoch [4/100], Loss Critic: 2.3963, Loss Generator: -0.8001\n","Epoch [5/100], Loss Critic: 1.9303, Loss Generator: -1.2335\n","Epoch [6/100], Loss Critic: 1.9383, Loss Generator: -1.5336\n","Epoch [7/100], Loss Critic: 2.0846, Loss Generator: -1.7862\n","Epoch [8/100], Loss Critic: 2.1941, Loss Generator: -1.9137\n","Epoch [9/100], Loss Critic: 2.1647, Loss Generator: -1.8367\n","Epoch [10/100], Loss Critic: 2.1126, Loss Generator: -1.7505\n","Epoch [11/100], Loss Critic: 2.0124, Loss Generator: -1.6491\n","Epoch [12/100], Loss Critic: 1.6782, Loss Generator: -1.3703\n","Epoch [13/100], Loss Critic: 1.5018, Loss Generator: -1.1898\n","Epoch [14/100], Loss Critic: 1.1588, Loss Generator: -0.8881\n","Epoch [15/100], Loss Critic: 0.7634, Loss Generator: -0.4966\n","Epoch [16/100], Loss Critic: 0.2328, Loss Generator: -0.0270\n","Epoch [17/100], Loss Critic: -0.1744, Loss Generator: 0.4449\n","Epoch [18/100], Loss Critic: -0.5222, Loss Generator: 0.7691\n","Epoch [19/100], Loss Critic: -0.8180, Loss Generator: 1.0034\n","Epoch [20/100], Loss Critic: -1.2186, Loss Generator: 1.3794\n","Epoch [21/100], Loss Critic: -1.4270, Loss Generator: 1.5613\n","Epoch [22/100], Loss Critic: -1.4759, Loss Generator: 1.6150\n","Epoch [23/100], Loss Critic: -1.5301, Loss Generator: 1.6551\n","Epoch [24/100], Loss Critic: -1.5227, Loss Generator: 1.6269\n","Epoch [25/100], Loss Critic: -1.5772, Loss Generator: 1.6727\n","Epoch [26/100], Loss Critic: -1.5057, Loss Generator: 1.5762\n","Epoch [27/100], Loss Critic: -1.4387, Loss Generator: 1.5004\n","Epoch [28/100], Loss Critic: -1.4336, Loss Generator: 1.4810\n","Epoch [29/100], Loss Critic: -1.2476, Loss Generator: 1.2688\n","Epoch [30/100], Loss Critic: -1.1769, Loss Generator: 1.1730\n","Epoch [31/100], Loss Critic: -1.0070, Loss Generator: 1.0084\n","Epoch [32/100], Loss Critic: -0.9057, Loss Generator: 0.8975\n","Epoch [33/100], Loss Critic: -0.7450, Loss Generator: 0.7524\n","Epoch [34/100], Loss Critic: -0.6957, Loss Generator: 0.7170\n","Epoch [35/100], Loss Critic: -0.6482, Loss Generator: 0.6628\n","Epoch [36/100], Loss Critic: -0.5379, Loss Generator: 0.5671\n","Epoch [37/100], Loss Critic: -0.4419, Loss Generator: 0.5127\n","Epoch [38/100], Loss Critic: -0.3289, Loss Generator: 0.4298\n","Epoch [39/100], Loss Critic: -0.2145, Loss Generator: 0.3708\n","Epoch [40/100], Loss Critic: -0.2494, Loss Generator: 0.3709\n","Epoch [41/100], Loss Critic: -0.0582, Loss Generator: 0.2935\n","Epoch [42/100], Loss Critic: -0.0431, Loss Generator: 0.2720\n","Epoch [43/100], Loss Critic: -0.0363, Loss Generator: 0.2922\n","Epoch [44/100], Loss Critic: -0.1485, Loss Generator: 0.2879\n","Epoch [45/100], Loss Critic: -0.2973, Loss Generator: 0.4107\n","Epoch [46/100], Loss Critic: -0.3569, Loss Generator: 0.4153\n","Epoch [47/100], Loss Critic: -0.3878, Loss Generator: 0.4632\n","Epoch [48/100], Loss Critic: -0.4047, Loss Generator: 0.4845\n","Epoch [49/100], Loss Critic: -0.4790, Loss Generator: 0.5276\n","Epoch [50/100], Loss Critic: -0.5007, Loss Generator: 0.5542\n","Epoch [51/100], Loss Critic: -0.4772, Loss Generator: 0.5433\n","Epoch [52/100], Loss Critic: -0.4854, Loss Generator: 0.5079\n","Epoch [53/100], Loss Critic: -0.5003, Loss Generator: 0.4941\n","Epoch [54/100], Loss Critic: -0.4594, Loss Generator: 0.4577\n","Epoch [55/100], Loss Critic: -0.4422, Loss Generator: 0.4295\n","Epoch [56/100], Loss Critic: -0.4202, Loss Generator: 0.4142\n","Epoch [57/100], Loss Critic: -0.4244, Loss Generator: 0.3924\n","Epoch [58/100], Loss Critic: -0.4208, Loss Generator: 0.3878\n","Epoch [59/100], Loss Critic: -0.4160, Loss Generator: 0.3756\n","Epoch [60/100], Loss Critic: -0.4094, Loss Generator: 0.3559\n","Epoch [61/100], Loss Critic: -0.3867, Loss Generator: 0.3404\n","Epoch [62/100], Loss Critic: -0.4055, Loss Generator: 0.3411\n","Epoch [63/100], Loss Critic: -0.3891, Loss Generator: 0.3203\n","Epoch [64/100], Loss Critic: -0.3722, Loss Generator: 0.3164\n","Epoch [65/100], Loss Critic: -0.3767, Loss Generator: 0.3109\n","Epoch [66/100], Loss Critic: -0.3652, Loss Generator: 0.2893\n","Epoch [67/100], Loss Critic: -0.3709, Loss Generator: 0.3051\n","Epoch [68/100], Loss Critic: -0.3630, Loss Generator: 0.2838\n","Epoch [69/100], Loss Critic: -0.3598, Loss Generator: 0.2926\n","Epoch [70/100], Loss Critic: -0.3481, Loss Generator: 0.2676\n","Epoch [71/100], Loss Critic: -0.3631, Loss Generator: 0.2760\n","Epoch [72/100], Loss Critic: -0.3478, Loss Generator: 0.2656\n","Epoch [73/100], Loss Critic: -0.3535, Loss Generator: 0.2593\n","Epoch [74/100], Loss Critic: -0.3246, Loss Generator: 0.2474\n","Epoch [75/100], Loss Critic: -0.3535, Loss Generator: 0.2584\n","Epoch [76/100], Loss Critic: -0.3299, Loss Generator: 0.2317\n","Epoch [77/100], Loss Critic: -0.3362, Loss Generator: 0.2434\n","Epoch [78/100], Loss Critic: -0.3421, Loss Generator: 0.2425\n","Epoch [79/100], Loss Critic: -0.3380, Loss Generator: 0.2363\n","Epoch [80/100], Loss Critic: -0.3252, Loss Generator: 0.2274\n","Epoch [81/100], Loss Critic: -0.3374, Loss Generator: 0.2333\n","Epoch [82/100], Loss Critic: -0.3233, Loss Generator: 0.2279\n","Epoch [83/100], Loss Critic: -0.3240, Loss Generator: 0.2120\n","Epoch [84/100], Loss Critic: -0.3273, Loss Generator: 0.2255\n","Epoch [85/100], Loss Critic: -0.3097, Loss Generator: 0.1982\n","Epoch [86/100], Loss Critic: -0.3307, Loss Generator: 0.2223\n","Epoch [87/100], Loss Critic: -0.3246, Loss Generator: 0.2157\n","Epoch [88/100], Loss Critic: -0.3258, Loss Generator: 0.2119\n","Epoch [89/100], Loss Critic: -0.3233, Loss Generator: 0.2101\n","Epoch [90/100], Loss Critic: -0.3085, Loss Generator: 0.2029\n","Epoch [91/100], Loss Critic: -0.3053, Loss Generator: 0.1945\n","Epoch [92/100], Loss Critic: -0.3111, Loss Generator: 0.1991\n","Epoch [93/100], Loss Critic: -0.3057, Loss Generator: 0.1910\n","Epoch [94/100], Loss Critic: -0.3204, Loss Generator: 0.2034\n","Epoch [95/100], Loss Critic: -0.3012, Loss Generator: 0.1875\n","Epoch [96/100], Loss Critic: -0.3072, Loss Generator: 0.1902\n","Epoch [97/100], Loss Critic: -0.3145, Loss Generator: 0.1982\n","Epoch [98/100], Loss Critic: -0.2925, Loss Generator: 0.1757\n","Epoch [99/100], Loss Critic: -0.3033, Loss Generator: 0.1833\n","Epoch [100/100], Loss Critic: -0.2958, Loss Generator: 0.1768\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.7240534327665245,\n","    \"FID (Original vs Eval)\": -0.09967537106276378,\n","    \"KL Divergence\": 0.062440850004716844,\n","    \"Cosine Similarity\": 7.217331130959792e-06,\n","    \"Spearman Rank Correlation\": 0.004998977898788714,\n","    \"Wasserstein Distance\": 0.018380609511241732,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.3961569595928135\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_FlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/100], D Loss: 1.3504, G Loss: 7.9388\n","Epoch [2/100], D Loss: 1.3127, G Loss: 5.6097\n","Epoch [3/100], D Loss: 1.2700, G Loss: 4.0850\n","Epoch [4/100], D Loss: 1.1988, G Loss: 3.1298\n","Epoch [5/100], D Loss: 1.1098, G Loss: 2.5319\n","Epoch [6/100], D Loss: 0.9946, G Loss: 2.2056\n","Epoch [7/100], D Loss: 0.9077, G Loss: 1.9524\n","Epoch [8/100], D Loss: 0.8082, G Loss: 1.8386\n","Epoch [9/100], D Loss: 0.7029, G Loss: 1.8103\n","Epoch [10/100], D Loss: 0.6173, G Loss: 1.8099\n","Epoch [11/100], D Loss: 0.5418, G Loss: 1.8254\n","Epoch [12/100], D Loss: 0.4646, G Loss: 1.9408\n","Epoch [13/100], D Loss: 0.4388, G Loss: 1.8861\n","Epoch [14/100], D Loss: 0.3832, G Loss: 1.9439\n","Epoch [15/100], D Loss: 0.3513, G Loss: 2.0374\n","Epoch [16/100], D Loss: 0.3102, G Loss: 2.1286\n","Epoch [17/100], D Loss: 0.2761, G Loss: 2.2432\n","Epoch [18/100], D Loss: 0.2295, G Loss: 2.4714\n","Epoch [19/100], D Loss: 0.2147, G Loss: 2.5658\n","Epoch [20/100], D Loss: 0.1880, G Loss: 2.6668\n","Epoch [21/100], D Loss: 0.1724, G Loss: 2.7533\n","Epoch [22/100], D Loss: 0.1470, G Loss: 2.9343\n","Epoch [23/100], D Loss: 0.1211, G Loss: 3.2556\n","Epoch [24/100], D Loss: 0.1253, G Loss: 3.1454\n","Epoch [25/100], D Loss: 0.1178, G Loss: 3.1599\n","Epoch [26/100], D Loss: 0.0998, G Loss: 3.4802\n","Epoch [27/100], D Loss: 0.0942, G Loss: 3.5019\n","Epoch [28/100], D Loss: 0.0813, G Loss: 3.6098\n","Epoch [29/100], D Loss: 0.0775, G Loss: 3.7446\n","Epoch [30/100], D Loss: 0.0685, G Loss: 3.7912\n","Epoch [31/100], D Loss: 0.0628, G Loss: 3.9495\n","Epoch [32/100], D Loss: 0.0572, G Loss: 4.0115\n","Epoch [33/100], D Loss: 0.0502, G Loss: 4.1733\n","Epoch [34/100], D Loss: 0.0514, G Loss: 4.2637\n","Epoch [35/100], D Loss: 0.0459, G Loss: 4.3460\n","Epoch [36/100], D Loss: 0.0515, G Loss: 4.1798\n","Epoch [37/100], D Loss: 0.0413, G Loss: 4.3750\n","Epoch [38/100], D Loss: 0.0391, G Loss: 4.3475\n","Epoch [39/100], D Loss: 0.0502, G Loss: 4.2526\n","Epoch [40/100], D Loss: 0.0464, G Loss: 4.3440\n","Epoch [41/100], D Loss: 0.0325, G Loss: 4.6770\n","Epoch [42/100], D Loss: 0.0351, G Loss: 4.8056\n","Epoch [43/100], D Loss: 0.0383, G Loss: 4.5350\n","Epoch [44/100], D Loss: 0.0324, G Loss: 4.7471\n","Epoch [45/100], D Loss: 0.0288, G Loss: 4.8474\n","Epoch [46/100], D Loss: 0.0277, G Loss: 4.9295\n","Epoch [47/100], D Loss: 0.0310, G Loss: 4.8479\n","Epoch [48/100], D Loss: 0.0314, G Loss: 4.7401\n","Epoch [49/100], D Loss: 0.0304, G Loss: 4.7191\n","Epoch [50/100], D Loss: 0.0298, G Loss: 4.8998\n","Epoch [51/100], D Loss: 0.0298, G Loss: 5.0432\n","Epoch [52/100], D Loss: 0.0258, G Loss: 5.0203\n","Epoch [53/100], D Loss: 0.0251, G Loss: 5.0492\n","Epoch [54/100], D Loss: 0.0278, G Loss: 5.0093\n","Epoch [55/100], D Loss: 0.0241, G Loss: 5.0671\n","Epoch [56/100], D Loss: 0.0194, G Loss: 5.1735\n","Epoch [57/100], D Loss: 0.0251, G Loss: 5.0887\n","Epoch [58/100], D Loss: 0.0223, G Loss: 5.0494\n","Epoch [59/100], D Loss: 0.0255, G Loss: 4.9932\n","Epoch [60/100], D Loss: 0.0246, G Loss: 5.1401\n","Epoch [61/100], D Loss: 0.0222, G Loss: 5.1444\n","Epoch [62/100], D Loss: 0.0207, G Loss: 5.2817\n","Epoch [63/100], D Loss: 0.0249, G Loss: 5.0732\n","Epoch [64/100], D Loss: 0.0209, G Loss: 5.2679\n","Epoch [65/100], D Loss: 0.0242, G Loss: 5.1028\n","Epoch [66/100], D Loss: 0.0298, G Loss: 5.1511\n","Epoch [67/100], D Loss: 0.0183, G Loss: 5.3888\n","Epoch [68/100], D Loss: 0.0222, G Loss: 5.2711\n","Epoch [69/100], D Loss: 0.0198, G Loss: 5.2430\n","Epoch [70/100], D Loss: 0.0207, G Loss: 5.3447\n","Epoch [71/100], D Loss: 0.0181, G Loss: 5.3017\n","Epoch [72/100], D Loss: 0.0209, G Loss: 5.1229\n","Epoch [73/100], D Loss: 0.0193, G Loss: 5.5127\n","Epoch [74/100], D Loss: 0.0186, G Loss: 5.2560\n","Epoch [75/100], D Loss: 0.0198, G Loss: 5.5085\n","Epoch [76/100], D Loss: 0.0200, G Loss: 5.4056\n","Epoch [77/100], D Loss: 0.0200, G Loss: 5.5068\n","Epoch [78/100], D Loss: 0.0223, G Loss: 5.1752\n","Epoch [79/100], D Loss: 0.0242, G Loss: 5.3636\n","Epoch [80/100], D Loss: 0.0193, G Loss: 5.4374\n","Epoch [81/100], D Loss: 0.0204, G Loss: 5.5145\n","Epoch [82/100], D Loss: 0.0182, G Loss: 5.3875\n","Epoch [83/100], D Loss: 0.0202, G Loss: 5.2778\n","Epoch [84/100], D Loss: 0.0192, G Loss: 5.3629\n","Epoch [85/100], D Loss: 0.0251, G Loss: 5.3106\n","Epoch [86/100], D Loss: 0.0218, G Loss: 5.1529\n","Epoch [87/100], D Loss: 0.0175, G Loss: 5.5009\n","Epoch [88/100], D Loss: 0.0240, G Loss: 5.3006\n","Epoch [89/100], D Loss: 0.0240, G Loss: 5.2447\n","Epoch [90/100], D Loss: 0.0226, G Loss: 5.3533\n","Epoch [91/100], D Loss: 0.0203, G Loss: 5.2793\n","Epoch [92/100], D Loss: 0.0255, G Loss: 5.0461\n","Epoch [93/100], D Loss: 0.0214, G Loss: 5.0702\n","Epoch [94/100], D Loss: 0.0230, G Loss: 5.3202\n","Epoch [95/100], D Loss: 0.0212, G Loss: 5.3228\n","Epoch [96/100], D Loss: 0.0243, G Loss: 5.0651\n","Epoch [97/100], D Loss: 0.0282, G Loss: 5.0945\n","Epoch [98/100], D Loss: 0.0248, G Loss: 5.1433\n","Epoch [99/100], D Loss: 0.0266, G Loss: 4.9689\n","Epoch [100/100], D Loss: 0.0279, G Loss: 5.1185\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.7517542597327296,\n","    \"FID (Original vs Eval)\": -0.09967537106276378,\n","    \"KL Divergence\": 0.0312878401684589,\n","    \"Cosine Similarity\": 1.4838344213785604e-05,\n","    \"Spearman Rank Correlation\": 0.005626610666552381,\n","    \"Wasserstein Distance\": 0.020310887927551834,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.40111469767659425\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_FlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/100], D Loss: 1.3508, G Loss: 0.7641\n","Epoch [2/100], D Loss: 1.3208, G Loss: 0.7985\n","Epoch [3/100], D Loss: 1.2973, G Loss: 0.8070\n","Epoch [4/100], D Loss: 1.2532, G Loss: 0.8536\n","Epoch [5/100], D Loss: 1.2059, G Loss: 0.8918\n","Epoch [6/100], D Loss: 1.1634, G Loss: 0.9077\n","Epoch [7/100], D Loss: 1.0934, G Loss: 0.9946\n","Epoch [8/100], D Loss: 1.0182, G Loss: 1.0592\n","Epoch [9/100], D Loss: 0.9408, G Loss: 1.1290\n","Epoch [10/100], D Loss: 0.8520, G Loss: 1.2146\n","Epoch [11/100], D Loss: 0.7333, G Loss: 1.3821\n","Epoch [12/100], D Loss: 0.6391, G Loss: 1.5119\n","Epoch [13/100], D Loss: 0.5558, G Loss: 1.6196\n","Epoch [14/100], D Loss: 0.4906, G Loss: 1.7014\n","Epoch [15/100], D Loss: 0.4268, G Loss: 1.8452\n","Epoch [16/100], D Loss: 0.3552, G Loss: 2.0507\n","Epoch [17/100], D Loss: 0.2980, G Loss: 2.2582\n","Epoch [18/100], D Loss: 0.2511, G Loss: 2.3932\n","Epoch [19/100], D Loss: 0.2246, G Loss: 2.5360\n","Epoch [20/100], D Loss: 0.1912, G Loss: 2.7492\n","Epoch [21/100], D Loss: 0.1828, G Loss: 2.7264\n","Epoch [22/100], D Loss: 0.1658, G Loss: 2.8261\n","Epoch [23/100], D Loss: 0.1533, G Loss: 2.9263\n","Epoch [24/100], D Loss: 0.1275, G Loss: 3.2203\n","Epoch [25/100], D Loss: 0.1141, G Loss: 3.2034\n","Epoch [26/100], D Loss: 0.1180, G Loss: 3.2431\n","Epoch [27/100], D Loss: 0.0916, G Loss: 3.5488\n","Epoch [28/100], D Loss: 0.0915, G Loss: 3.4801\n","Epoch [29/100], D Loss: 0.0845, G Loss: 3.6111\n","Epoch [30/100], D Loss: 0.0825, G Loss: 3.7697\n","Epoch [31/100], D Loss: 0.0655, G Loss: 3.9314\n","Epoch [32/100], D Loss: 0.0684, G Loss: 3.8253\n","Epoch [33/100], D Loss: 0.0643, G Loss: 3.9814\n","Epoch [34/100], D Loss: 0.0698, G Loss: 3.7609\n","Epoch [35/100], D Loss: 0.0617, G Loss: 4.1599\n","Epoch [36/100], D Loss: 0.0627, G Loss: 4.1615\n","Epoch [37/100], D Loss: 0.0576, G Loss: 4.0633\n","Epoch [38/100], D Loss: 0.0592, G Loss: 4.0430\n","Epoch [39/100], D Loss: 0.0513, G Loss: 4.4117\n","Epoch [40/100], D Loss: 0.0493, G Loss: 4.1703\n","Epoch [41/100], D Loss: 0.0461, G Loss: 4.3981\n","Epoch [42/100], D Loss: 0.0501, G Loss: 4.5326\n","Epoch [43/100], D Loss: 0.0405, G Loss: 4.4396\n","Epoch [44/100], D Loss: 0.0377, G Loss: 4.6228\n","Epoch [45/100], D Loss: 0.0359, G Loss: 4.5106\n","Epoch [46/100], D Loss: 0.0383, G Loss: 4.6452\n","Epoch [47/100], D Loss: 0.0327, G Loss: 4.7018\n","Epoch [48/100], D Loss: 0.0409, G Loss: 4.6065\n","Epoch [49/100], D Loss: 0.0341, G Loss: 4.6109\n","Epoch [50/100], D Loss: 0.0377, G Loss: 4.6869\n","Epoch [51/100], D Loss: 0.0425, G Loss: 4.6162\n","Epoch [52/100], D Loss: 0.0350, G Loss: 4.6439\n","Epoch [53/100], D Loss: 0.0379, G Loss: 4.4566\n","Epoch [54/100], D Loss: 0.0374, G Loss: 4.7104\n","Epoch [55/100], D Loss: 0.0348, G Loss: 4.7291\n","Epoch [56/100], D Loss: 0.0278, G Loss: 4.8840\n","Epoch [57/100], D Loss: 0.0336, G Loss: 4.7761\n","Epoch [58/100], D Loss: 0.0292, G Loss: 4.8816\n","Epoch [59/100], D Loss: 0.0273, G Loss: 4.8311\n","Epoch [60/100], D Loss: 0.0336, G Loss: 4.8828\n","Epoch [61/100], D Loss: 0.0275, G Loss: 4.8805\n","Epoch [62/100], D Loss: 0.0255, G Loss: 4.9673\n","Epoch [63/100], D Loss: 0.0274, G Loss: 4.8664\n","Epoch [64/100], D Loss: 0.0340, G Loss: 4.7881\n","Epoch [65/100], D Loss: 0.0278, G Loss: 4.8376\n","Epoch [66/100], D Loss: 0.0311, G Loss: 4.8572\n","Epoch [67/100], D Loss: 0.0252, G Loss: 5.0566\n","Epoch [68/100], D Loss: 0.0287, G Loss: 4.7969\n","Epoch [69/100], D Loss: 0.0296, G Loss: 4.8377\n","Epoch [70/100], D Loss: 0.0334, G Loss: 4.7737\n","Epoch [71/100], D Loss: 0.0332, G Loss: 4.8925\n","Epoch [72/100], D Loss: 0.0277, G Loss: 5.2344\n","Epoch [73/100], D Loss: 0.0281, G Loss: 4.9286\n","Epoch [74/100], D Loss: 0.0378, G Loss: 4.7821\n","Epoch [75/100], D Loss: 0.0366, G Loss: 4.7957\n","Epoch [76/100], D Loss: 0.0360, G Loss: 4.8472\n","Epoch [77/100], D Loss: 0.0258, G Loss: 5.0669\n","Epoch [78/100], D Loss: 0.0301, G Loss: 5.0481\n","Epoch [79/100], D Loss: 0.0330, G Loss: 4.7018\n","Epoch [80/100], D Loss: 0.0334, G Loss: 5.0595\n","Epoch [81/100], D Loss: 0.0274, G Loss: 5.0071\n","Epoch [82/100], D Loss: 0.0291, G Loss: 5.0582\n","Epoch [83/100], D Loss: 0.0319, G Loss: 4.7821\n","Epoch [84/100], D Loss: 0.0297, G Loss: 4.9197\n","Epoch [85/100], D Loss: 0.0256, G Loss: 5.0180\n","Epoch [86/100], D Loss: 0.0297, G Loss: 5.0274\n","Epoch [87/100], D Loss: 0.0269, G Loss: 4.9519\n","Epoch [88/100], D Loss: 0.0335, G Loss: 5.1442\n","Epoch [89/100], D Loss: 0.0261, G Loss: 5.0756\n","Epoch [90/100], D Loss: 0.0335, G Loss: 4.6744\n","Epoch [91/100], D Loss: 0.0298, G Loss: 5.1695\n","Epoch [92/100], D Loss: 0.0422, G Loss: 4.9065\n","Epoch [93/100], D Loss: 0.0362, G Loss: 4.9150\n","Epoch [94/100], D Loss: 0.0396, G Loss: 4.8763\n","Epoch [95/100], D Loss: 0.0326, G Loss: 4.9049\n","Epoch [96/100], D Loss: 0.0363, G Loss: 4.8179\n","Epoch [97/100], D Loss: 0.0375, G Loss: 4.9736\n","Epoch [98/100], D Loss: 0.0311, G Loss: 5.0663\n","Epoch [99/100], D Loss: 0.0381, G Loss: 5.2285\n","Epoch [100/100], D Loss: 0.0385, G Loss: 4.9721\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.4503084320443485,\n","    \"FID (Original vs Eval)\": -0.09967537106276378,\n","    \"KL Divergence\": 0.031721872889069413,\n","    \"Cosine Similarity\": -0.0001916338223963976,\n","    \"Spearman Rank Correlation\": -0.0013088136331473562,\n","    \"Wasserstein Distance\": 0.018688936588855384,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.569484887170565\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_FlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/100], D Loss: 1.3786, G Loss: 0.6858\n","Epoch [2/100], D Loss: 1.3799, G Loss: 0.6889\n","Epoch [3/100], D Loss: 1.3738, G Loss: 0.6962\n","Epoch [4/100], D Loss: 1.3727, G Loss: 0.6963\n","Epoch [5/100], D Loss: 1.3735, G Loss: 0.6948\n","Epoch [6/100], D Loss: 1.3743, G Loss: 0.6976\n","Epoch [7/100], D Loss: 1.3725, G Loss: 0.7014\n","Epoch [8/100], D Loss: 1.3739, G Loss: 0.6992\n","Epoch [9/100], D Loss: 1.3739, G Loss: 0.6992\n","Epoch [10/100], D Loss: 1.3742, G Loss: 0.6999\n","Epoch [11/100], D Loss: 1.3745, G Loss: 0.7000\n","Epoch [12/100], D Loss: 1.3736, G Loss: 0.7000\n","Epoch [13/100], D Loss: 1.3747, G Loss: 0.6993\n","Epoch [14/100], D Loss: 1.3751, G Loss: 0.6994\n","Epoch [15/100], D Loss: 1.3754, G Loss: 0.6991\n","Epoch [16/100], D Loss: 1.3752, G Loss: 0.6993\n","Epoch [17/100], D Loss: 1.3758, G Loss: 0.6987\n","Epoch [18/100], D Loss: 1.3760, G Loss: 0.6985\n","Epoch [19/100], D Loss: 1.3771, G Loss: 0.6979\n","Epoch [20/100], D Loss: 1.3773, G Loss: 0.6977\n","Epoch [21/100], D Loss: 1.3780, G Loss: 0.6974\n","Epoch [22/100], D Loss: 1.3784, G Loss: 0.6973\n","Epoch [23/100], D Loss: 1.3787, G Loss: 0.6973\n","Epoch [24/100], D Loss: 1.3791, G Loss: 0.6973\n","Epoch [25/100], D Loss: 1.3791, G Loss: 0.6970\n","Epoch [26/100], D Loss: 1.3792, G Loss: 0.6971\n","Epoch [27/100], D Loss: 1.3794, G Loss: 0.6971\n","Epoch [28/100], D Loss: 1.3793, G Loss: 0.6971\n","Epoch [29/100], D Loss: 1.3799, G Loss: 0.6968\n","Epoch [30/100], D Loss: 1.3797, G Loss: 0.6970\n","Epoch [31/100], D Loss: 1.3797, G Loss: 0.6970\n","Epoch [32/100], D Loss: 1.3795, G Loss: 0.6976\n","Epoch [33/100], D Loss: 1.3798, G Loss: 0.6966\n","Epoch [34/100], D Loss: 1.3797, G Loss: 0.6963\n","Epoch [35/100], D Loss: 1.3797, G Loss: 0.6969\n","Epoch [36/100], D Loss: 1.3796, G Loss: 0.6967\n","Epoch [37/100], D Loss: 1.3791, G Loss: 0.6975\n","Epoch [38/100], D Loss: 1.3788, G Loss: 0.6974\n","Epoch [39/100], D Loss: 1.3790, G Loss: 0.6969\n","Epoch [40/100], D Loss: 1.3791, G Loss: 0.6972\n","Epoch [41/100], D Loss: 1.3796, G Loss: 0.6961\n","Epoch [42/100], D Loss: 1.3788, G Loss: 0.6974\n","Epoch [43/100], D Loss: 1.3788, G Loss: 0.6968\n","Epoch [44/100], D Loss: 1.3787, G Loss: 0.6972\n","Epoch [45/100], D Loss: 1.3788, G Loss: 0.6972\n","Epoch [46/100], D Loss: 1.3787, G Loss: 0.6975\n","Epoch [47/100], D Loss: 1.3784, G Loss: 0.6977\n","Epoch [48/100], D Loss: 1.3776, G Loss: 0.6980\n","Epoch [49/100], D Loss: 1.3785, G Loss: 0.6969\n","Epoch [50/100], D Loss: 1.3784, G Loss: 0.6972\n","Epoch [51/100], D Loss: 1.3772, G Loss: 0.6985\n","Epoch [52/100], D Loss: 1.3778, G Loss: 0.6974\n","Epoch [53/100], D Loss: 1.3773, G Loss: 0.6976\n","Epoch [54/100], D Loss: 1.3776, G Loss: 0.6976\n","Epoch [55/100], D Loss: 1.3763, G Loss: 0.6986\n","Epoch [56/100], D Loss: 1.3767, G Loss: 0.6978\n","Epoch [57/100], D Loss: 1.3761, G Loss: 0.6991\n","Epoch [58/100], D Loss: 1.3760, G Loss: 0.6988\n","Epoch [59/100], D Loss: 1.3761, G Loss: 0.6984\n","Epoch [60/100], D Loss: 1.3760, G Loss: 0.6984\n","Epoch [61/100], D Loss: 1.3735, G Loss: 0.6999\n","Epoch [62/100], D Loss: 1.3746, G Loss: 0.6991\n","Epoch [63/100], D Loss: 1.3732, G Loss: 0.7006\n","Epoch [64/100], D Loss: 1.3742, G Loss: 0.7000\n","Epoch [65/100], D Loss: 1.3750, G Loss: 0.6986\n","Epoch [66/100], D Loss: 1.3736, G Loss: 0.7002\n","Epoch [67/100], D Loss: 1.3742, G Loss: 0.7002\n","Epoch [68/100], D Loss: 1.3744, G Loss: 0.6995\n","Epoch [69/100], D Loss: 1.3740, G Loss: 0.6998\n","Epoch [70/100], D Loss: 1.3719, G Loss: 0.7024\n","Epoch [71/100], D Loss: 1.3666, G Loss: 0.7046\n","Epoch [72/100], D Loss: 1.3658, G Loss: 0.7054\n","Epoch [73/100], D Loss: 1.3722, G Loss: 0.7009\n","Epoch [74/100], D Loss: 1.3711, G Loss: 0.7025\n","Epoch [75/100], D Loss: 1.3618, G Loss: 0.7079\n","Epoch [76/100], D Loss: 1.3665, G Loss: 0.7033\n","Epoch [77/100], D Loss: 1.3618, G Loss: 0.7084\n","Epoch [78/100], D Loss: 1.3704, G Loss: 0.7018\n","Epoch [79/100], D Loss: 1.3623, G Loss: 0.7053\n","Epoch [80/100], D Loss: 1.3668, G Loss: 0.7043\n","Epoch [81/100], D Loss: 1.3585, G Loss: 0.7077\n","Epoch [82/100], D Loss: 1.3547, G Loss: 0.7081\n","Epoch [83/100], D Loss: 1.3558, G Loss: 0.7110\n","Epoch [84/100], D Loss: 1.3547, G Loss: 0.7105\n","Epoch [85/100], D Loss: 1.3678, G Loss: 0.7045\n","Epoch [86/100], D Loss: 1.3566, G Loss: 0.7100\n","Epoch [87/100], D Loss: 1.3654, G Loss: 0.7040\n","Epoch [88/100], D Loss: 1.3612, G Loss: 0.7062\n","Epoch [89/100], D Loss: 1.3620, G Loss: 0.7026\n","Epoch [90/100], D Loss: 1.3516, G Loss: 0.7085\n","Epoch [91/100], D Loss: 1.3570, G Loss: 0.7088\n","Epoch [92/100], D Loss: 1.3557, G Loss: 0.7100\n","Epoch [93/100], D Loss: 1.3590, G Loss: 0.7045\n","Epoch [94/100], D Loss: 1.3472, G Loss: 0.7087\n","Epoch [95/100], D Loss: 1.3583, G Loss: 0.7099\n","Epoch [96/100], D Loss: 1.3563, G Loss: 0.7149\n","Epoch [97/100], D Loss: 1.3572, G Loss: 0.7128\n","Epoch [98/100], D Loss: 1.3460, G Loss: 0.7142\n","Epoch [99/100], D Loss: 1.3341, G Loss: 0.7228\n","Epoch [100/100], D Loss: 1.3289, G Loss: 0.7195\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.5296575636152461,\n","    \"FID (Original vs Eval)\": -0.09967537106276378,\n","    \"KL Divergence\": 0.018101199565792613,\n","    \"Cosine Similarity\": 0.0021243744995445013,\n","    \"Spearman Rank Correlation\": 0.00403556695305209,\n","    \"Wasserstein Distance\": 0.020139068370544272,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.5587689924975707\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_FlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n","\n","ðŸš€ Processing embedding: vae_ImprovedFlexibleVAE_mse/ImprovedFlexibleVAE_mse_embeddings.pt\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a3b46d8deeb0>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n","Epoch [1/100], Loss Critic: 6.5923, Loss Generator: -0.0701\n","Epoch [2/100], Loss Critic: 4.5649, Loss Generator: -0.2277\n","Epoch [3/100], Loss Critic: 2.9949, Loss Generator: -0.5184\n","Epoch [4/100], Loss Critic: 1.9001, Loss Generator: -0.8854\n","Epoch [5/100], Loss Critic: 1.6276, Loss Generator: -1.2805\n","Epoch [6/100], Loss Critic: 1.7391, Loss Generator: -1.5802\n","Epoch [7/100], Loss Critic: 1.9134, Loss Generator: -1.7463\n","Epoch [8/100], Loss Critic: 1.8855, Loss Generator: -1.7236\n","Epoch [9/100], Loss Critic: 1.9209, Loss Generator: -1.7360\n","Epoch [10/100], Loss Critic: 1.7077, Loss Generator: -1.5300\n","Epoch [11/100], Loss Critic: 1.4827, Loss Generator: -1.3124\n","Epoch [12/100], Loss Critic: 1.1819, Loss Generator: -1.0283\n","Epoch [13/100], Loss Critic: 0.7023, Loss Generator: -0.5763\n","Epoch [14/100], Loss Critic: 0.1831, Loss Generator: -0.0842\n","Epoch [15/100], Loss Critic: -0.2985, Loss Generator: 0.3934\n","Epoch [16/100], Loss Critic: -0.7435, Loss Generator: 0.8421\n","Epoch [17/100], Loss Critic: -1.0090, Loss Generator: 1.1047\n","Epoch [18/100], Loss Critic: -1.2294, Loss Generator: 1.3127\n","Epoch [19/100], Loss Critic: -1.4402, Loss Generator: 1.5043\n","Epoch [20/100], Loss Critic: -1.4068, Loss Generator: 1.4522\n","Epoch [21/100], Loss Critic: -1.3992, Loss Generator: 1.4307\n","Epoch [22/100], Loss Critic: -1.3246, Loss Generator: 1.3244\n","Epoch [23/100], Loss Critic: -1.2372, Loss Generator: 1.2223\n","Epoch [24/100], Loss Critic: -1.1415, Loss Generator: 1.1065\n","Epoch [25/100], Loss Critic: -1.0901, Loss Generator: 1.0400\n","Epoch [26/100], Loss Critic: -0.8987, Loss Generator: 0.8294\n","Epoch [27/100], Loss Critic: -0.7825, Loss Generator: 0.7129\n","Epoch [28/100], Loss Critic: -0.6298, Loss Generator: 0.5796\n","Epoch [29/100], Loss Critic: -0.5831, Loss Generator: 0.5104\n","Epoch [30/100], Loss Critic: -0.3853, Loss Generator: 0.3656\n","Epoch [31/100], Loss Critic: -0.1485, Loss Generator: 0.2527\n","Epoch [32/100], Loss Critic: -0.0896, Loss Generator: 0.2086\n","Epoch [33/100], Loss Critic: 0.0259, Loss Generator: 0.1256\n","Epoch [34/100], Loss Critic: 0.0158, Loss Generator: 0.0586\n","Epoch [35/100], Loss Critic: -0.0049, Loss Generator: 0.0915\n","Epoch [36/100], Loss Critic: -0.0317, Loss Generator: 0.0782\n","Epoch [37/100], Loss Critic: -0.0819, Loss Generator: 0.0846\n","Epoch [38/100], Loss Critic: -0.1124, Loss Generator: 0.1364\n","Epoch [39/100], Loss Critic: -0.2014, Loss Generator: 0.1896\n","Epoch [40/100], Loss Critic: -0.2805, Loss Generator: 0.2356\n","Epoch [41/100], Loss Critic: -0.2998, Loss Generator: 0.2867\n","Epoch [42/100], Loss Critic: -0.3466, Loss Generator: 0.3278\n","Epoch [43/100], Loss Critic: -0.4277, Loss Generator: 0.3869\n","Epoch [44/100], Loss Critic: -0.4975, Loss Generator: 0.4553\n","Epoch [45/100], Loss Critic: -0.5978, Loss Generator: 0.5359\n","Epoch [46/100], Loss Critic: -0.5393, Loss Generator: 0.5149\n","Epoch [47/100], Loss Critic: -0.5509, Loss Generator: 0.4693\n","Epoch [48/100], Loss Critic: -0.5087, Loss Generator: 0.4268\n","Epoch [49/100], Loss Critic: -0.4881, Loss Generator: 0.3965\n","Epoch [50/100], Loss Critic: -0.4579, Loss Generator: 0.3669\n","Epoch [51/100], Loss Critic: -0.4741, Loss Generator: 0.3508\n","Epoch [52/100], Loss Critic: -0.4507, Loss Generator: 0.3172\n","Epoch [53/100], Loss Critic: -0.4523, Loss Generator: 0.2940\n","Epoch [54/100], Loss Critic: -0.4352, Loss Generator: 0.2968\n","Epoch [55/100], Loss Critic: -0.4460, Loss Generator: 0.2757\n","Epoch [56/100], Loss Critic: -0.4219, Loss Generator: 0.2644\n","Epoch [57/100], Loss Critic: -0.4269, Loss Generator: 0.2494\n","Epoch [58/100], Loss Critic: -0.4171, Loss Generator: 0.2314\n","Epoch [59/100], Loss Critic: -0.4008, Loss Generator: 0.2290\n","Epoch [60/100], Loss Critic: -0.3795, Loss Generator: 0.2004\n","Epoch [61/100], Loss Critic: -0.3791, Loss Generator: 0.1856\n","Epoch [62/100], Loss Critic: -0.3884, Loss Generator: 0.2031\n","Epoch [63/100], Loss Critic: -0.3776, Loss Generator: 0.1860\n","Epoch [64/100], Loss Critic: -0.3747, Loss Generator: 0.1713\n","Epoch [65/100], Loss Critic: -0.3784, Loss Generator: 0.1757\n","Epoch [66/100], Loss Critic: -0.3742, Loss Generator: 0.1599\n","Epoch [67/100], Loss Critic: -0.3686, Loss Generator: 0.1545\n","Epoch [68/100], Loss Critic: -0.3773, Loss Generator: 0.1558\n","Epoch [69/100], Loss Critic: -0.3549, Loss Generator: 0.1459\n","Epoch [70/100], Loss Critic: -0.3547, Loss Generator: 0.1340\n","Epoch [71/100], Loss Critic: -0.3579, Loss Generator: 0.1334\n","Epoch [72/100], Loss Critic: -0.3627, Loss Generator: 0.1369\n","Epoch [73/100], Loss Critic: -0.3404, Loss Generator: 0.1284\n","Epoch [74/100], Loss Critic: -0.3436, Loss Generator: 0.1136\n","Epoch [75/100], Loss Critic: -0.3591, Loss Generator: 0.1194\n","Epoch [76/100], Loss Critic: -0.3334, Loss Generator: 0.1046\n","Epoch [77/100], Loss Critic: -0.3209, Loss Generator: 0.1079\n","Epoch [78/100], Loss Critic: -0.3308, Loss Generator: 0.0946\n","Epoch [79/100], Loss Critic: -0.3457, Loss Generator: 0.1146\n","Epoch [80/100], Loss Critic: -0.3357, Loss Generator: 0.0974\n","Epoch [81/100], Loss Critic: -0.3248, Loss Generator: 0.0884\n","Epoch [82/100], Loss Critic: -0.3391, Loss Generator: 0.0937\n","Epoch [83/100], Loss Critic: -0.3309, Loss Generator: 0.0847\n","Epoch [84/100], Loss Critic: -0.3359, Loss Generator: 0.0866\n","Epoch [85/100], Loss Critic: -0.3285, Loss Generator: 0.0834\n","Epoch [86/100], Loss Critic: -0.3319, Loss Generator: 0.0854\n","Epoch [87/100], Loss Critic: -0.3282, Loss Generator: 0.0830\n","Epoch [88/100], Loss Critic: -0.3405, Loss Generator: 0.0911\n","Epoch [89/100], Loss Critic: -0.3205, Loss Generator: 0.0729\n","Epoch [90/100], Loss Critic: -0.3115, Loss Generator: 0.0652\n","Epoch [91/100], Loss Critic: -0.3104, Loss Generator: 0.0578\n","Epoch [92/100], Loss Critic: -0.3045, Loss Generator: 0.0526\n","Epoch [93/100], Loss Critic: -0.3125, Loss Generator: 0.0666\n","Epoch [94/100], Loss Critic: -0.3096, Loss Generator: 0.0634\n","Epoch [95/100], Loss Critic: -0.2803, Loss Generator: 0.0577\n","Epoch [96/100], Loss Critic: -0.3054, Loss Generator: 0.0502\n","Epoch [97/100], Loss Critic: -0.3007, Loss Generator: 0.0478\n","Epoch [98/100], Loss Critic: -0.3037, Loss Generator: 0.0470\n","Epoch [99/100], Loss Critic: -0.3011, Loss Generator: 0.0391\n","Epoch [100/100], Loss Critic: -0.3079, Loss Generator: 0.0504\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.6700855507222268,\n","    \"FID (Original vs Eval)\": -0.09999999999999347,\n","    \"KL Divergence\": 1.5512111501267685,\n","    \"Cosine Similarity\": 0.09969688951969147,\n","    \"Spearman Rank Correlation\": -0.0031320931558752453,\n","    \"Wasserstein Distance\": 0.3404699314940386,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.3070095919751053\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_ImprovedFlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/100], D Loss: 1.3548, G Loss: 7.3430\n","Epoch [2/100], D Loss: 1.3155, G Loss: 5.1600\n","Epoch [3/100], D Loss: 1.2744, G Loss: 3.7305\n","Epoch [4/100], D Loss: 1.2117, G Loss: 2.8270\n","Epoch [5/100], D Loss: 1.1353, G Loss: 2.2528\n","Epoch [6/100], D Loss: 1.0417, G Loss: 1.9213\n","Epoch [7/100], D Loss: 0.9288, G Loss: 1.7492\n","Epoch [8/100], D Loss: 0.8463, G Loss: 1.6030\n","Epoch [9/100], D Loss: 0.7414, G Loss: 1.5729\n","Epoch [10/100], D Loss: 0.6459, G Loss: 1.5823\n","Epoch [11/100], D Loss: 0.5575, G Loss: 1.6133\n","Epoch [12/100], D Loss: 0.4932, G Loss: 1.6724\n","Epoch [13/100], D Loss: 0.4609, G Loss: 1.6602\n","Epoch [14/100], D Loss: 0.4158, G Loss: 1.7704\n","Epoch [15/100], D Loss: 0.3722, G Loss: 1.8074\n","Epoch [16/100], D Loss: 0.3323, G Loss: 1.9170\n","Epoch [17/100], D Loss: 0.2832, G Loss: 2.1120\n","Epoch [18/100], D Loss: 0.2628, G Loss: 2.1536\n","Epoch [19/100], D Loss: 0.2298, G Loss: 2.3282\n","Epoch [20/100], D Loss: 0.2083, G Loss: 2.4516\n","Epoch [21/100], D Loss: 0.1891, G Loss: 2.5345\n","Epoch [22/100], D Loss: 0.1613, G Loss: 2.7447\n","Epoch [23/100], D Loss: 0.1464, G Loss: 2.8037\n","Epoch [24/100], D Loss: 0.1298, G Loss: 2.9934\n","Epoch [25/100], D Loss: 0.1204, G Loss: 3.0428\n","Epoch [26/100], D Loss: 0.1131, G Loss: 3.0801\n","Epoch [27/100], D Loss: 0.0970, G Loss: 3.2935\n","Epoch [28/100], D Loss: 0.0983, G Loss: 3.3212\n","Epoch [29/100], D Loss: 0.0839, G Loss: 3.5023\n","Epoch [30/100], D Loss: 0.0683, G Loss: 3.6263\n","Epoch [31/100], D Loss: 0.0740, G Loss: 3.6367\n","Epoch [32/100], D Loss: 0.0694, G Loss: 3.7251\n","Epoch [33/100], D Loss: 0.0657, G Loss: 3.8253\n","Epoch [34/100], D Loss: 0.0549, G Loss: 3.9008\n","Epoch [35/100], D Loss: 0.0567, G Loss: 4.0118\n","Epoch [36/100], D Loss: 0.0526, G Loss: 4.1288\n","Epoch [37/100], D Loss: 0.0421, G Loss: 4.3244\n","Epoch [38/100], D Loss: 0.0408, G Loss: 4.3106\n","Epoch [39/100], D Loss: 0.0472, G Loss: 4.2544\n","Epoch [40/100], D Loss: 0.0455, G Loss: 4.1853\n","Epoch [41/100], D Loss: 0.0406, G Loss: 4.3343\n","Epoch [42/100], D Loss: 0.0430, G Loss: 4.3201\n","Epoch [43/100], D Loss: 0.0337, G Loss: 4.4600\n","Epoch [44/100], D Loss: 0.0340, G Loss: 4.5255\n","Epoch [45/100], D Loss: 0.0367, G Loss: 4.4770\n","Epoch [46/100], D Loss: 0.0351, G Loss: 4.5699\n","Epoch [47/100], D Loss: 0.0304, G Loss: 4.7031\n","Epoch [48/100], D Loss: 0.0321, G Loss: 4.7003\n","Epoch [49/100], D Loss: 0.0307, G Loss: 4.7662\n","Epoch [50/100], D Loss: 0.0276, G Loss: 4.8001\n","Epoch [51/100], D Loss: 0.0285, G Loss: 4.7298\n","Epoch [52/100], D Loss: 0.0281, G Loss: 4.7726\n","Epoch [53/100], D Loss: 0.0258, G Loss: 4.9094\n","Epoch [54/100], D Loss: 0.0232, G Loss: 4.9078\n","Epoch [55/100], D Loss: 0.0223, G Loss: 5.0497\n","Epoch [56/100], D Loss: 0.0222, G Loss: 4.9014\n","Epoch [57/100], D Loss: 0.0252, G Loss: 4.9428\n","Epoch [58/100], D Loss: 0.0202, G Loss: 5.1006\n","Epoch [59/100], D Loss: 0.0252, G Loss: 4.9225\n","Epoch [60/100], D Loss: 0.0236, G Loss: 4.9228\n","Epoch [61/100], D Loss: 0.0221, G Loss: 5.1331\n","Epoch [62/100], D Loss: 0.0241, G Loss: 5.0521\n","Epoch [63/100], D Loss: 0.0216, G Loss: 4.9713\n","Epoch [64/100], D Loss: 0.0186, G Loss: 5.1197\n","Epoch [65/100], D Loss: 0.0224, G Loss: 5.3128\n","Epoch [66/100], D Loss: 0.0230, G Loss: 5.1048\n","Epoch [67/100], D Loss: 0.0197, G Loss: 5.2996\n","Epoch [68/100], D Loss: 0.0250, G Loss: 4.9690\n","Epoch [69/100], D Loss: 0.0203, G Loss: 5.1713\n","Epoch [70/100], D Loss: 0.0215, G Loss: 5.1882\n","Epoch [71/100], D Loss: 0.0207, G Loss: 5.0427\n","Epoch [72/100], D Loss: 0.0182, G Loss: 5.5957\n","Epoch [73/100], D Loss: 0.0212, G Loss: 5.1682\n","Epoch [74/100], D Loss: 0.0219, G Loss: 5.0942\n","Epoch [75/100], D Loss: 0.0199, G Loss: 5.4073\n","Epoch [76/100], D Loss: 0.0193, G Loss: 5.3411\n","Epoch [77/100], D Loss: 0.0180, G Loss: 5.4964\n","Epoch [78/100], D Loss: 0.0217, G Loss: 5.1853\n","Epoch [79/100], D Loss: 0.0193, G Loss: 5.1227\n","Epoch [80/100], D Loss: 0.0260, G Loss: 5.0162\n","Epoch [81/100], D Loss: 0.0200, G Loss: 5.2939\n","Epoch [82/100], D Loss: 0.0186, G Loss: 5.2881\n","Epoch [83/100], D Loss: 0.0181, G Loss: 5.5648\n","Epoch [84/100], D Loss: 0.0213, G Loss: 5.2237\n","Epoch [85/100], D Loss: 0.0180, G Loss: 5.4169\n","Epoch [86/100], D Loss: 0.0199, G Loss: 5.1859\n","Epoch [87/100], D Loss: 0.0191, G Loss: 5.3190\n","Epoch [88/100], D Loss: 0.0234, G Loss: 5.3201\n","Epoch [89/100], D Loss: 0.0215, G Loss: 5.2633\n","Epoch [90/100], D Loss: 0.0209, G Loss: 5.1631\n","Epoch [91/100], D Loss: 0.0227, G Loss: 5.1886\n","Epoch [92/100], D Loss: 0.0206, G Loss: 5.1098\n","Epoch [93/100], D Loss: 0.0236, G Loss: 4.9979\n","Epoch [94/100], D Loss: 0.0201, G Loss: 5.2615\n","Epoch [95/100], D Loss: 0.0236, G Loss: 4.9702\n","Epoch [96/100], D Loss: 0.0271, G Loss: 4.9752\n","Epoch [97/100], D Loss: 0.0286, G Loss: 5.0926\n","Epoch [98/100], D Loss: 0.0240, G Loss: 5.0534\n","Epoch [99/100], D Loss: 0.0232, G Loss: 5.1095\n","Epoch [100/100], D Loss: 0.0375, G Loss: 5.1952\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.941588192063625,\n","    \"FID (Original vs Eval)\": -0.09999999999999347,\n","    \"KL Divergence\": 1.8058435188236388,\n","    \"Cosine Similarity\": -0.04813872650265694,\n","    \"Spearman Rank Correlation\": 0.004010450866809349,\n","    \"Wasserstein Distance\": 0.3489231098392008,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2635350481487613\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_ImprovedFlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/100], D Loss: 1.3568, G Loss: 0.6813\n","Epoch [2/100], D Loss: 1.3251, G Loss: 0.7083\n","Epoch [3/100], D Loss: 1.2998, G Loss: 0.7293\n","Epoch [4/100], D Loss: 1.2795, G Loss: 0.7461\n","Epoch [5/100], D Loss: 1.2350, G Loss: 0.7864\n","Epoch [6/100], D Loss: 1.1825, G Loss: 0.8281\n","Epoch [7/100], D Loss: 1.1399, G Loss: 0.8547\n","Epoch [8/100], D Loss: 1.0720, G Loss: 0.9082\n","Epoch [9/100], D Loss: 0.9979, G Loss: 0.9675\n","Epoch [10/100], D Loss: 0.9143, G Loss: 1.0282\n","Epoch [11/100], D Loss: 0.8208, G Loss: 1.1205\n","Epoch [12/100], D Loss: 0.7053, G Loss: 1.2564\n","Epoch [13/100], D Loss: 0.6024, G Loss: 1.4080\n","Epoch [14/100], D Loss: 0.5214, G Loss: 1.5070\n","Epoch [15/100], D Loss: 0.4369, G Loss: 1.6666\n","Epoch [16/100], D Loss: 0.3839, G Loss: 1.7904\n","Epoch [17/100], D Loss: 0.3269, G Loss: 1.9348\n","Epoch [18/100], D Loss: 0.2796, G Loss: 2.1009\n","Epoch [19/100], D Loss: 0.2429, G Loss: 2.2330\n","Epoch [20/100], D Loss: 0.2049, G Loss: 2.4114\n","Epoch [21/100], D Loss: 0.1772, G Loss: 2.5989\n","Epoch [22/100], D Loss: 0.1437, G Loss: 2.8341\n","Epoch [23/100], D Loss: 0.1390, G Loss: 2.8170\n","Epoch [24/100], D Loss: 0.1213, G Loss: 3.0127\n","Epoch [25/100], D Loss: 0.1166, G Loss: 2.9853\n","Epoch [26/100], D Loss: 0.1012, G Loss: 3.1370\n","Epoch [27/100], D Loss: 0.1015, G Loss: 3.2763\n","Epoch [28/100], D Loss: 0.0921, G Loss: 3.3416\n","Epoch [29/100], D Loss: 0.0856, G Loss: 3.3331\n","Epoch [30/100], D Loss: 0.0778, G Loss: 3.4915\n","Epoch [31/100], D Loss: 0.0713, G Loss: 3.5426\n","Epoch [32/100], D Loss: 0.0670, G Loss: 3.7332\n","Epoch [33/100], D Loss: 0.0687, G Loss: 3.6810\n","Epoch [34/100], D Loss: 0.0620, G Loss: 3.7363\n","Epoch [35/100], D Loss: 0.0649, G Loss: 3.8429\n","Epoch [36/100], D Loss: 0.0590, G Loss: 3.8490\n","Epoch [37/100], D Loss: 0.0529, G Loss: 3.9827\n","Epoch [38/100], D Loss: 0.0501, G Loss: 3.9961\n","Epoch [39/100], D Loss: 0.0458, G Loss: 4.0900\n","Epoch [40/100], D Loss: 0.0447, G Loss: 4.2352\n","Epoch [41/100], D Loss: 0.0395, G Loss: 4.2611\n","Epoch [42/100], D Loss: 0.0400, G Loss: 4.2474\n","Epoch [43/100], D Loss: 0.0443, G Loss: 4.2100\n","Epoch [44/100], D Loss: 0.0423, G Loss: 4.2738\n","Epoch [45/100], D Loss: 0.0354, G Loss: 4.4005\n","Epoch [46/100], D Loss: 0.0403, G Loss: 4.4251\n","Epoch [47/100], D Loss: 0.0545, G Loss: 4.2719\n","Epoch [48/100], D Loss: 0.0364, G Loss: 4.4643\n","Epoch [49/100], D Loss: 0.0407, G Loss: 4.4816\n","Epoch [50/100], D Loss: 0.0399, G Loss: 4.5492\n","Epoch [51/100], D Loss: 0.0404, G Loss: 4.4652\n","Epoch [52/100], D Loss: 0.0383, G Loss: 4.5028\n","Epoch [53/100], D Loss: 0.0354, G Loss: 4.6986\n","Epoch [54/100], D Loss: 0.0321, G Loss: 4.5906\n","Epoch [55/100], D Loss: 0.0323, G Loss: 4.7425\n","Epoch [56/100], D Loss: 0.0352, G Loss: 4.6792\n","Epoch [57/100], D Loss: 0.0321, G Loss: 4.6842\n","Epoch [58/100], D Loss: 0.0411, G Loss: 4.5491\n","Epoch [59/100], D Loss: 0.0321, G Loss: 4.8205\n","Epoch [60/100], D Loss: 0.0370, G Loss: 4.7228\n","Epoch [61/100], D Loss: 0.0312, G Loss: 4.8426\n","Epoch [62/100], D Loss: 0.0292, G Loss: 4.9515\n","Epoch [63/100], D Loss: 0.0315, G Loss: 4.6919\n","Epoch [64/100], D Loss: 0.0323, G Loss: 4.7623\n","Epoch [65/100], D Loss: 0.0295, G Loss: 4.8399\n","Epoch [66/100], D Loss: 0.0333, G Loss: 4.6864\n","Epoch [67/100], D Loss: 0.0346, G Loss: 4.6964\n","Epoch [68/100], D Loss: 0.0280, G Loss: 4.8172\n","Epoch [69/100], D Loss: 0.0293, G Loss: 5.1321\n","Epoch [70/100], D Loss: 0.0282, G Loss: 4.8406\n","Epoch [71/100], D Loss: 0.0354, G Loss: 4.8427\n","Epoch [72/100], D Loss: 0.0290, G Loss: 4.9773\n","Epoch [73/100], D Loss: 0.0379, G Loss: 4.9228\n","Epoch [74/100], D Loss: 0.0339, G Loss: 4.9038\n","Epoch [75/100], D Loss: 0.0414, G Loss: 4.5607\n","Epoch [76/100], D Loss: 0.0261, G Loss: 5.0682\n","Epoch [77/100], D Loss: 0.0325, G Loss: 4.8263\n","Epoch [78/100], D Loss: 0.0374, G Loss: 4.6992\n","Epoch [79/100], D Loss: 0.0357, G Loss: 4.9779\n","Epoch [80/100], D Loss: 0.0339, G Loss: 4.9443\n","Epoch [81/100], D Loss: 0.0392, G Loss: 4.6042\n","Epoch [82/100], D Loss: 0.0428, G Loss: 4.9605\n","Epoch [83/100], D Loss: 0.0590, G Loss: 4.6985\n","Epoch [84/100], D Loss: 0.0315, G Loss: 4.8630\n","Epoch [85/100], D Loss: 0.0431, G Loss: 4.7782\n","Epoch [86/100], D Loss: 0.0542, G Loss: 4.9865\n","Epoch [87/100], D Loss: 0.0407, G Loss: 5.2181\n","Epoch [88/100], D Loss: 0.0545, G Loss: 4.6828\n","Epoch [89/100], D Loss: 0.0630, G Loss: 4.8751\n","Epoch [90/100], D Loss: 0.0472, G Loss: 4.5375\n","Epoch [91/100], D Loss: 0.0543, G Loss: 5.0207\n","Epoch [92/100], D Loss: 0.0767, G Loss: 4.6416\n","Epoch [93/100], D Loss: 0.0595, G Loss: 4.6153\n","Epoch [94/100], D Loss: 0.0597, G Loss: 4.9300\n","Epoch [95/100], D Loss: 0.0769, G Loss: 4.4861\n","Epoch [96/100], D Loss: 0.0726, G Loss: 5.1293\n","Epoch [97/100], D Loss: 0.0794, G Loss: 5.2011\n","Epoch [98/100], D Loss: 0.0704, G Loss: 4.8767\n","Epoch [99/100], D Loss: 0.0691, G Loss: 4.9857\n","Epoch [100/100], D Loss: 0.1083, G Loss: 4.6552\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.4897723033181429,\n","    \"FID (Original vs Eval)\": -0.09999999999999347,\n","    \"KL Divergence\": 2.0203231843706773,\n","    \"Cosine Similarity\": 0.047283533960580826,\n","    \"Spearman Rank Correlation\": -0.0034099059677381266,\n","    \"Wasserstein Distance\": 0.33325385007328123,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.44383120214121086\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_ImprovedFlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/100], D Loss: 1.3823, G Loss: 0.7135\n","Epoch [2/100], D Loss: 1.3810, G Loss: 0.7057\n","Epoch [3/100], D Loss: 1.3798, G Loss: 0.7034\n","Epoch [4/100], D Loss: 1.3680, G Loss: 0.7124\n","Epoch [5/100], D Loss: 1.3794, G Loss: 0.6963\n","Epoch [6/100], D Loss: 1.3711, G Loss: 0.7071\n","Epoch [7/100], D Loss: 1.3718, G Loss: 0.7022\n","Epoch [8/100], D Loss: 1.3747, G Loss: 0.6997\n","Epoch [9/100], D Loss: 1.3721, G Loss: 0.7011\n","Epoch [10/100], D Loss: 1.3733, G Loss: 0.6991\n","Epoch [11/100], D Loss: 1.3723, G Loss: 0.7006\n","Epoch [12/100], D Loss: 1.3730, G Loss: 0.6997\n","Epoch [13/100], D Loss: 1.3727, G Loss: 0.7000\n","Epoch [14/100], D Loss: 1.3728, G Loss: 0.7003\n","Epoch [15/100], D Loss: 1.3726, G Loss: 0.7001\n","Epoch [16/100], D Loss: 1.3733, G Loss: 0.6992\n","Epoch [17/100], D Loss: 1.3726, G Loss: 0.7002\n","Epoch [18/100], D Loss: 1.3734, G Loss: 0.6994\n","Epoch [19/100], D Loss: 1.3730, G Loss: 0.6997\n","Epoch [20/100], D Loss: 1.3737, G Loss: 0.6991\n","Epoch [21/100], D Loss: 1.3737, G Loss: 0.6993\n","Epoch [22/100], D Loss: 1.3743, G Loss: 0.6991\n","Epoch [23/100], D Loss: 1.3743, G Loss: 0.6992\n","Epoch [24/100], D Loss: 1.3749, G Loss: 0.6987\n","Epoch [25/100], D Loss: 1.3746, G Loss: 0.6994\n","Epoch [26/100], D Loss: 1.3750, G Loss: 0.6989\n","Epoch [27/100], D Loss: 1.3756, G Loss: 0.6991\n","Epoch [28/100], D Loss: 1.3757, G Loss: 0.6977\n","Epoch [29/100], D Loss: 1.3760, G Loss: 0.6986\n","Epoch [30/100], D Loss: 1.3761, G Loss: 0.6984\n","Epoch [31/100], D Loss: 1.3762, G Loss: 0.6985\n","Epoch [32/100], D Loss: 1.3762, G Loss: 0.6979\n","Epoch [33/100], D Loss: 1.3758, G Loss: 0.6983\n","Epoch [34/100], D Loss: 1.3756, G Loss: 0.6995\n","Epoch [35/100], D Loss: 1.3754, G Loss: 0.6989\n","Epoch [36/100], D Loss: 1.3758, G Loss: 0.6981\n","Epoch [37/100], D Loss: 1.3759, G Loss: 0.6983\n","Epoch [38/100], D Loss: 1.3745, G Loss: 0.6998\n","Epoch [39/100], D Loss: 1.3748, G Loss: 0.6993\n","Epoch [40/100], D Loss: 1.3752, G Loss: 0.6989\n","Epoch [41/100], D Loss: 1.3754, G Loss: 0.6984\n","Epoch [42/100], D Loss: 1.3746, G Loss: 0.6992\n","Epoch [43/100], D Loss: 1.3740, G Loss: 0.6998\n","Epoch [44/100], D Loss: 1.3741, G Loss: 0.6994\n","Epoch [45/100], D Loss: 1.3743, G Loss: 0.6989\n","Epoch [46/100], D Loss: 1.3737, G Loss: 0.6997\n","Epoch [47/100], D Loss: 1.3729, G Loss: 0.7001\n","Epoch [48/100], D Loss: 1.3729, G Loss: 0.6993\n","Epoch [49/100], D Loss: 1.3732, G Loss: 0.6995\n","Epoch [50/100], D Loss: 1.3724, G Loss: 0.7002\n","Epoch [51/100], D Loss: 1.3724, G Loss: 0.6998\n","Epoch [52/100], D Loss: 1.3713, G Loss: 0.7007\n","Epoch [53/100], D Loss: 1.3717, G Loss: 0.7001\n","Epoch [54/100], D Loss: 1.3702, G Loss: 0.7017\n","Epoch [55/100], D Loss: 1.3699, G Loss: 0.7014\n","Epoch [56/100], D Loss: 1.3700, G Loss: 0.7017\n","Epoch [57/100], D Loss: 1.3699, G Loss: 0.7005\n","Epoch [58/100], D Loss: 1.3680, G Loss: 0.7029\n","Epoch [59/100], D Loss: 1.3684, G Loss: 0.7015\n","Epoch [60/100], D Loss: 1.3670, G Loss: 0.7032\n","Epoch [61/100], D Loss: 1.3646, G Loss: 0.7050\n","Epoch [62/100], D Loss: 1.3639, G Loss: 0.7053\n","Epoch [63/100], D Loss: 1.3662, G Loss: 0.7037\n","Epoch [64/100], D Loss: 1.3643, G Loss: 0.7050\n","Epoch [65/100], D Loss: 1.3654, G Loss: 0.7031\n","Epoch [66/100], D Loss: 1.3632, G Loss: 0.7055\n","Epoch [67/100], D Loss: 1.3590, G Loss: 0.7088\n","Epoch [68/100], D Loss: 1.3624, G Loss: 0.7052\n","Epoch [69/100], D Loss: 1.3592, G Loss: 0.7063\n","Epoch [70/100], D Loss: 1.3592, G Loss: 0.7074\n","Epoch [71/100], D Loss: 1.3559, G Loss: 0.7086\n","Epoch [72/100], D Loss: 1.3557, G Loss: 0.7085\n","Epoch [73/100], D Loss: 1.3540, G Loss: 0.7105\n","Epoch [74/100], D Loss: 1.3509, G Loss: 0.7093\n","Epoch [75/100], D Loss: 1.3520, G Loss: 0.7126\n","Epoch [76/100], D Loss: 1.3571, G Loss: 0.7111\n","Epoch [77/100], D Loss: 1.3539, G Loss: 0.7112\n","Epoch [78/100], D Loss: 1.3448, G Loss: 0.7161\n","Epoch [79/100], D Loss: 1.3512, G Loss: 0.7115\n","Epoch [80/100], D Loss: 1.3492, G Loss: 0.7155\n","Epoch [81/100], D Loss: 1.3520, G Loss: 0.7093\n","Epoch [82/100], D Loss: 1.3428, G Loss: 0.7126\n","Epoch [83/100], D Loss: 1.3517, G Loss: 0.7110\n","Epoch [84/100], D Loss: 1.3473, G Loss: 0.7129\n","Epoch [85/100], D Loss: 1.3433, G Loss: 0.7143\n","Epoch [86/100], D Loss: 1.3448, G Loss: 0.7204\n","Epoch [87/100], D Loss: 1.3392, G Loss: 0.7152\n","Epoch [88/100], D Loss: 1.3412, G Loss: 0.7180\n","Epoch [89/100], D Loss: 1.3282, G Loss: 0.7174\n","Epoch [90/100], D Loss: 1.3351, G Loss: 0.7261\n","Epoch [91/100], D Loss: 1.3244, G Loss: 0.7269\n","Epoch [92/100], D Loss: 1.3083, G Loss: 0.7320\n","Epoch [93/100], D Loss: 1.2842, G Loss: 0.7503\n","Epoch [94/100], D Loss: 1.3147, G Loss: 0.7286\n","Epoch [95/100], D Loss: 1.3144, G Loss: 0.7322\n","Epoch [96/100], D Loss: 1.3099, G Loss: 0.7296\n","Epoch [97/100], D Loss: 1.2885, G Loss: 0.7430\n","Epoch [98/100], D Loss: 1.2966, G Loss: 0.7381\n","Epoch [99/100], D Loss: 1.2805, G Loss: 0.7495\n","Epoch [100/100], D Loss: 1.2817, G Loss: 0.7510\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.5175176947827219,\n","    \"FID (Original vs Eval)\": -0.09999999999999347,\n","    \"KL Divergence\": 2.100504263024691,\n","    \"Cosine Similarity\": 0.05516941472887993,\n","    \"Spearman Rank Correlation\": -0.004743547406656515,\n","    \"Wasserstein Distance\": 0.3353626512507413,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.4386535241072024\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_ImprovedFlexibleVAE_mse.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n","\n","ðŸš€ Processing embedding: vae_ImprovedVAE_mse/ImprovedVAE_mse_embeddings.pt\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a3b46d8deeb0>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸš€ Running pipeline for GAN type: WGAN-GP\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training WGAN-GP...\n","Epoch [1/100], Loss Critic: 6.5949, Loss Generator: -0.0593\n","Epoch [2/100], Loss Critic: 4.7480, Loss Generator: -0.2180\n","Epoch [3/100], Loss Critic: 3.2602, Loss Generator: -0.5442\n","Epoch [4/100], Loss Critic: 2.0770, Loss Generator: -0.9102\n","Epoch [5/100], Loss Critic: 1.7844, Loss Generator: -1.3975\n","Epoch [6/100], Loss Critic: 1.9360, Loss Generator: -1.6760\n","Epoch [7/100], Loss Critic: 2.0279, Loss Generator: -1.7469\n","Epoch [8/100], Loss Critic: 1.9167, Loss Generator: -1.6801\n","Epoch [9/100], Loss Critic: 1.9567, Loss Generator: -1.6018\n","Epoch [10/100], Loss Critic: 1.7125, Loss Generator: -1.4487\n","Epoch [11/100], Loss Critic: 1.4856, Loss Generator: -1.2002\n","Epoch [12/100], Loss Critic: 1.2574, Loss Generator: -0.9973\n","Epoch [13/100], Loss Critic: 0.8711, Loss Generator: -0.6681\n","Epoch [14/100], Loss Critic: 0.6037, Loss Generator: -0.4044\n","Epoch [15/100], Loss Critic: 0.2901, Loss Generator: -0.0910\n","Epoch [16/100], Loss Critic: -0.1032, Loss Generator: 0.2696\n","Epoch [17/100], Loss Critic: -0.4674, Loss Generator: 0.6206\n","Epoch [18/100], Loss Critic: -0.6079, Loss Generator: 0.7867\n","Epoch [19/100], Loss Critic: -0.8326, Loss Generator: 0.9859\n","Epoch [20/100], Loss Critic: -1.0467, Loss Generator: 1.2003\n","Epoch [21/100], Loss Critic: -1.1425, Loss Generator: 1.2872\n","Epoch [22/100], Loss Critic: -1.1355, Loss Generator: 1.2641\n","Epoch [23/100], Loss Critic: -1.3154, Loss Generator: 1.4217\n","Epoch [24/100], Loss Critic: -1.2354, Loss Generator: 1.3287\n","Epoch [25/100], Loss Critic: -1.3774, Loss Generator: 1.4698\n","Epoch [26/100], Loss Critic: -1.2443, Loss Generator: 1.3066\n","Epoch [27/100], Loss Critic: -1.1990, Loss Generator: 1.2471\n","Epoch [28/100], Loss Critic: -1.1324, Loss Generator: 1.1689\n","Epoch [29/100], Loss Critic: -1.0703, Loss Generator: 1.0846\n","Epoch [30/100], Loss Critic: -0.8742, Loss Generator: 0.8787\n","Epoch [31/100], Loss Critic: -0.6861, Loss Generator: 0.6920\n","Epoch [32/100], Loss Critic: -0.6585, Loss Generator: 0.6670\n","Epoch [33/100], Loss Critic: -0.5524, Loss Generator: 0.5975\n","Epoch [34/100], Loss Critic: -0.3329, Loss Generator: 0.4759\n","Epoch [35/100], Loss Critic: -0.3343, Loss Generator: 0.4230\n","Epoch [36/100], Loss Critic: -0.1883, Loss Generator: 0.3348\n","Epoch [37/100], Loss Critic: -0.0865, Loss Generator: 0.2343\n","Epoch [38/100], Loss Critic: 0.0005, Loss Generator: 0.1906\n","Epoch [39/100], Loss Critic: 0.0110, Loss Generator: 0.1666\n","Epoch [40/100], Loss Critic: 0.0987, Loss Generator: 0.1017\n","Epoch [41/100], Loss Critic: -0.0021, Loss Generator: 0.1523\n","Epoch [42/100], Loss Critic: -0.0128, Loss Generator: 0.1214\n","Epoch [43/100], Loss Critic: -0.0835, Loss Generator: 0.1721\n","Epoch [44/100], Loss Critic: -0.1590, Loss Generator: 0.2622\n","Epoch [45/100], Loss Critic: -0.2814, Loss Generator: 0.3888\n","Epoch [46/100], Loss Critic: -0.4393, Loss Generator: 0.5071\n","Epoch [47/100], Loss Critic: -0.4397, Loss Generator: 0.5374\n","Epoch [48/100], Loss Critic: -0.4739, Loss Generator: 0.5691\n","Epoch [49/100], Loss Critic: -0.4749, Loss Generator: 0.5705\n","Epoch [50/100], Loss Critic: -0.5043, Loss Generator: 0.5532\n","Epoch [51/100], Loss Critic: -0.4899, Loss Generator: 0.5346\n","Epoch [52/100], Loss Critic: -0.4940, Loss Generator: 0.4990\n","Epoch [53/100], Loss Critic: -0.4622, Loss Generator: 0.4594\n","Epoch [54/100], Loss Critic: -0.4477, Loss Generator: 0.4391\n","Epoch [55/100], Loss Critic: -0.4226, Loss Generator: 0.4155\n","Epoch [56/100], Loss Critic: -0.3908, Loss Generator: 0.4109\n","Epoch [57/100], Loss Critic: -0.4501, Loss Generator: 0.4155\n","Epoch [58/100], Loss Critic: -0.4113, Loss Generator: 0.3931\n","Epoch [59/100], Loss Critic: -0.4128, Loss Generator: 0.3941\n","Epoch [60/100], Loss Critic: -0.4068, Loss Generator: 0.3680\n","Epoch [61/100], Loss Critic: -0.3763, Loss Generator: 0.3466\n","Epoch [62/100], Loss Critic: -0.3957, Loss Generator: 0.3451\n","Epoch [63/100], Loss Critic: -0.3966, Loss Generator: 0.3473\n","Epoch [64/100], Loss Critic: -0.4187, Loss Generator: 0.3593\n","Epoch [65/100], Loss Critic: -0.3839, Loss Generator: 0.3262\n","Epoch [66/100], Loss Critic: -0.3791, Loss Generator: 0.3319\n","Epoch [67/100], Loss Critic: -0.3650, Loss Generator: 0.3105\n","Epoch [68/100], Loss Critic: -0.3761, Loss Generator: 0.3202\n","Epoch [69/100], Loss Critic: -0.3789, Loss Generator: 0.3129\n","Epoch [70/100], Loss Critic: -0.3703, Loss Generator: 0.3030\n","Epoch [71/100], Loss Critic: -0.3524, Loss Generator: 0.2937\n","Epoch [72/100], Loss Critic: -0.3640, Loss Generator: 0.2893\n","Epoch [73/100], Loss Critic: -0.3622, Loss Generator: 0.2879\n","Epoch [74/100], Loss Critic: -0.3646, Loss Generator: 0.2875\n","Epoch [75/100], Loss Critic: -0.3424, Loss Generator: 0.2742\n","Epoch [76/100], Loss Critic: -0.3559, Loss Generator: 0.2845\n","Epoch [77/100], Loss Critic: -0.3546, Loss Generator: 0.2708\n","Epoch [78/100], Loss Critic: -0.3502, Loss Generator: 0.2713\n","Epoch [79/100], Loss Critic: -0.3493, Loss Generator: 0.2664\n","Epoch [80/100], Loss Critic: -0.3511, Loss Generator: 0.2749\n","Epoch [81/100], Loss Critic: -0.3416, Loss Generator: 0.2557\n","Epoch [82/100], Loss Critic: -0.3392, Loss Generator: 0.2475\n","Epoch [83/100], Loss Critic: -0.3329, Loss Generator: 0.2554\n","Epoch [84/100], Loss Critic: -0.3332, Loss Generator: 0.2464\n","Epoch [85/100], Loss Critic: -0.3288, Loss Generator: 0.2451\n","Epoch [86/100], Loss Critic: -0.3280, Loss Generator: 0.2477\n","Epoch [87/100], Loss Critic: -0.3188, Loss Generator: 0.2434\n","Epoch [88/100], Loss Critic: -0.3187, Loss Generator: 0.2295\n","Epoch [89/100], Loss Critic: -0.3300, Loss Generator: 0.2355\n","Epoch [90/100], Loss Critic: -0.2981, Loss Generator: 0.2338\n","Epoch [91/100], Loss Critic: -0.3111, Loss Generator: 0.2145\n","Epoch [92/100], Loss Critic: -0.3195, Loss Generator: 0.2313\n","Epoch [93/100], Loss Critic: -0.3259, Loss Generator: 0.2332\n","Epoch [94/100], Loss Critic: -0.3243, Loss Generator: 0.2293\n","Epoch [95/100], Loss Critic: -0.3205, Loss Generator: 0.2300\n","Epoch [96/100], Loss Critic: -0.3107, Loss Generator: 0.2108\n","Epoch [97/100], Loss Critic: -0.3129, Loss Generator: 0.2190\n","Epoch [98/100], Loss Critic: -0.3186, Loss Generator: 0.2234\n","Epoch [99/100], Loss Critic: -0.3091, Loss Generator: 0.2142\n","Epoch [100/100], Loss Critic: -0.3018, Loss Generator: 0.2099\n","âœ… WGAN-GP training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.827421661013825,\n","    \"FID (Original vs Eval)\": -0.0999999999999815,\n","    \"KL Divergence\": 1.4323996276174515,\n","    \"Cosine Similarity\": 0.1098562702536583,\n","    \"Spearman Rank Correlation\": 0.0021336442377579193,\n","    \"Wasserstein Distance\": 0.35647896219695024,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.3089169468858014\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_WGAN-GP_ImprovedVAE_mse.json\n","âœ… Completed pipeline for GAN type: WGAN-GP\n","\n","\n","ðŸš€ Running pipeline for GAN type: VAE-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training VAE-GAN...\n","Epoch [1/100], D Loss: 1.3471, G Loss: 7.4685\n","Epoch [2/100], D Loss: 1.3114, G Loss: 5.2576\n","Epoch [3/100], D Loss: 1.2657, G Loss: 3.8287\n","Epoch [4/100], D Loss: 1.2102, G Loss: 2.8841\n","Epoch [5/100], D Loss: 1.1365, G Loss: 2.3008\n","Epoch [6/100], D Loss: 1.0434, G Loss: 1.9707\n","Epoch [7/100], D Loss: 0.9429, G Loss: 1.8032\n","Epoch [8/100], D Loss: 0.8616, G Loss: 1.6707\n","Epoch [9/100], D Loss: 0.7703, G Loss: 1.6464\n","Epoch [10/100], D Loss: 0.6605, G Loss: 1.7109\n","Epoch [11/100], D Loss: 0.5986, G Loss: 1.6531\n","Epoch [12/100], D Loss: 0.5209, G Loss: 1.7357\n","Epoch [13/100], D Loss: 0.4447, G Loss: 1.8693\n","Epoch [14/100], D Loss: 0.4192, G Loss: 1.8470\n","Epoch [15/100], D Loss: 0.3825, G Loss: 1.8628\n","Epoch [16/100], D Loss: 0.3232, G Loss: 2.0562\n","Epoch [17/100], D Loss: 0.2908, G Loss: 2.1279\n","Epoch [18/100], D Loss: 0.2443, G Loss: 2.3741\n","Epoch [19/100], D Loss: 0.2152, G Loss: 2.4845\n","Epoch [20/100], D Loss: 0.2025, G Loss: 2.5243\n","Epoch [21/100], D Loss: 0.1734, G Loss: 2.6733\n","Epoch [22/100], D Loss: 0.1383, G Loss: 2.9746\n","Epoch [23/100], D Loss: 0.1425, G Loss: 2.8482\n","Epoch [24/100], D Loss: 0.1155, G Loss: 3.1237\n","Epoch [25/100], D Loss: 0.1170, G Loss: 3.1568\n","Epoch [26/100], D Loss: 0.1073, G Loss: 3.2972\n","Epoch [27/100], D Loss: 0.0971, G Loss: 3.2719\n","Epoch [28/100], D Loss: 0.0812, G Loss: 3.4643\n","Epoch [29/100], D Loss: 0.0757, G Loss: 3.6120\n","Epoch [30/100], D Loss: 0.0652, G Loss: 3.8807\n","Epoch [31/100], D Loss: 0.0623, G Loss: 3.8172\n","Epoch [32/100], D Loss: 0.0645, G Loss: 3.8858\n","Epoch [33/100], D Loss: 0.0586, G Loss: 3.9651\n","Epoch [34/100], D Loss: 0.0537, G Loss: 4.0551\n","Epoch [35/100], D Loss: 0.0463, G Loss: 4.0605\n","Epoch [36/100], D Loss: 0.0516, G Loss: 4.1369\n","Epoch [37/100], D Loss: 0.0461, G Loss: 4.1759\n","Epoch [38/100], D Loss: 0.0476, G Loss: 4.0900\n","Epoch [39/100], D Loss: 0.0457, G Loss: 4.1746\n","Epoch [40/100], D Loss: 0.0396, G Loss: 4.2712\n","Epoch [41/100], D Loss: 0.0352, G Loss: 4.4522\n","Epoch [42/100], D Loss: 0.0415, G Loss: 4.3332\n","Epoch [43/100], D Loss: 0.0401, G Loss: 4.3809\n","Epoch [44/100], D Loss: 0.0350, G Loss: 4.5035\n","Epoch [45/100], D Loss: 0.0343, G Loss: 4.4753\n","Epoch [46/100], D Loss: 0.0295, G Loss: 4.6140\n","Epoch [47/100], D Loss: 0.0251, G Loss: 4.7884\n","Epoch [48/100], D Loss: 0.0286, G Loss: 4.7929\n","Epoch [49/100], D Loss: 0.0242, G Loss: 4.8661\n","Epoch [50/100], D Loss: 0.0269, G Loss: 4.8414\n","Epoch [51/100], D Loss: 0.0300, G Loss: 4.7619\n","Epoch [52/100], D Loss: 0.0318, G Loss: 4.7457\n","Epoch [53/100], D Loss: 0.0273, G Loss: 4.7894\n","Epoch [54/100], D Loss: 0.0254, G Loss: 4.8582\n","Epoch [55/100], D Loss: 0.0265, G Loss: 4.9135\n","Epoch [56/100], D Loss: 0.0265, G Loss: 5.0972\n","Epoch [57/100], D Loss: 0.0246, G Loss: 4.9883\n","Epoch [58/100], D Loss: 0.0264, G Loss: 5.0409\n","Epoch [59/100], D Loss: 0.0258, G Loss: 4.8974\n","Epoch [60/100], D Loss: 0.0248, G Loss: 4.8607\n","Epoch [61/100], D Loss: 0.0230, G Loss: 5.0699\n","Epoch [62/100], D Loss: 0.0249, G Loss: 5.0577\n","Epoch [63/100], D Loss: 0.0289, G Loss: 5.0806\n","Epoch [64/100], D Loss: 0.0236, G Loss: 5.0724\n","Epoch [65/100], D Loss: 0.0242, G Loss: 5.0020\n","Epoch [66/100], D Loss: 0.0189, G Loss: 5.2103\n","Epoch [67/100], D Loss: 0.0208, G Loss: 5.0847\n","Epoch [68/100], D Loss: 0.0203, G Loss: 5.1255\n","Epoch [69/100], D Loss: 0.0196, G Loss: 5.2750\n","Epoch [70/100], D Loss: 0.0221, G Loss: 5.3533\n","Epoch [71/100], D Loss: 0.0248, G Loss: 5.1281\n","Epoch [72/100], D Loss: 0.0203, G Loss: 5.0373\n","Epoch [73/100], D Loss: 0.0183, G Loss: 5.3381\n","Epoch [74/100], D Loss: 0.0194, G Loss: 5.1433\n","Epoch [75/100], D Loss: 0.0195, G Loss: 5.2275\n","Epoch [76/100], D Loss: 0.0231, G Loss: 5.0870\n","Epoch [77/100], D Loss: 0.0234, G Loss: 5.2121\n","Epoch [78/100], D Loss: 0.0177, G Loss: 5.2409\n","Epoch [79/100], D Loss: 0.0200, G Loss: 5.3196\n","Epoch [80/100], D Loss: 0.0179, G Loss: 5.2777\n","Epoch [81/100], D Loss: 0.0192, G Loss: 5.1850\n","Epoch [82/100], D Loss: 0.0191, G Loss: 5.3713\n","Epoch [83/100], D Loss: 0.0166, G Loss: 5.3251\n","Epoch [84/100], D Loss: 0.0159, G Loss: 5.4082\n","Epoch [85/100], D Loss: 0.0175, G Loss: 5.2277\n","Epoch [86/100], D Loss: 0.0225, G Loss: 5.1096\n","Epoch [87/100], D Loss: 0.0208, G Loss: 5.2449\n","Epoch [88/100], D Loss: 0.0218, G Loss: 5.2470\n","Epoch [89/100], D Loss: 0.0209, G Loss: 5.1480\n","Epoch [90/100], D Loss: 0.0232, G Loss: 5.1306\n","Epoch [91/100], D Loss: 0.0219, G Loss: 5.2042\n","Epoch [92/100], D Loss: 0.0316, G Loss: 5.2055\n","Epoch [93/100], D Loss: 0.0228, G Loss: 5.0828\n","Epoch [94/100], D Loss: 0.0247, G Loss: 5.1534\n","Epoch [95/100], D Loss: 0.0245, G Loss: 5.3743\n","Epoch [96/100], D Loss: 0.0207, G Loss: 5.3014\n","Epoch [97/100], D Loss: 0.0226, G Loss: 5.1304\n","Epoch [98/100], D Loss: 0.0275, G Loss: 5.1167\n","Epoch [99/100], D Loss: 0.0277, G Loss: 5.0669\n","Epoch [100/100], D Loss: 0.0237, G Loss: 5.0564\n","âœ… VAE-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 2.7423724385572,\n","    \"FID (Original vs Eval)\": -0.0999999999999815,\n","    \"KL Divergence\": 1.4217475624262967,\n","    \"Cosine Similarity\": 0.010838971473276615,\n","    \"Spearman Rank Correlation\": -0.0022325216407147186,\n","    \"Wasserstein Distance\": 0.35340698716773916,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.2914136034808904\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_VAE-GAN_ImprovedVAE_mse.json\n","âœ… Completed pipeline for GAN type: VAE-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Contrastive-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Contrastive-GAN...\n","Epoch [1/100], D Loss: 1.3522, G Loss: 0.8681\n","Epoch [2/100], D Loss: 1.3165, G Loss: 0.9022\n","Epoch [3/100], D Loss: 1.2900, G Loss: 0.9244\n","Epoch [4/100], D Loss: 1.2686, G Loss: 0.9440\n","Epoch [5/100], D Loss: 1.2354, G Loss: 0.9745\n","Epoch [6/100], D Loss: 1.1852, G Loss: 1.0176\n","Epoch [7/100], D Loss: 1.1202, G Loss: 1.0799\n","Epoch [8/100], D Loss: 1.0550, G Loss: 1.1306\n","Epoch [9/100], D Loss: 0.9651, G Loss: 1.2173\n","Epoch [10/100], D Loss: 0.8648, G Loss: 1.3225\n","Epoch [11/100], D Loss: 0.7710, G Loss: 1.3893\n","Epoch [12/100], D Loss: 0.6686, G Loss: 1.5014\n","Epoch [13/100], D Loss: 0.5325, G Loss: 1.7233\n","Epoch [14/100], D Loss: 0.4695, G Loss: 1.7951\n","Epoch [15/100], D Loss: 0.3746, G Loss: 2.0137\n","Epoch [16/100], D Loss: 0.3220, G Loss: 2.1401\n","Epoch [17/100], D Loss: 0.2698, G Loss: 2.3415\n","Epoch [18/100], D Loss: 0.2430, G Loss: 2.4000\n","Epoch [19/100], D Loss: 0.2004, G Loss: 2.6147\n","Epoch [20/100], D Loss: 0.1755, G Loss: 2.7783\n","Epoch [21/100], D Loss: 0.1440, G Loss: 3.0608\n","Epoch [22/100], D Loss: 0.1362, G Loss: 3.0539\n","Epoch [23/100], D Loss: 0.1149, G Loss: 3.2052\n","Epoch [24/100], D Loss: 0.1252, G Loss: 3.1032\n","Epoch [25/100], D Loss: 0.0872, G Loss: 3.5952\n","Epoch [26/100], D Loss: 0.0959, G Loss: 3.4424\n","Epoch [27/100], D Loss: 0.0810, G Loss: 3.6101\n","Epoch [28/100], D Loss: 0.0838, G Loss: 3.5883\n","Epoch [29/100], D Loss: 0.0825, G Loss: 3.6003\n","Epoch [30/100], D Loss: 0.0764, G Loss: 3.7402\n","Epoch [31/100], D Loss: 0.0635, G Loss: 3.9141\n","Epoch [32/100], D Loss: 0.0547, G Loss: 4.1186\n","Epoch [33/100], D Loss: 0.0567, G Loss: 4.1355\n","Epoch [34/100], D Loss: 0.0510, G Loss: 4.2442\n","Epoch [35/100], D Loss: 0.0565, G Loss: 4.0076\n","Epoch [36/100], D Loss: 0.0521, G Loss: 4.1304\n","Epoch [37/100], D Loss: 0.0443, G Loss: 4.3538\n","Epoch [38/100], D Loss: 0.0407, G Loss: 4.3816\n","Epoch [39/100], D Loss: 0.0389, G Loss: 4.5130\n","Epoch [40/100], D Loss: 0.0454, G Loss: 4.3043\n","Epoch [41/100], D Loss: 0.0391, G Loss: 4.5741\n","Epoch [42/100], D Loss: 0.0433, G Loss: 4.4606\n","Epoch [43/100], D Loss: 0.0443, G Loss: 4.4087\n","Epoch [44/100], D Loss: 0.0316, G Loss: 4.6641\n","Epoch [45/100], D Loss: 0.0334, G Loss: 4.6971\n","Epoch [46/100], D Loss: 0.0315, G Loss: 4.8156\n","Epoch [47/100], D Loss: 0.0312, G Loss: 4.7412\n","Epoch [48/100], D Loss: 0.0340, G Loss: 4.7401\n","Epoch [49/100], D Loss: 0.0272, G Loss: 4.8122\n","Epoch [50/100], D Loss: 0.0327, G Loss: 4.7088\n","Epoch [51/100], D Loss: 0.0304, G Loss: 4.8926\n","Epoch [52/100], D Loss: 0.0268, G Loss: 4.9316\n","Epoch [53/100], D Loss: 0.0272, G Loss: 4.8529\n","Epoch [54/100], D Loss: 0.0275, G Loss: 4.9295\n","Epoch [55/100], D Loss: 0.0283, G Loss: 5.0949\n","Epoch [56/100], D Loss: 0.0261, G Loss: 5.1058\n","Epoch [57/100], D Loss: 0.0330, G Loss: 4.8463\n","Epoch [58/100], D Loss: 0.0290, G Loss: 4.8850\n","Epoch [59/100], D Loss: 0.0297, G Loss: 4.9960\n","Epoch [60/100], D Loss: 0.0291, G Loss: 5.0022\n","Epoch [61/100], D Loss: 0.0294, G Loss: 4.8406\n","Epoch [62/100], D Loss: 0.0250, G Loss: 5.1863\n","Epoch [63/100], D Loss: 0.0285, G Loss: 5.0472\n","Epoch [64/100], D Loss: 0.0228, G Loss: 5.1819\n","Epoch [65/100], D Loss: 0.0260, G Loss: 5.2141\n","Epoch [66/100], D Loss: 0.0218, G Loss: 5.3546\n","Epoch [67/100], D Loss: 0.0207, G Loss: 5.3609\n","Epoch [68/100], D Loss: 0.0261, G Loss: 4.9980\n","Epoch [69/100], D Loss: 0.0345, G Loss: 4.9760\n","Epoch [70/100], D Loss: 0.0213, G Loss: 5.5688\n","Epoch [71/100], D Loss: 0.0281, G Loss: 5.0758\n","Epoch [72/100], D Loss: 0.0267, G Loss: 4.9517\n","Epoch [73/100], D Loss: 0.0223, G Loss: 5.1656\n","Epoch [74/100], D Loss: 0.0318, G Loss: 5.1869\n","Epoch [75/100], D Loss: 0.0262, G Loss: 5.2141\n","Epoch [76/100], D Loss: 0.0262, G Loss: 5.2252\n","Epoch [77/100], D Loss: 0.0255, G Loss: 5.4047\n","Epoch [78/100], D Loss: 0.0265, G Loss: 5.2458\n","Epoch [79/100], D Loss: 0.0295, G Loss: 5.0912\n","Epoch [80/100], D Loss: 0.0270, G Loss: 5.0916\n","Epoch [81/100], D Loss: 0.0283, G Loss: 5.1948\n","Epoch [82/100], D Loss: 0.0299, G Loss: 5.2087\n","Epoch [83/100], D Loss: 0.0351, G Loss: 4.9768\n","Epoch [84/100], D Loss: 0.0339, G Loss: 5.0821\n","Epoch [85/100], D Loss: 0.0273, G Loss: 5.1442\n","Epoch [86/100], D Loss: 0.0376, G Loss: 4.9796\n","Epoch [87/100], D Loss: 0.0287, G Loss: 5.2443\n","Epoch [88/100], D Loss: 0.0333, G Loss: 5.0011\n","Epoch [89/100], D Loss: 0.0272, G Loss: 5.1677\n","Epoch [90/100], D Loss: 0.0345, G Loss: 5.1661\n","Epoch [91/100], D Loss: 0.0218, G Loss: 5.6179\n","Epoch [92/100], D Loss: 0.0310, G Loss: 5.1602\n","Epoch [93/100], D Loss: 0.0439, G Loss: 4.9187\n","Epoch [94/100], D Loss: 0.0320, G Loss: 5.2530\n","Epoch [95/100], D Loss: 0.0360, G Loss: 5.2914\n","Epoch [96/100], D Loss: 0.0376, G Loss: 5.1956\n","Epoch [97/100], D Loss: 0.0302, G Loss: 5.3927\n","Epoch [98/100], D Loss: 0.0377, G Loss: 5.3972\n","Epoch [99/100], D Loss: 0.0493, G Loss: 5.2204\n","Epoch [100/100], D Loss: 0.0418, G Loss: 5.1625\n","âœ… Contrastive-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.4259492185233519,\n","    \"FID (Original vs Eval)\": -0.0999999999999815,\n","    \"KL Divergence\": 1.4089475710364172,\n","    \"Cosine Similarity\": -0.16452746093273163,\n","    \"Spearman Rank Correlation\": -0.005799022680221213,\n","    \"Wasserstein Distance\": 0.3495755032594086,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.4300533115775942\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Contrastive-GAN_ImprovedVAE_mse.json\n","âœ… Completed pipeline for GAN type: Contrastive-GAN\n","\n","\n","ðŸš€ Running pipeline for GAN type: Cross-Domain-GAN\n","ðŸ”„ Running GAN training...\n","ðŸš€ Training Cross-Domain-GAN...\n","Epoch [1/100], D Loss: 1.3847, G Loss: 0.6651\n","Epoch [2/100], D Loss: 1.3777, G Loss: 0.6747\n","Epoch [3/100], D Loss: 1.3808, G Loss: 0.6763\n","Epoch [4/100], D Loss: 1.3736, G Loss: 0.6854\n","Epoch [5/100], D Loss: 1.3730, G Loss: 0.6870\n","Epoch [6/100], D Loss: 1.3747, G Loss: 0.6874\n","Epoch [7/100], D Loss: 1.3736, G Loss: 0.6925\n","Epoch [8/100], D Loss: 1.3720, G Loss: 0.6980\n","Epoch [9/100], D Loss: 1.3731, G Loss: 0.6971\n","Epoch [10/100], D Loss: 1.3734, G Loss: 0.6962\n","Epoch [11/100], D Loss: 1.3726, G Loss: 0.6978\n","Epoch [12/100], D Loss: 1.3737, G Loss: 0.6981\n","Epoch [13/100], D Loss: 1.3731, G Loss: 0.7005\n","Epoch [14/100], D Loss: 1.3733, G Loss: 0.6995\n","Epoch [15/100], D Loss: 1.3728, G Loss: 0.7001\n","Epoch [16/100], D Loss: 1.3737, G Loss: 0.6998\n","Epoch [17/100], D Loss: 1.3738, G Loss: 0.6997\n","Epoch [18/100], D Loss: 1.3737, G Loss: 0.6996\n","Epoch [19/100], D Loss: 1.3740, G Loss: 0.6997\n","Epoch [20/100], D Loss: 1.3742, G Loss: 0.7003\n","Epoch [21/100], D Loss: 1.3743, G Loss: 0.6997\n","Epoch [22/100], D Loss: 1.3749, G Loss: 0.6989\n","Epoch [23/100], D Loss: 1.3754, G Loss: 0.6990\n","Epoch [24/100], D Loss: 1.3761, G Loss: 0.6982\n","Epoch [25/100], D Loss: 1.3760, G Loss: 0.6989\n","Epoch [26/100], D Loss: 1.3760, G Loss: 0.6989\n","Epoch [27/100], D Loss: 1.3757, G Loss: 0.6989\n","Epoch [28/100], D Loss: 1.3759, G Loss: 0.6992\n","Epoch [29/100], D Loss: 1.3767, G Loss: 0.6986\n","Epoch [30/100], D Loss: 1.3765, G Loss: 0.6993\n","Epoch [31/100], D Loss: 1.3768, G Loss: 0.6980\n","Epoch [32/100], D Loss: 1.3762, G Loss: 0.6984\n","Epoch [33/100], D Loss: 1.3764, G Loss: 0.6987\n","Epoch [34/100], D Loss: 1.3764, G Loss: 0.6985\n","Epoch [35/100], D Loss: 1.3758, G Loss: 0.6993\n","Epoch [36/100], D Loss: 1.3758, G Loss: 0.6988\n","Epoch [37/100], D Loss: 1.3759, G Loss: 0.6987\n","Epoch [38/100], D Loss: 1.3758, G Loss: 0.6987\n","Epoch [39/100], D Loss: 1.3756, G Loss: 0.6984\n","Epoch [40/100], D Loss: 1.3752, G Loss: 0.6990\n","Epoch [41/100], D Loss: 1.3747, G Loss: 0.6995\n","Epoch [42/100], D Loss: 1.3746, G Loss: 0.6994\n","Epoch [43/100], D Loss: 1.3741, G Loss: 0.6994\n","Epoch [44/100], D Loss: 1.3737, G Loss: 0.7004\n","Epoch [45/100], D Loss: 1.3742, G Loss: 0.6993\n","Epoch [46/100], D Loss: 1.3738, G Loss: 0.6995\n","Epoch [47/100], D Loss: 1.3727, G Loss: 0.7006\n","Epoch [48/100], D Loss: 1.3724, G Loss: 0.7007\n","Epoch [49/100], D Loss: 1.3734, G Loss: 0.6997\n","Epoch [50/100], D Loss: 1.3723, G Loss: 0.7011\n","Epoch [51/100], D Loss: 1.3709, G Loss: 0.7013\n","Epoch [52/100], D Loss: 1.3710, G Loss: 0.7011\n","Epoch [53/100], D Loss: 1.3725, G Loss: 0.6997\n","Epoch [54/100], D Loss: 1.3714, G Loss: 0.7007\n","Epoch [55/100], D Loss: 1.3692, G Loss: 0.7025\n","Epoch [56/100], D Loss: 1.3696, G Loss: 0.7018\n","Epoch [57/100], D Loss: 1.3681, G Loss: 0.7030\n","Epoch [58/100], D Loss: 1.3688, G Loss: 0.7024\n","Epoch [59/100], D Loss: 1.3677, G Loss: 0.7034\n","Epoch [60/100], D Loss: 1.3664, G Loss: 0.7040\n","Epoch [61/100], D Loss: 1.3672, G Loss: 0.7033\n","Epoch [62/100], D Loss: 1.3659, G Loss: 0.7036\n","Epoch [63/100], D Loss: 1.3661, G Loss: 0.7040\n","Epoch [64/100], D Loss: 1.3668, G Loss: 0.7022\n","Epoch [65/100], D Loss: 1.3640, G Loss: 0.7049\n","Epoch [66/100], D Loss: 1.3649, G Loss: 0.7037\n","Epoch [67/100], D Loss: 1.3653, G Loss: 0.7039\n","Epoch [68/100], D Loss: 1.3636, G Loss: 0.7052\n","Epoch [69/100], D Loss: 1.3620, G Loss: 0.7060\n","Epoch [70/100], D Loss: 1.3614, G Loss: 0.7037\n","Epoch [71/100], D Loss: 1.3592, G Loss: 0.7084\n","Epoch [72/100], D Loss: 1.3581, G Loss: 0.7088\n","Epoch [73/100], D Loss: 1.3582, G Loss: 0.7069\n","Epoch [74/100], D Loss: 1.3598, G Loss: 0.7055\n","Epoch [75/100], D Loss: 1.3596, G Loss: 0.7057\n","Epoch [76/100], D Loss: 1.3548, G Loss: 0.7107\n","Epoch [77/100], D Loss: 1.3519, G Loss: 0.7123\n","Epoch [78/100], D Loss: 1.3595, G Loss: 0.7063\n","Epoch [79/100], D Loss: 1.3504, G Loss: 0.7143\n","Epoch [80/100], D Loss: 1.3474, G Loss: 0.7167\n","Epoch [81/100], D Loss: 1.3415, G Loss: 0.7206\n","Epoch [82/100], D Loss: 1.3541, G Loss: 0.7100\n","Epoch [83/100], D Loss: 1.3442, G Loss: 0.7170\n","Epoch [84/100], D Loss: 1.3466, G Loss: 0.7107\n","Epoch [85/100], D Loss: 1.3501, G Loss: 0.7139\n","Epoch [86/100], D Loss: 1.3515, G Loss: 0.7140\n","Epoch [87/100], D Loss: 1.3339, G Loss: 0.7241\n","Epoch [88/100], D Loss: 1.3416, G Loss: 0.7099\n","Epoch [89/100], D Loss: 1.3414, G Loss: 0.7173\n","Epoch [90/100], D Loss: 1.3309, G Loss: 0.7244\n","Epoch [91/100], D Loss: 1.3219, G Loss: 0.7273\n","Epoch [92/100], D Loss: 1.3162, G Loss: 0.7320\n","Epoch [93/100], D Loss: 1.3294, G Loss: 0.7274\n","Epoch [94/100], D Loss: 1.3194, G Loss: 0.7341\n","Epoch [95/100], D Loss: 1.3094, G Loss: 0.7451\n","Epoch [96/100], D Loss: 1.3249, G Loss: 0.7340\n","Epoch [97/100], D Loss: 1.3207, G Loss: 0.7259\n","Epoch [98/100], D Loss: 1.3324, G Loss: 0.7307\n","Epoch [99/100], D Loss: 1.3324, G Loss: 0.7254\n","Epoch [100/100], D Loss: 1.3117, G Loss: 0.7322\n","âœ… Cross-Domain-GAN training completed!\n","ðŸ“Š Evaluating GAN...\n","{\n","    \"FID (Original vs Generated)\": 0.5062056155646517,\n","    \"FID (Original vs Eval)\": -0.0999999999999815,\n","    \"KL Divergence\": 1.3406118374564924,\n","    \"Cosine Similarity\": -0.07397186756134033,\n","    \"Spearman Rank Correlation\": 0.007303826450957772,\n","    \"Wasserstein Distance\": 0.36115677334955837,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.43695174409294296\n","}\n","âœ… Report saved: ./reports/GANs_Evaluations/evaluation_Cross-Domain-GAN_ImprovedVAE_mse.json\n","âœ… Completed pipeline for GAN type: Cross-Domain-GAN\n","\n"]}]},{"cell_type":"code","source":["import time\n","from rich.progress import track\n","from rich.table import Table\n","from rich.console import Console\n","import pandas as pd\n","\n","# Initialize Rich console for pretty printing\n","console = Console()\n","\n","def print_summary_table(results):\n","    \"\"\"Print a summary table of all results.\"\"\"\n","    table = Table(title=\"GAN Evaluation Summary\", show_header=True, header_style=\"bold magenta\")\n","    table.add_column(\"Embedding\", style=\"cyan\")\n","    table.add_column(\"GAN Type\", style=\"green\")\n","    table.add_column(\"FID (Original vs Generated)\", justify=\"right\")\n","    table.add_column(\"Aggregate Quality Score\", justify=\"right\")\n","    table.add_column(\"Unique Embedding Ratio\", justify=\"right\")\n","    table.add_column(\"Time Taken (s)\", justify=\"right\")\n","\n","    for result in results:\n","        table.add_row(\n","            result[\"embedding\"],\n","            result[\"gan_type\"],\n","            f\"{result['metrics']['FID (Original vs Generated)']:.4f}\",\n","            f\"{result['metrics']['Aggregate Quality Score']:.4f}\",\n","            f\"{result['metrics']['Unique Embedding Ratio']:.4f}\",\n","            f\"{result['time_taken']:.2f}\"\n","        )\n","\n","    console.print(table)\n","\n","def save_results_to_csv(results, filename=\"gan_evaluation_results.csv\"):\n","    \"\"\"Save all results to a CSV file.\"\"\"\n","    df = pd.DataFrame(results)\n","    df.to_csv(filename, index=False)\n","    console.print(f\"âœ… All results saved to [bold green]{filename}[/bold green]\")\n","\n","def run_pipeline(config, embedding_relative_path):\n","    \"\"\"Run the GAN pipeline for a single embedding.\"\"\"\n","    results = []\n","\n","    # Full path to the embedding file\n","    embedding_file = os.path.join(embedding_base_dir, embedding_relative_path)\n","\n","    # Extract identifier from the path (remove directory and `_embeddings.pt`)\n","    embedding_identifier = embedding_relative_path.split(\"/\")[-1].replace(\"_embeddings.pt\", \"\")\n","\n","    # Update config with the current embedding\n","    config.update({\n","        \"embedding_identifier\": embedding_identifier,\n","        \"embedding_file\": embedding_file\n","    })\n","\n","    # Load embeddings and split\n","    embeddings, labels, full_data_loader = load_embeddings(embedding_file, device)\n","    config[\"embedding_dim\"] = embeddings.size(1)\n","    train_loader, eval_loader, train_embeddings, eval_embeddings = split_embeddings(embeddings, labels, config[\"eval_fraction\"], config[\"batch_size\"])\n","\n","    # Update config with data loaders\n","    config.update({\n","        \"data_loader\": train_loader,\n","        \"data_loader_a\": train_loader,\n","        \"data_loader_b\": train_loader,\n","        \"eval_loader\": eval_loader,\n","        \"original_embeddings\": embeddings  # Store original embeddings for evaluation\n","    })\n","\n","    # Loop through each GAN type\n","    for gan_type in config[\"gan_types\"]:\n","        console.print(f\"\\nðŸš€ [bold cyan]Running pipeline for GAN type: {gan_type}[/bold cyan]\")\n","\n","        # Update the GAN type in the config\n","        config[\"gan_type\"] = gan_type\n","\n","        # Initialize GAN components\n","        gan_components = initialize_gan_components(config, gan_configurations[config[\"gan_type\"]])\n","\n","        # Run GAN training\n","        start_time = time.time()\n","        console.print(\"ðŸ”„ [bold yellow]Running GAN training...[/bold yellow]\")\n","        run_gan_training(config)\n","\n","        # Evaluate GAN\n","        console.print(\"ðŸ“Š [bold yellow]Evaluating GAN...[/bold yellow]\")\n","        metrics = evaluate_gan(gan_components, config)\n","\n","        # Calculate time taken\n","        time_taken = time.time() - start_time\n","\n","        # Store results\n","        results.append({\n","            \"embedding\": embedding_identifier,\n","            \"gan_type\": gan_type,\n","            \"metrics\": metrics,\n","            \"time_taken\": time_taken\n","        })\n","\n","        console.print(f\"âœ… [bold green]Completed pipeline for GAN type: {gan_type}[/bold green]\")\n","\n","    return results\n","\n","# Main execution\n","all_results = []\n","autoencoder_embeddings = list_available_embeddings(embedding_base_dir, filter_by=\"vae\")\n","\n","for embedding_relative_path in track(autoencoder_embeddings, description=\"Processing embeddings...\"):\n","    console.print(f\"\\nðŸš€ [bold blue]Processing embedding: {embedding_relative_path}[/bold blue]\")\n","    results = run_pipeline(config, embedding_relative_path)\n","    all_results.extend(results)\n","\n","# Print summary table\n","print_summary_table(all_results)\n","\n","# Save all results to a CSV file\n","save_results_to_csv(all_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7d494d6e862a42c586e99f9ce44862cf","16e96275d9c84b06a4b71498af204f5a"]},"id":"9BmK9rOR2jHx","executionInfo":{"status":"error","timestamp":1738268100784,"user_tz":-210,"elapsed":2424,"user":{"displayName":"Farshad H","userId":"17155898055621377416"}},"outputId":"0ef360e9-6830-43b3-baab-c9380ace35cb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d494d6e862a42c586e99f9ce44862cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n","ðŸš€ \u001b[1;34mProcessing embedding: vae_BasicVAE_mse/BasicVAE_mse_embeddings.pt\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","ðŸš€ <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Processing embedding: vae_BasicVAE_mse/BasicVAE_mse_embeddings.pt</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<ipython-input-44-1c86f0ee2a21>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the \n","current default value), which uses the default pickle module implicitly. It is possible to construct malicious \n","pickle data which will execute arbitrary code during unpickling (See \n","https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, \n","the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed \n","during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are \n","explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting \n","`weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on \n","GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&lt;ipython-input-44-1c86f0ee2a21&gt;:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the \n","current default value), which uses the default pickle module implicitly. It is possible to construct malicious \n","pickle data which will execute arbitrary code during unpickling (See \n","https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, \n","the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed \n","during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are \n","explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting \n","`weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on \n","GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n","ðŸš€ \u001b[1;36mRunning pipeline for GAN type: WGAN-GP\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","ðŸš€ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Running pipeline for GAN type: WGAN-GP</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["ðŸ”„ \u001b[1;33mRunning GAN training\u001b[0m\u001b[1;33m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸ”„ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Running GAN training...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["ðŸš€ Training WGAN-GP...\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸš€ Training WGAN-GP...\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Epoch [1/1], Loss Critic: 6.2211, Loss Generator: -0.0669\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch [1/1], Loss Critic: 6.2211, Loss Generator: -0.0669\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["âœ… WGAN-GP training completed!\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… WGAN-GP training completed!\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["ðŸ“Š \u001b[1;33mEvaluating GAN\u001b[0m\u001b[1;33m...\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸ“Š <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Evaluating GAN...</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{\n","    \"FID (Original vs Generated)\": 2.0426993267040827,\n","    \"FID (Original vs Eval)\": -0.00230956595124613,\n","    \"KL Divergence\": 0.012482362750640724,\n","    \"Cosine Similarity\": 0.0007560572121292353,\n","    \"Spearman Rank Correlation\": -0.0045435115465250215,\n","    \"Wasserstein Distance\": 0.033092808348368516,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.4286933827136227\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n","\u001b[1m}\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">{\n","    \"FID (Original vs Generated)\": 2.0426993267040827,\n","    \"FID (Original vs Eval)\": -0.00230956595124613,\n","    \"KL Divergence\": 0.012482362750640724,\n","    \"Cosine Similarity\": 0.0007560572121292353,\n","    \"Spearman Rank Correlation\": -0.0045435115465250215,\n","    \"Wasserstein Distance\": 0.033092808348368516,\n","    \"Coverage Score\": 0.0,\n","    \"Memorization Score\": 0.0,\n","    \"Unique Embedding Ratio\": 1.0,\n","    \"Aggregate Quality Score\": 0.4286933827136227\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","<span style=\"font-weight: bold\">}</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"error","ename":"RecursionError","evalue":"maximum recursion depth exceeded in comparison","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36mprint\u001b[0;34m(self, sep, end, style, justify, overflow, no_wrap, emoji, markup, highlight, width, height, crop, soft_wrap, new_line_start, *objects)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1679\u001b[0;31m             renderables = self._collect_renderables(\n\u001b[0m\u001b[1;32m   1680\u001b[0m                 \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36m_collect_renderables\u001b[0;34m(self, objects, sep, end, justify, emoji, markup, highlight)\u001b[0m\n\u001b[1;32m   1538\u001b[0m                 append_text(\n\u001b[0;32m-> 1539\u001b[0;31m                     self.render_str(\n\u001b[0m\u001b[1;32m   1540\u001b[0m                         \u001b[0mrenderable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36mrender_str\u001b[0;34m(self, text, style, justify, overflow, emoji, markup, highlight, highlighter)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmarkup_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m             rich_text = render_markup(\n\u001b[0m\u001b[1;32m   1430\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/markup.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(markup, style, emoji, emoji_variant)\u001b[0m\n\u001b[1;32m    129\u001b[0m         return Text(\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0memoji_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_variant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memoji_variant\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0memoji\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/_emoji_replace.py\u001b[0m in \u001b[0;36m_emoji_replace\u001b[0;34m(text, default_variant, _emoji_sub)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0memoji_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memoji_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/typing.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-88eef81db2b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0membedding_relative_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nðŸš€ [bold blue]Processing embedding: {embedding_relative_path}[/bold blue]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_relative_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-88eef81db2b2>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(config, embedding_relative_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Evaluate GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ“Š [bold yellow]Evaluating GAN...[/bold yellow]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Calculate time taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-44-1c86f0ee2a21>\u001b[0m in \u001b[0;36mevaluate_gan\u001b[0;34m(gan_components, config)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/file_proxy.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mconsole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 output = Text(\"\\n\").join(\n\u001b[1;32m     45\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ansi_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;34m\"\"\"Exit buffer context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exit_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_capture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36m_exit_buffer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;34m\"\"\"Leave buffer context, and render content if required.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_index\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlive\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Live\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36m_check_buffer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2020\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBrokenPipeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_broken_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36m_write_buffer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2033\u001b[0m                     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2035\u001b[0;31m                     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2036\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/jupyter.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(segments, text)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mipython_display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mipython_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjupyter_renderable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Handle the case where the Console has force_jupyter=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;31m# kwarg-specified metadata gets precedence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0m_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDisplayHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mpublish_display_data\u001b[0;34m(data, metadata, source, transient, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transient'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     display_pub.publish(\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36mpublish\u001b[0;34m(self, data, metadata, source, transient, update)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msend\u001b[0m \u001b[0man\u001b[0m \u001b[0mupdate_display_data\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdisplay_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36m_flush_streams\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_flush_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m\"\"\"flush IO Streams prior to display\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/file_proxy.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__console\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36mprint\u001b[0;34m(self, sep, end, style, justify, overflow, no_wrap, emoji, markup, highlight, width, height, crop, soft_wrap, new_line_start, *objects)\u001b[0m\n\u001b[1;32m   1676\u001b[0m             \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0mrender_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1678\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m             renderables = self._collect_renderables(\n\u001b[1;32m   1680\u001b[0m                 \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","... last 11 frames repeated, from the frame below ...\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rich/console.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;34m\"\"\"Exit buffer context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exit_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_capture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1738145087430,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"pPZn0htEf73X","outputId":"9f9993a1-fe49-451d-c49f-014a35e424e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO - Loading embeddings from: ./saved_embeddings/embeddings/autoencoders_BasicAutoencoder/BasicAutoencoder_embeddings.pt\n","<ipython-input-24-23dfe1fe608b>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data = torch.load(embedding_file)\n"]}],"source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","def load_embeddings_v2(embedding_file, device, batch_size=64):\n","    \"\"\"\n","    Loads embeddings and their associated labels from a specified file and\n","    creates a DataLoader for batching the embeddings.\n","\n","    Args:\n","        embedding_file (str): Path to the file containing embeddings and labels.\n","        device (torch.device): The device (CPU/GPU) to load the tensors onto.\n","        batch_size (int, optional): The batch size for DataLoader. Default is 64.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - embeddings (torch.Tensor): Loaded embeddings.\n","            - labels (torch.Tensor): Corresponding labels for the embeddings.\n","            - data_loader (DataLoader): DataLoader for batching embeddings.\n","    \"\"\"\n","    logger.info(f\"Loading embeddings from: {embedding_file}\")\n","    data = torch.load(embedding_file)\n","    embeddings = data[\"embeddings\"].to(device)\n","    labels = data[\"labels\"].to(device)\n","\n","    # Create a TensorDataset and DataLoader\n","    dataset = TensorDataset(embeddings, labels)\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    return embeddings, labels, data_loader\n","\n","# Now it will work correctly!\n","embeddings, labels, data_loader = load_embeddings_v2(embedding_file, device)\n","\n","\n","# interesting way to load the embeddings...\n","\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def load_embeddings(embedding_file, device, batch_size=64, return_labels=True):\n","    \"\"\"\n","    Loads embeddings and their associated labels from a specified file and\n","    creates a DataLoader for batching the embeddings (and optionally labels).\n","\n","    Args:\n","        embedding_file (str): Path to the file containing embeddings and labels.\n","        device (torch.device): The device (CPU/GPU) to load the tensors onto.\n","        batch_size (int, optional): The batch size for DataLoader. Default is 64.\n","        return_labels (bool, optional): Whether to include labels in the DataLoader. Default is True.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - embeddings (torch.Tensor): Loaded embeddings.\n","            - labels (torch.Tensor, optional): Corresponding labels for the embeddings (if return_labels=True).\n","            - data_loader (DataLoader): DataLoader for batching embeddings (and labels if required).\n","    \"\"\"\n","    print(f\"Loading embeddings from: {embedding_file}\")\n","    data = torch.load(embedding_file)\n","    embeddings = data[\"embeddings\"].to(device)\n","\n","    # Initialize the DataLoader only with embeddings if labels are not required\n","    if return_labels:\n","        labels = data[\"labels\"].to(device)\n","        # Create a TensorDataset containing both embeddings and labels\n","        dataset = TensorDataset(embeddings, labels)\n","        return embeddings, labels, DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    else:\n","        # Create DataLoader for embeddings only\n","        dataset = TensorDataset(embeddings)  # Just embeddings\n","        return embeddings, DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","embeddings, labels, data_loader_v2 = load_embeddings(embedding_file, device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27072,"status":"ok","timestamp":1738171305231,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"NVxxi3hMtJwP","outputId":"7251b83e-a199-48f7-edc7-16551ea3f7f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialized components: {'generator': SimpleGANGenerator(\n","  (model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=100, out_features=1024, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=1024, out_features=512, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (2): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=512, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (3): Linear(in_features=256, out_features=50, bias=True)\n","    (4): Tanh()\n","  )\n","), 'discriminator': SemiSupervisedGANDiscriminator(\n","  (shared_model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=50, out_features=512, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=512, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","  )\n","  (adv_head): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=256, out_features=128, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): Linear(in_features=128, out_features=1, bias=True)\n","    (2): Sigmoid()\n","  )\n","  (class_head): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=256, out_features=128, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): Linear(in_features=128, out_features=10, bias=True)\n","  )\n",")}\n","ðŸš€ Training Semi-Supervised-GAN...\n","Epoch [1/1], D Loss: 0.0584, G Loss: 8.6814\n","âœ… Semi-Supervised-GAN training completed!\n"]}],"source":["import torch\n","\n","# ========================\n","# MAIN CONFIGURATION\n","# ========================\n","config = {\n","    \"gan_type\": \"Semi-Supervised-GAN\",  # Change this to switch models\n","    \"latent_dim\": 100,  # Latent space dimension\n","    \"embedding_dim\": embeddings.size(1),  # Embedding dimension\n","    \"num_classes\": 10,  # For conditional models\n","    \"categorical_dim\": 10,  # For InfoGAN\n","    \"epochs\": 1,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 1e-4,\n","    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","    \"lambda_gp\": 10,  # For WGAN-GP\n","    \"beta1\": 0.5,  # Adam optimizer beta1\n","    \"beta2\": 0.999,  # Adam optimizer beta2\n","    \"save_path\": \"gan_model.pth\",  # Path to save the model\n","    \"data_loader\": data_loader_v2,  # Main data loader\n","    \"data_loader_a\": data_loader,  # For cross-domain models\n","    \"data_loader_b\": data_loader,  # For cross-domain models\n","    \"show_model_architecture\": True  # Toggle to print model architectures\n","}\n","\n","# ========================\n","# MODEL INITIALIZATION\n","# ========================\n","\n","def initialize_gan_components(config, gan_config):\n","    \"\"\"Initialize all components for the specified GAN type\"\"\"\n","    components = {}\n","    gan_type = config[\"gan_type\"]\n","    multi_gan_types = [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]\n","\n","    if gan_type in multi_gan_types:\n","        gen_args = {\"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type != \"Cycle-GAN\":\n","            gen_args[\"latent_dim\"] = config[\"latent_dim\"]\n","\n","        components.update({\n","            \"generator_a\": gan_config[\"generator_a\"](**gen_args).to(config[\"device\"]),\n","            \"generator_b\": gan_config[\"generator_b\"](**gen_args).to(config[\"device\"]),\n","            \"discriminator_a\": gan_config[\"discriminator_a\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"]),\n","            \"discriminator_b\": gan_config[\"discriminator_b\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","        })\n","    else:\n","        gen_args = {\n","            \"latent_dim\": config[\"latent_dim\"],\n","            \"embedding_dim\": config[\"embedding_dim\"]\n","        }\n","        if gan_type in [\"Conditional-GAN\", \"InfoGAN\"]:\n","            gen_args[\"num_classes\"] = config[\"num_classes\"]\n","        if gan_type == \"InfoGAN\":\n","            gen_args[\"categorical_dim\"] = config[\"categorical_dim\"]\n","\n","        components[\"generator\"] = gan_config[\"generator\"](**gen_args).to(config[\"device\"])\n","\n","    if gan_type == \"WGAN-GP\":\n","        components[\"critic\"] = gan_config[\"critic\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","    elif gan_type == \"VAE-GAN\":\n","        components[\"encoder\"] = gan_config[\"encoder\"](embedding_dim=config[\"embedding_dim\"], latent_dim=config[\"latent_dim\"]).to(config[\"device\"])\n","        components[\"discriminator\"] = gan_config[\"discriminator\"](embedding_dim=config[\"embedding_dim\"]).to(config[\"device\"])\n","    elif gan_type not in multi_gan_types:\n","        disc_args = {\"embedding_dim\": config[\"embedding_dim\"]}\n","        if gan_type in [\"Conditional-GAN\", \"InfoGAN\", \"Semi-Supervised-GAN\"]:\n","            disc_args[\"num_classes\"] = config[\"num_classes\"]\n","        if gan_type == \"InfoGAN\":\n","            disc_args[\"categorical_dim\"] = config[\"categorical_dim\"]\n","\n","        components[\"discriminator\"] = gan_config[\"discriminator\"](**disc_args).to(config[\"device\"])\n","\n","    if config[\"show_model_architecture\"]:\n","        print(f\"Initialized components: {components}\")\n","\n","    return components\n","\n","# ========================\n","# MAIN EXECUTION\n","# ========================\n","\n","def run_gan_training(config):\n","    \"\"\"Main function to initialize and train the GAN\"\"\"\n","    gan_type = config[\"gan_type\"]\n","    if gan_type not in gan_configurations:\n","        raise ValueError(f\"Unsupported GAN type: {gan_type}\")\n","\n","    gan_config = gan_configurations[gan_type]\n","    components = initialize_gan_components(config, gan_config)\n","    train_function = gan_config[\"train_function\"]\n","\n","    print(f\"ðŸš€ Training {gan_type}...\")\n","\n","    if gan_type == \"VAE-GAN\":\n","        train_function(components[\"encoder\"], components[\"generator\"], components[\"discriminator\"], **config)\n","    elif gan_type in [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]:\n","        train_function(components[\"generator_a\"], components[\"generator_b\"], components[\"discriminator_a\"], components[\"discriminator_b\"], **config)\n","    else:\n","        discriminator = components.get(\"discriminator\", components.get(\"critic\", None))\n","        train_function(components[\"generator\"], discriminator, **config)\n","\n","    print(f\"âœ… {gan_type} training completed!\")\n","\n","# ========================\n","# EXECUTION\n","# ========================\n","run_gan_training(config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28153,"status":"ok","timestamp":1738169050587,"user":{"displayName":"Farshad H","userId":"17155898055621377416"},"user_tz":-210},"id":"zMFweDi0uQo5","outputId":"46d354a4-da6b-48c3-9733-5076f80069c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing Semi-Supervised-GAN configuration: {'generator': <class 'src.gan_workflows.plan2.plan2_gan_models.SimpleGANGenerator'>, 'discriminator': <class 'src.gan_workflows.plan2.plan2_gan_models.SemiSupervisedGANDiscriminator'>, 'train_function': <function train_semi_supervised_gan at 0x7d7ef691dda0>}\n","Generator initialized on SimpleGANGenerator(\n","  (model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=100, out_features=1024, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=1024, out_features=512, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (2): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=512, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (3): Linear(in_features=256, out_features=50, bias=True)\n","    (4): Tanh()\n","  )\n",")\n","Semi-Supervised-GAN discriminator initialized on SemiSupervisedGANDiscriminator(\n","  (shared_model): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=50, out_features=512, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=512, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","  )\n","  (adv_head): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=256, out_features=128, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): Linear(in_features=128, out_features=1, bias=True)\n","    (2): Sigmoid()\n","  )\n","  (class_head): Sequential(\n","    (0): LinearBlock(\n","      (block): Sequential(\n","        (0): Linear(in_features=256, out_features=128, bias=True)\n","        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","      )\n","    )\n","    (1): Linear(in_features=128, out_features=10, bias=True)\n","  )\n",")\n","Epoch [1/1], D Loss: 0.1360, G Loss: 6.9575\n","Semi-Supervised-GAN training test passed!\n"]}],"source":["# GAN Configuration\n","gan_type = \"Semi-Supervised-GAN\"  # Change to the desired GAN type\n","latent_dim = 100  # Latent dimension for the generator\n","embedding_dim = embeddings.size(1)  # Embedding dimension based on loaded embeddings\n","num_classes = 10  # For Conditional GAN and InfoGAN (e.g., 10 classes for MNIST)\n","categorical_dim = 10\n","epochs = 1\n","learning_rate = 0.0001\n","# Ensure device is properly set up\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize the data loaders before the GAN models\n","embedding_loader_a = data_loader\n","embedding_loader_b = data_loader\n","embedding_loader = data_loader_v2\n","\n","# Initialize the correct GAN models based on `gan_type`\n","if gan_type in gan_configurations:\n","    config = gan_configurations[gan_type]\n","    print(f\"Initializing {gan_type} configuration: {config}\")\n","\n","    # Generator initialization for Dual-GAN, Cycle-GAN, and Contrastive-Dual-GAN\n","    if gan_type in [\"Dual-GAN\", \"Cycle-GAN\", \"Contrastive-Dual-GAN\"]:\n","        if gan_type == \"Cycle-GAN\":\n","            # Cycle-GAN requires two generators and two discriminators\n","            generator_a = config[\"generator_a\"](embedding_dim=embedding_dim).to(device)\n","            generator_b = config[\"generator_b\"](embedding_dim=embedding_dim).to(device)\n","            discriminator_a = config[\"discriminator_a\"](embedding_dim=embedding_dim).to(device)\n","            discriminator_b = config[\"discriminator_b\"](embedding_dim=embedding_dim).to(device)\n","            print(f\"CycleGAN generators and discriminators initialized on {generator_a}, {generator_b}, {discriminator_a}, {discriminator_b}\")\n","        else:\n","            # Dual-GAN or Contrastive-Dual-GAN requires both latent_dim and embedding_dim for generators\n","            generator_a = config[\"generator_a\"](latent_dim=latent_dim, embedding_dim=embedding_dim).to(device)\n","            generator_b = config[\"generator_b\"](latent_dim=latent_dim, embedding_dim=embedding_dim).to(device)\n","            discriminator_a = config[\"discriminator_a\"](embedding_dim=embedding_dim).to(device)\n","            discriminator_b = config[\"discriminator_b\"](embedding_dim=embedding_dim).to(device)\n","            print(f\"DualGAN/ContrastiveDualGAN generators and discriminators initialized on {generator_a}, {generator_b}, {discriminator_a}, {discriminator_b}\")\n","    else:\n","        # For other GANs (WGAN-GP, VAE-GAN, etc.), initialize a single generator\n","        generator_args = {\"latent_dim\": latent_dim, \"embedding_dim\": embedding_dim}\n","        if gan_type == \"InfoGAN\":\n","            # Correctly initialize InfoGAN generator with latent_dim and categorical_dim\n","            generator_args[\"categorical_dim\"] = categorical_dim\n","        if gan_type == \"Conditional-GAN\":\n","            # Conditional-GAN generator requires num_classes\n","            generator_args[\"num_classes\"] = num_classes\n","        generator = config[\"generator\"](**generator_args).to(device)\n","        print(f\"Generator initialized on {generator}\")\n","\n","    # Handle critic/discriminator initialization conditionally\n","    if gan_type == \"WGAN-GP\":\n","        # WGAN-GP uses a critic instead of a discriminator\n","        critic = config[\"critic\"](embedding_dim=embedding_dim).to(device)\n","        print(f\"Critic initialized on {critic}\")\n","    elif gan_type == \"Semi-Supervised-GAN\":\n","        # Semi-Supervised-GAN requires num_classes for the discriminator\n","        discriminator = config[\"discriminator\"](embedding_dim=embedding_dim, num_classes=num_classes).to(device)\n","        print(f\"Semi-Supervised-GAN discriminator initialized on {discriminator}\")\n","    elif gan_type == \"InfoGAN\":\n","        # InfoGAN discriminator requires embedding_dim and categorical_dim\n","        discriminator = config[\"discriminator\"](embedding_dim=embedding_dim, categorical_dim=categorical_dim).to(device)\n","        print(f\"InfoGAN discriminator initialized on {discriminator}\")\n","    elif gan_type == \"Conditional-GAN\":\n","        # Conditional-GAN discriminator requires embedding_dim and num_classes\n","        discriminator = config[\"discriminator\"](embedding_dim=embedding_dim, num_classes=num_classes).to(device)\n","        print(f\"Conditional-GAN discriminator initialized on {discriminator}\")\n","    elif gan_type not in [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"]:\n","        # For other models (VAE-GAN, Conditional-GAN, etc.), initialize a single discriminator\n","        discriminator_args = {\"embedding_dim\": embedding_dim}\n","        if gan_type in [\"Conditional-GAN\", \"InfoGAN\"]:\n","            discriminator_args[\"num_classes\"] = num_classes\n","        if gan_type == \"InfoGAN\":\n","            discriminator_args[\"categorical_dim\"] = categorical_dim\n","        discriminator = config[\"discriminator\"](**discriminator_args).to(device)\n","        print(f\"Discriminator initialized on {discriminator}\")\n","    else:\n","        # If discriminator is part of the configuration (Cycle-GAN, Dual-GAN, etc.), use the ones in the config\n","        discriminator_a = config[\"discriminator_a\"](embedding_dim=embedding_dim).to(device)\n","        discriminator_b = config[\"discriminator_b\"](embedding_dim=embedding_dim).to(device)\n","        print(f\"Discriminators initialized on {discriminator_a}, {discriminator_b}\")\n","\n","    # Initialize encoder for VAE-GAN\n","    if gan_type == \"VAE-GAN\":\n","        encoder = config[\"encoder\"](embedding_dim=embedding_dim, latent_dim=latent_dim).to(device)\n","        print(f\"Encoder initialized on {encoder}\")\n","\n","    # Select the appropriate training function\n","    train_function = config[\"train_function\"]\n","\n","    # Handle any additional configurations (like learning rate, lambda_gp)\n","    train_kwargs = config.get(\"train_kwargs\", {})  # Default to empty dict if not specified\n","    train_kwargs.update({\n","        \"latent_dim\": latent_dim,\n","        \"epochs\": epochs,  # Pass epochs as part of train_kwargs\n","        \"device\": device,\n","        \"learning_rate\": learning_rate,\n","        \"num_classes\": num_classes if gan_type in [\"Conditional-GAN\", \"InfoGAN\", \"Semi-Supervised-GAN\"] else None,  # Add for Conditional, InfoGAN, and Semi-Supervised-GAN\n","        \"categorical_dim\": categorical_dim if gan_type == \"InfoGAN\" else None,  # Add only for InfoGAN\n","        \"lambda_gp\": 10 if gan_type == \"WGAN-GP\" else None,  # Add only for WGAN-GP\n","    })\n","\n","    # Add data loaders conditionally\n","    if gan_type in [\"Dual-GAN\", \"Cycle-GAN\", \"Contrastive-Dual-GAN\"]:\n","        # Dual-GAN, Cycle-GAN, and Contrastive-Dual-GAN: pass two data loaders\n","        train_kwargs.update({\n","            \"data_loader_a\": embedding_loader_a,  # Two loaders for these models\n","            \"data_loader_b\": embedding_loader_b\n","        })\n","    else:\n","        # For all other models (WGAN-GP, VAE-GAN, etc.), use a single loader\n","        train_kwargs.update({\n","            \"data_loader\": embedding_loader  # Single data loader for other models\n","        })\n","\n","    # Training loop (now handled inside the train_function)\n","    if gan_type == \"VAE-GAN\":\n","        # VAE-GAN requires encoder, generator, and discriminator\n","        train_function(encoder, generator, discriminator, **train_kwargs)\n","    else:\n","        # For other models, pass the appropriate generator/discriminator/critic\n","        train_function(generator_a, generator_b, discriminator_a, discriminator_b, **train_kwargs) if gan_type in [\"Cycle-GAN\", \"Dual-GAN\", \"Contrastive-Dual-GAN\"] else train_function(generator, discriminator if \"discriminator\" in locals() else critic, **train_kwargs)\n","\n","    print(f\"{gan_type} training test passed!\")\n","\n","else:\n","    raise ValueError(f\"Unsupported GAN type: {gan_type}\")\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7d494d6e862a42c586e99f9ce44862cf":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_16e96275d9c84b06a4b71498af204f5a","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"Processing embeddings... \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processing embeddings... <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">  0%</span> <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span>\n</pre>\n"},"metadata":{}}]}},"16e96275d9c84b06a4b71498af204f5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}