{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Test Notebook for Project\n","\n","## Objective\n","#This notebook is dedicated to testing the core functions and modules of the project. Each section corresponds to a specific functionality, ensuring that individual components work as expected and integrate seamlessly.\n","\n","### Import Libraries and Setup\n","\n","import pytest\n","import numpy as np\n","import torch\n","# Add the src directory to sys.path\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/GAN-thesis-project/src')\n","from data_utils import preprocess_images, load_mnist_data, split_dataset"],"metadata":{"id":"Ax6PGrDJ_QBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iq-Jt6pVJeCy","cellView":"code"},"outputs":[],"source":["### Test `preprocess_images`\n","\n","#### Description\n","#The function `preprocess_images` normalizes images to the range [-1, 1].\n","\n","#### Test Cases\n","#- Verify that output values are in the range [-1, 1].\n","#- Check the output shape matches the expected dimensions.\n","\n","# Generate a sample batch of random images\n","sample_images = np.random.randint(0, 256, (10, 28, 28), dtype=np.uint8)\n","preprocessed_images = preprocess_images(sample_images)\n","\n","# Assertions\n","assert preprocessed_images.min() >= -1.0, \"Preprocessed images should not have values below -1.\"\n","assert preprocessed_images.max() <= 1.0, \"Preprocessed images should not have values above 1.\"\n","assert preprocessed_images.shape == (10, 1, 28, 28), \"Image shape after preprocessing is incorrect.\"\n","\n","print(\"`preprocess_images` test passed.\")"]},{"cell_type":"code","source":["### Test `load_mnist_data`\n","\n","#### Description\n","#The function `load_mnist_data` loads and preprocesses the MNIST dataset, allowing for fraction-based sampling and batching.\n","\n","#### Test Cases\n","#- Verify the dataset fraction is applied correctly.\n","#- Ensure the batch size is respected.\n","\n","data_loader = load_mnist_data(fraction=0.1, batch_size=8, shuffle=True)\n","\n","# Fetch a single batch for testing\n","for batch in data_loader:\n","    images, labels = batch\n","    # Assertions\n","    assert images.shape[0] <= 8, \"Batch size exceeds the specified limit.\"\n","    assert images.shape[1:] == (1, 28, 28), \"Image dimensions are incorrect.\"\n","    print(f\"Batch shape: {images.shape}, Labels shape: {labels.shape}\")\n","    break\n","\n","print(\"`load_mnist_data` test passed.\")"],"metadata":{"id":"V4CztA17_cGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Test `split_dataset`\n","\n","#### Description\n","#The function `split_dataset` splits a dataset into training and validation sets based on a specified ratio.\n","\n","#### Test Cases\n","#- Ensure proper splitting ratios.\n","#- Verify no data leakage between training and validation sets.\n","\n","# Generate random data and labels\n","data = np.random.rand(100, 28, 28)\n","labels = np.random.randint(0, 10, 100)\n","\n","(train_x, train_y), (val_x, val_y) = split_dataset(data, labels, validation_split=0.2)\n","\n","# Assertions\n","assert len(train_x) == 80, \"Training data size is incorrect.\"\n","assert len(val_x) == 20, \"Validation data size is incorrect.\"\n","assert len(train_y) == 80, \"Training labels size is incorrect.\"\n","assert len(val_y) == 20, \"Validation labels size is incorrect.\"\n","\n","# # Check for data leakage\n","# assert not set(train_y).intersection(set(val_y)), \"Data leakage detected between training and validation sets.\"\n","\n","# print(\"`split_dataset` test passed.\")\n"],"metadata":{"id":"mnBVBXpR_h55"},"execution_count":null,"outputs":[]}]}